WEBVTT FILE

1
00:00:00.000 --> 00:00:06.000
I'm Kevin Millar I'm one of the
engineering managers within ec2
specifically in some of our networking

2
00:00:06.000 --> 00:00:13.000
area and I've been with Amazon now for
about seven years so can talk about kind
of things that have happened over a long

3
00:00:13.000 --> 00:00:19.000
duration but today in particular this
morning I want to talk a little bit
about network performance and this is a

4
00:00:19.000 --> 00:00:26.000
400 level talk this is we're going to
deep dive on TCP performance and really
get into some of the the underlying

5
00:00:26.000 --> 00:00:33.000
nitty-gritty that happens within TCP and
and so the first you know the first
couple sections will really focus on you

6
00:00:33.000 --> 00:00:40.000
know what is TCP how does it work what
are some of the underlying parameters
and inputs the TCP uses to control how

7
00:00:40.000 --> 00:00:46.000
fast data is sent on the network which
obviously I'm sure most of you really do
care about when you're in the cloud and

8
00:00:46.000 --> 00:00:53.000
and you're trying to send data back and
forth or you know acts as a database or
deliver content and customers obviously

9
00:00:53.000 --> 00:00:58.000
network performance matters a great deal
so we're going to spend a couple the
first couple seconds really focused on

10
00:00:58.000 --> 00:01:07.000
that and then we're going to talk about
some some applications of this and some
samples I put together some situations I

11
00:01:07.000 --> 00:01:16.000
put together to really test how we could
tune tcp on linux to obtain higher
levels of network performance and so as

12
00:01:16.000 --> 00:01:22.000
a little bit of a of a teaser in one of
these applications later you'll see
actually we found a way to increase

13
00:01:22.000 --> 00:01:27.000
network performance a hundred
thirty-seven percent just with some
simple tweaks that we did that I did to

14
00:01:27.000 --> 00:01:35.000
the Linux settings on the server so
there is it is possible to really impact
TCP meaningfully with just some simple

15
00:01:35.000 --> 00:01:41.000
tweaks and really understanding how your
application works or and what you need
out of the network and so that's really

16
00:01:41.000 --> 00:01:49.000
the focus of today's talk again we're
going to dive super deep so so hang
tight all right so first off yes I you

17
00:01:49.000 --> 00:01:55.000
know I've been at amazon for seven years
i love the cloud but more than that i'm
a network guy i really love tcp so this

18
00:01:55.000 --> 00:02:02.000
is going to be let's let's let's dive
deep into tcp you know just to get
started tcp is a transmission control

19
00:02:02.000 --> 00:02:09.000
protocol i'm sure lots of you can you
know sort of name off the the usual sort
of three-way handshake about TCP most

20
00:02:09.000 --> 00:02:13.000
people know that TCP has a three-way
handshake
there's a lot more that's going on under

21
00:02:13.000 --> 00:02:21.000
under the hood and you know but
hopefully most of you many of you may
recognize tcp really does underlie

22
00:02:21.000 --> 00:02:28.000
almost all of the protocols that we use
to deliver application data so whether
that's you know management thrust this

23
00:02:28.000 --> 00:02:37.000
age delivering content through HTTP or
HTTPS accessing a database server from
an application server sending email you

24
00:02:37.000 --> 00:02:44.000
know you name it there's there's only a
few protocols really in widespread use
that don't use TCP and one of the

25
00:02:44.000 --> 00:02:51.000
primary reasons for that is because TCP
does provide stream-based delivery of
content so TCP is responsible for

26
00:02:51.000 --> 00:02:56.000
maintaining in order delivery of your
your messages making sure your your
messages on your network sockets are

27
00:02:56.000 --> 00:03:02.000
delivered in the same order that they're
sent it also performs flow control to
try to speed up or slow down a

28
00:03:02.000 --> 00:03:08.000
connection based on what it's perceiving
in the network in terms of congestion so
you know a lot of the internet uses tcp

29
00:03:08.000 --> 00:03:16.000
and the mechanisms here are obviously
have pretty widespread applicability all
right so throughout today we're going to

30
00:03:16.000 --> 00:03:21.000
be talking primarily about just some you
know very simple test bed and and and
you know this is the kind of thing that

31
00:03:21.000 --> 00:03:29.000
you know I spun up in an ending them in
a few minutes with VC to something that
you can obviously spin up as well so

32
00:03:29.000 --> 00:03:34.000
we're gonna have Jack and Jill and let's
say that Jack and Jill want to connect
with each other they want to have a TCP

33
00:03:34.000 --> 00:03:41.000
connection between them and exchange of
data for whatever purpose now the first
thing you realize with with TCP is that

34
00:03:41.000 --> 00:03:51.000
although we often commonly talk about a
connection between Jo to to institute
instances or two servers or a client and

35
00:03:51.000 --> 00:03:57.000
server we often talk about it as a
connection you know really when we are
digging the next level deep we really

36
00:03:57.000 --> 00:04:03.000
want to be thinking about the fact that
it is a pair of unidirectional
connections so Jack may want to send

37
00:04:03.000 --> 00:04:10.000
some data to Jill Jill may want to send
some data back to Jack on the same
connection but when you think about when

38
00:04:10.000 --> 00:04:16.000
we look at all the TCP parameters and
the mechanisms involved there we really
should separate out the set you know the

39
00:04:16.000 --> 00:04:24.000
path in one direction from the path back
in the opposite direction because they
can operate

40
00:04:24.000 --> 00:04:34.000
alright so we're going to be talking
about several of the aspects of tcp and
well one thing sort of the next the next

41
00:04:34.000 --> 00:04:43.000
level of detail in the next level that
we're going to dig into is how does TCP
control you how much data is in flight

42
00:04:43.000 --> 00:04:50.000
at any one time and as you'll see in a
moment you know the amount of data you
have in flight at one time between Jack

43
00:04:50.000 --> 00:04:57.000
and Jill or Jill and Jack that has a
direct correlation to your bandwidth to
how much throughput you actually do

44
00:04:57.000 --> 00:05:03.000
perceive in the end so you know
typically we often get the final number
we see oh this connection you know was a

45
00:05:03.000 --> 00:05:10.000
it was operating at 100 megabits per
second or is operating at 20 megabits
per second but but the the sort of the

46
00:05:10.000 --> 00:05:18.000
inputs the next level of inputs to that
are basically the amount of data that is
being sent at any one time or is in

47
00:05:18.000 --> 00:05:26.000
flight between these two machines and
there's there's really two ways that TCP
controls that or to two inputs to

48
00:05:26.000 --> 00:05:33.000
control that amount of data and those
are the receive window and the
congestion window so the receive window

49
00:05:33.000 --> 00:05:44.000
is essentially what Jack is signaling to
Jill to say this is how much data you
can send me at one time before I send

50
00:05:44.000 --> 00:05:52.000
you an acknowledgement because as as
Jill is sending data to Jack Jack needs
to queue that data and deliver it to an

51
00:05:52.000 --> 00:05:58.000
application and so that TCP status is
signaling back to Jill saying you know
you can send me this amount of data

52
00:05:58.000 --> 00:06:04.000
before you get an acknowledgement back
for me and that will make sure that I
don't over on my buffers on Jack's side

53
00:06:04.000 --> 00:06:10.000
so that's one of the two mechanisms
that's used to control the amount of
data in flight the second is this

54
00:06:10.000 --> 00:06:19.000
congestion window and the congestion
window is managed on the sender side so
Jill is as jilla sending data to jack

55
00:06:19.000 --> 00:06:27.000
the tcp stack on Jill is watching what's
happening on that connection you know
specifically it's watching am I seeing

56
00:06:27.000 --> 00:06:36.000
time out to my perceiving loss and
adjusting the amount of data it's
willing in this case Jill is willing to

57
00:06:36.000 --> 00:06:42.000
put on the network until an
acknowledgment comes back from Jack so
these are two mechanisms that are really

58
00:06:42.000 --> 00:06:50.000
operating in parallel and you are as
Jill is sending then Jill is responsible
for making sure that both the receive

59
00:06:50.000 --> 00:06:58.000
window and the congestion window are
being adhered to and that I mentioned as
well you know bandwidth is really is a

60
00:06:58.000 --> 00:07:04.000
function of how much data is in flight
it's also a function of the round-trip
time between Jack and Jill and we'll dig

61
00:07:04.000 --> 00:07:14.000
into that a little bit more here in a
moment specifically you know here's an
example you know of that so the term for

62
00:07:14.000 --> 00:07:21.000
this is the bandwidth delay product
right and that is essentially how much
data is in flight and what is the

63
00:07:21.000 --> 00:07:26.000
round-trip time so I have an example
here again Jill is sending data to Jack
and let's say they have a 2 millisecond

64
00:07:26.000 --> 00:07:33.000
round-trip time you know this would be
for example if I was operating in the
same availability zone or or you know or

65
00:07:33.000 --> 00:07:38.000
even it within the same region I may see
up to about it you know I mean I may see
about a 2 millisecond trip time

66
00:07:38.000 --> 00:07:45.000
sometimes a little bit more but but you
know that's fairly typical and so if
Jack has a 100 kilobyte receive window

67
00:07:45.000 --> 00:07:51.000
and I have a 2 millisecond round-trip
time you can do the math fairly easily
it's it's shown here it's worked out

68
00:07:51.000 --> 00:08:00.000
that you basically convert that and you
can see that that works out to a maximum
throughput of 400 megabits per second

69
00:08:00.000 --> 00:08:05.000
and that is so again that is simply a
function of how big is the receive
window on Jack and what is the

70
00:08:05.000 --> 00:08:11.000
round-trip time between Jack and Jill
and the reason we use the round-trip
time is because again Jill is not going

71
00:08:11.000 --> 00:08:17.000
to send another piece of data until an
acknowledgment comes back so from the
time jail sends a piece of data that has

72
00:08:17.000 --> 00:08:23.000
to go to Jack and then jack will send
the end acknowledgement back to Jill and
that's what then triggers Jill to say

73
00:08:23.000 --> 00:08:29.000
okay I'll send some more data so we look
at the round-trip time as the as one of
the inputs to this function alright so

74
00:08:29.000 --> 00:08:34.000
that's one example foreigner megabits
per second sounds pretty good if I'm
trying to transfer a fair bit of data

75
00:08:34.000 --> 00:08:41.000
you know foreign megabits per second is
is okay it's not bad we could do better
but you know it'll suffice but let's say

76
00:08:41.000 --> 00:08:45.000
that Jack and Jill are actually
separated by a much larger distance you
know let's say they're going across

77
00:08:45.000 --> 00:08:51.000
country where we might see up to 100
millisecond around
time across the united states depending

78
00:08:51.000 --> 00:08:56.000
on where you're going a hundred
milliseconds might be your round trip
time well now you're talking about a

79
00:08:56.000 --> 00:09:03.000
much different output you're talking
about a maximum potential throughput of
around eight megabits per second and so

80
00:09:03.000 --> 00:09:10.000
you can see that Jack's receive window
the amount of data jack is willing to
send at anyone or receive rather at any

81
00:09:10.000 --> 00:09:17.000
one time has a has a huge impact on the
peak throughput that we can obtain so
we'll look into this in a little more

82
00:09:17.000 --> 00:09:27.000
detail so again the receive window is
controlled by the by the receiver and
then the receiver has to signal that to

83
00:09:27.000 --> 00:09:33.000
the sender and throughout the day I'm
going to show you plenty of examples
these are all on linux and typically

84
00:09:33.000 --> 00:09:39.000
I've done these all on the you know if
many of these are long-standing tweaks
you can do on many versions of Linux but

85
00:09:39.000 --> 00:09:47.000
in general I've been looking at these
recently on the most recent amazon linux
of am i so but if i want to interrogate

86
00:09:47.000 --> 00:09:55.000
the colonel to see you know what is the
maximum size of the receive window i can
look at this this versus CTL and you can

87
00:09:55.000 --> 00:10:02.000
see here that it's saying the maximum it
will allow is 11 megabytes you know and
that this is the analogous to that 100

88
00:10:02.000 --> 00:10:07.000
kilobyte buffer that i mentioned on the
previous slide so this is obviously
quite you know this is fairly generous

89
00:10:07.000 --> 00:10:15.000
again it's fairly generous particularly
if i have a very low latency connection
but again that the longer your that the

90
00:10:15.000 --> 00:10:21.000
larger the latency the larger the
round-trip time to between your two
machines the larger you need to make

91
00:10:21.000 --> 00:10:29.000
this to ink to get to increase your
maximum possible t speed performance so
that's the first command the second

92
00:10:29.000 --> 00:10:36.000
command is specifically for tcp and it's
a little bit more complicated because in
in linux it has three settings that are

93
00:10:36.000 --> 00:10:43.000
all sort of combined together into one
sis et al and that is essentially what
is the minimum the default and the

94
00:10:43.000 --> 00:10:50.000
maximum buffer size that will be
allocated and linux then we'll
automatically manage the buffer sizes

95
00:10:50.000 --> 00:10:57.000
for you you know based on on other
inputs and other algorithms but but
essentially here we're saying i think

96
00:10:57.000 --> 00:11:02.000
probably the most interesting thing to
look at here is the default which is to
say we will default in this case

97
00:11:02.000 --> 00:11:16.000
allocate a half mega byte buffer 524 288
bytes for my receive window and so again
if whoops so again if I if I wanted to

98
00:11:16.000 --> 00:11:24.000
increase my performance for particularly
for a high latency a high round trip
time connection these would be some

99
00:11:24.000 --> 00:11:32.000
sissy tl's that we could change to to
increase the the receive window okay so
we talked a little bit about the receive

100
00:11:32.000 --> 00:11:38.000
window and now we're going to shift
focus and talk a little bit about the
congestion window because again there's

101
00:11:38.000 --> 00:11:46.000
you know both windows are have a both
windows need to be respected as we're
sending data across a TCP connection so

102
00:11:46.000 --> 00:11:52.000
it you know even if we've adjusted the
receive window to give ourselves more
room to receive data we might be facing

103
00:11:52.000 --> 00:11:58.000
if we're looking at a performance
bottleneck or we want to increase TCP
performance we need to make sure that we

104
00:11:58.000 --> 00:12:05.000
do so in the congestion window as well
so the congestion window is is one of
these things it's a little bit of a

105
00:12:05.000 --> 00:12:13.000
little bit of magic and it will go
through that but it's controlled by the
sender and is managed by the congestion

106
00:12:13.000 --> 00:12:20.000
control algorithm this is the TCP
congestion control algorithm and this is
where we get some of the magic in TCP so

107
00:12:20.000 --> 00:12:30.000
the key inputs to the congestion window
typically it does vary by algorithm but
the two key inputs tend to be loss you

108
00:12:30.000 --> 00:12:40.000
know that is the sender's perception of
packets that are lost between the sender
and the receiver and and latency what is

109
00:12:40.000 --> 00:12:46.000
the you know the round-trip time
basically how long does it take for a
packet to get to the destination and

110
00:12:46.000 --> 00:12:51.000
then it's an acknowledgement to come
back and and some protocols actually do
have other inputs I should note but I'm

111
00:12:51.000 --> 00:12:57.000
not going to go into a ton of detail
today the other input that you sometimes
see is explicit signaling there are

112
00:12:57.000 --> 00:13:06.000
protocols to do explicit signaling of
congestion or congestion or other or
loss and those can be explicitly

113
00:13:06.000 --> 00:13:15.000
signaled back to the sender but for
today's purposes we're going to focus
really on loss and latency so when

114
00:13:15.000 --> 00:13:22.000
when when a tcp connection is first
established the congestion window is not
yet under complete control of the

115
00:13:22.000 --> 00:13:28.000
congestion control algorithm because the
algorithm doesn't have the inputs you
know the loss or the latency data

116
00:13:28.000 --> 00:13:35.000
necessarily to make a decision about how
fast or slow should I go how big should
my congestion window be so one of the

117
00:13:35.000 --> 00:13:41.000
settings that actually does have some
potentially a meaningful impact on your
application is the initial congestion

118
00:13:41.000 --> 00:13:49.000
window and that is basically the default
initial size the amount of data that we
that the TCP will allow to be put on the

119
00:13:49.000 --> 00:13:55.000
network before it starts to get the
inputs back about data being
successfully sent or received

120
00:13:55.000 --> 00:14:01.000
successfully received specifically so in
Linux the default you know the way you
see this the initial congestion window

121
00:14:01.000 --> 00:14:07.000
is actually by using the looking at the
routes the reason for that is that you
can actually set this on a per route

122
00:14:07.000 --> 00:14:12.000
basis I'll show you how to do that in a
moment and so you look at the route
table and you don't see anything here

123
00:14:12.000 --> 00:14:18.000
that really calls out as an initial
congestion window and again that's
because it's just defaulted and so the

124
00:14:18.000 --> 00:14:27.000
default and Linux is 33 frames or 33
packet switch for a 15 if you have a
1500 bite maximum transmission unit

125
00:14:27.000 --> 00:14:35.000
which connections to the internet
typically do that's going to work out to
about 4,300 bytes and that means that in

126
00:14:35.000 --> 00:14:43.000
the immediate moment after TCP is
connection at TCP connection is
established TCP is not going to put more

127
00:14:43.000 --> 00:14:48.000
than 4,300 bites on to the wire until it
starts to get acknowledgments back you
know which again could be fairly

128
00:14:48.000 --> 00:14:55.000
meaningful we'll talk about how that
really has impact in a little bit so
some you know if you if you look around

129
00:14:55.000 --> 00:15:03.000
some people recommend increasing this
window size again particularly if you
have a you know a particular you have a

130
00:15:03.000 --> 00:15:10.000
small amount of data that if you're if
you have a lot of TCP connections that
are very short-lived an exchange a small

131
00:15:10.000 --> 00:15:17.000
amount of data and and your round trip
time can be high you can actually see
some pretty meaningful impact as well

132
00:15:17.000 --> 00:15:23.000
because I'll show later by increasing
your initial congestion window and again
all this is doing is telling TCP that in

133
00:15:23.000 --> 00:15:28.000
that very first moment after a
connection is established I want you to
put more day

134
00:15:28.000 --> 00:15:34.000
on the wire waiting you know and then
wait for the acknowledgments to come
back so here i show an example of how i

135
00:15:34.000 --> 00:15:42.000
can update a route and say I'd like to
increase the initial congestion window
from 3 up to 16 packets and that gets me

136
00:15:42.000 --> 00:15:49.000
instead of that 4300 bytes in that
initial congestion window I'm now
increasing that to 23 23 k so imagine if

137
00:15:49.000 --> 00:15:56.000
you have a you know application that's
doing some heart beating or something
where the response payload is less than

138
00:15:56.000 --> 00:16:03.000
23 k you know now suddenly you've
changed this from being it so let's say
let's assume that it's between 4k and 23

139
00:16:03.000 --> 00:16:13.000
k now what you've done is you
potentially eliminated you know one one
half of a round trip of latency to to

140
00:16:13.000 --> 00:16:24.000
deliver your result on this on this
socket alright so we've talked a little
bit about the initial congestion window

141
00:16:24.000 --> 00:16:30.000
now I want to focus a little bit on
unlost know again loss is one of these
inputs to congestion and you know this

142
00:16:30.000 --> 00:16:38.000
is a this is a this chart as the result
of some experiments I did and you know
it's actually did this a few years ago

143
00:16:38.000 --> 00:16:44.000
and it's one of these things that is has
had a big impression on me because
previously I sort of thought well you

144
00:16:44.000 --> 00:16:50.000
know if you know one percent loss it
doesn't sound so bad or two percent loss
doesn't it sort of sound so bad but if

145
00:16:50.000 --> 00:16:58.000
you actually measure the impact on TCP
throughput loss has a hugely impactful
has huge impact on TCP throughput and

146
00:16:58.000 --> 00:17:04.000
again this is just sort of doing a
normal is normalized 100 if I if I have
a zero lost connection you know that's

147
00:17:04.000 --> 00:17:10.000
normalized to 100 what is my throughput
look like as I increase the loss rate
you can see that by the time you get to

148
00:17:10.000 --> 00:17:16.000
five or six percent you are basically
dead in the water I mean this is you
know you the amount of data that the

149
00:17:16.000 --> 00:17:23.000
amount of throughput you can get through
TCP once you get to five percent or six
percent error rate is just way way lower

150
00:17:23.000 --> 00:17:30.000
than what you can get at a completely
clean zero percent loss so again loss
has a you know this and all of this

151
00:17:30.000 --> 00:17:36.000
really is because loss is one of these
inputs to the congestion control
algorithm and then adds it to text loss

152
00:17:36.000 --> 00:17:40.000
it's going to it's going to pull the
it's going to close that window it's
going to keep that window

153
00:17:40.000 --> 00:17:50.000
to try to avoid creating more congestion
in the network and so you know again it
has a fairly significant human and

154
00:17:50.000 --> 00:17:56.000
essentially the takeaway for me is if
you're having you know if I'm having a
TCP throughput issue a network

155
00:17:56.000 --> 00:18:02.000
performance issue you know I can look at
the receive window we talked about that
earlier but one of the first leading

156
00:18:02.000 --> 00:18:11.000
suspects is no are we sensing loss on
this path so if i want to start
investigating this let's say i have that

157
00:18:11.000 --> 00:18:17.000
complaint i'm being told hey you know my
performance is bad i'm seeing queuing i
might be seeing data you know i'm trying

158
00:18:17.000 --> 00:18:22.000
to transfer data and it's just not
transferring very quickly or it's
potentially it's you know i have data

159
00:18:22.000 --> 00:18:28.000
backing up how do i investigate this
well the first thing you can do on linux
and there's a rough equivalent on

160
00:18:28.000 --> 00:18:35.000
windows as well for that matter you can
use netstat and you can just nuts FS you
can look for the retransmission counters

161
00:18:35.000 --> 00:18:44.000
and these are these are OS wide counters
so it's pretty coarse course data you
don't necessarily have this it doesn't

162
00:18:44.000 --> 00:18:49.000
necessarily help identify exactly what's
going on or where it's going on but it
does give you some clue that something's

163
00:18:49.000 --> 00:18:58.000
going awry you're seeing retransmissions
and retransmissions are done typically
in response to sense loss so so if you

164
00:18:58.000 --> 00:19:04.000
see a retransmission TCP thinks there's
lost somewhere and now again keep in
mind these are counters that are

165
00:19:04.000 --> 00:19:12.000
initialized to zero when your instance
is booted so you know 58,000 might seem
high but it might that might have been

166
00:19:12.000 --> 00:19:17.000
two weeks ago and right now you might
not be seeing it so you know typically
what I would do with this is just watch

167
00:19:17.000 --> 00:19:23.000
this over a few seconds or a few minutes
and see you know are my counter is
increasing right now am i sensing loss

168
00:19:23.000 --> 00:19:31.000
right now but we can do better we can
actually dig a little bit deeper than
just at the the macro OS level and to do

169
00:19:31.000 --> 00:19:38.000
that there's a command in in linux
installed by default typically and
certainly it's available in the amazon

170
00:19:38.000 --> 00:19:46.000
linux am I the SS command and what SS
does is actually gives you an the next
level of detail it really shows you on a

171
00:19:46.000 --> 00:19:53.000
per connection basis what are you seeing
right now you know essentially this is
giving you an

172
00:19:53.000 --> 00:20:01.000
bit of visibility into the TCP algorithm
the congestion control algorithm in the
state machine to really see what's going

173
00:20:01.000 --> 00:20:07.000
on with the socket now the downside is
you have to you have to watch us in real
time this is with netstat you can get a

174
00:20:07.000 --> 00:20:13.000
historical view of what's going on with
SS you're going to see what am i seeing
right this very moment but certainly if

175
00:20:13.000 --> 00:20:18.000
you're actively trouble shooting
something it's a great way to really get
some insight know what's going on so you

176
00:20:18.000 --> 00:20:24.000
know I won't necessarily call out every
little bit in here that's you know that
can be look you can look that up but you

177
00:20:24.000 --> 00:20:31.000
know a few things I will call out first
off in the upper left you see the TCP
state and that just tells you what is

178
00:20:31.000 --> 00:20:37.000
the the TCP stack think the state of
this connection is right now in this
case it's established so that's good

179
00:20:37.000 --> 00:20:46.000
that means we can send data back and
forth we see the send queue so we see
that actually the application whatever

180
00:20:46.000 --> 00:20:53.000
this this is no you can see actually in
the next bit you can see that this is an
HTTPS port locally so you know so what

181
00:20:53.000 --> 00:21:00.000
presumably this is my web server my web
server has written some data into my
kernel into my TCP socket and it's

182
00:21:00.000 --> 00:21:05.000
queued for transmission at this moment I
actually have you know almost 4
megabytes ready to be transmitted just

183
00:21:05.000 --> 00:21:12.000
sitting there waiting for the
acknowledgments to come back I can see
that the word cubic here we're going to

184
00:21:12.000 --> 00:21:18.000
dig and look into this a little bit
later but cubic is actually the name of
the TCP congestion control algorithm

185
00:21:18.000 --> 00:21:23.000
that we're using on this socket and
cubic is the default and Linux at this
point so this basically means i'm using

186
00:21:23.000 --> 00:21:32.000
the default TCP congestion control
algorithm but i'm going to show you
later how we can actually tweak this RTO

187
00:21:32.000 --> 00:21:40.000
stands for retransmission timeout and so
what that means is this is measured in
milliseconds and so what this indicates

188
00:21:40.000 --> 00:21:46.000
is that this for this socket the
congestion control algorithm is going to
wait up to two hundred and four

189
00:21:46.000 --> 00:21:52.000
milliseconds at this moment it's going
to wait up to 240 milliseconds after a
packet is transmitted before it

190
00:21:52.000 --> 00:21:58.000
considers it to have been lost and will
then initiate a retransmission and so
we'll talk a little bit about that

191
00:21:58.000 --> 00:22:06.000
actually in a few more slides but again
this is measured in milliseconds and
then cwnd stands for congestion window

192
00:22:06.000 --> 00:22:11.000
and so just like we talked about earlier
we talked about the size of the
congestion no here I can see that my

193
00:22:11.000 --> 00:22:19.000
congestion window is 138 bytes long and
so you can then do the math actually on
the the MSS is actually the oops is

194
00:22:19.000 --> 00:22:25.000
actually the previous field so you can
do the math to figure out basically the
size the total bite size of your

195
00:22:25.000 --> 00:22:32.000
congestion window as we as we talked
about earlier all right and then finally
you know again there's lots of stuff

196
00:22:32.000 --> 00:22:37.000
here you can dig into more detail but
finally we'll look at the retransmission
counter and see that we have seen some

197
00:22:37.000 --> 00:22:46.000
retransmissions on this socket so again
this is an indication that TCP has
previously seen loss on this socket and

198
00:22:46.000 --> 00:22:55.000
it's initiated it's retransmitted data
to accommodate that loss to account for
that loss alright and then the next

199
00:22:55.000 --> 00:23:00.000
level of detail so so you know what
thats ask you again you can get a bunch
of really really helpful information to

200
00:23:00.000 --> 00:23:06.000
see what's going on with my my tcp state
right now there's a tool that i actually
have the URL to this tool here it's

201
00:23:06.000 --> 00:23:13.000
brendan greg is you know one of the
engineers at netflix actually and he has
some some pretty nice performance

202
00:23:13.000 --> 00:23:21.000
diagnostic tools one of them at sorry
these tools in general all use the linux
kernel tracing capabilities to get more

203
00:23:21.000 --> 00:23:28.000
insight as far as what's going on with
my network right now and so he is a
there's a tool in his toolbox ecp

204
00:23:28.000 --> 00:23:35.000
retrans and what this does is simply
monitors the what it actually does is
hooks into the literally the kernel

205
00:23:35.000 --> 00:23:44.000
function that controls retransmissions
and it's essentially just watching for
initiations you know four hits on that

206
00:23:44.000 --> 00:23:53.000
function so this will actually give you
essentially a real-time view of which
sockets are seeing retransmissions you

207
00:23:53.000 --> 00:24:00.000
know i mean again in near real time so
in this case you know as I I ran this
it'll just sit there and as you're

208
00:24:00.000 --> 00:24:05.000
sending data back and forth it will you
know print out when it sees a
retransmission event so this can be a

209
00:24:05.000 --> 00:24:12.000
super valuable way to just watching in
real-time what sockets are seeing
retransmissions a que los you know at

210
00:24:12.000 --> 00:24:17.000
this at this time and again when you
when it sees that it gives you a little
bit of metadata tell

211
00:24:17.000 --> 00:24:24.000
you the source port and destination port
that's that's so you get the local
address oops wrong button there so you

212
00:24:24.000 --> 00:24:31.000
get the local address you get the port
number you know obviously 443 would be
https court and then you get the remote

213
00:24:31.000 --> 00:24:41.000
address remote port number and the state
of the socket and the this method there
this tool tries to correlate the socket

214
00:24:41.000 --> 00:24:48.000
back to a particular process ID running
on your machine so you can see it's
showing this process ID 10 6 55 or 588

215
00:24:48.000 --> 00:24:55.000
and so this is giving me an indication
that you know this that process ID owns
this socket and is seeing a

216
00:24:55.000 --> 00:25:02.000
retransmission right now ok so super
valuable this is giving me nice you know
good data to figure out where am I

217
00:25:02.000 --> 00:25:07.000
seeing a retransmission at this moment
so we're going to talk you know now
we're going to talk about you know what

218
00:25:07.000 --> 00:25:13.000
can we do about it how can we respond to
loss or retransmissions and so I
mentioned earlier that you know the

219
00:25:13.000 --> 00:25:22.000
congestion control algorithm is a little
bit of the magic that goes on here and
so you know I'm not going to go i could

220
00:25:22.000 --> 00:25:29.000
probably have a you know a multi-hour
talk on congestion control algorithms
and the details here there's a lot of

221
00:25:29.000 --> 00:25:34.000
complex math that goes into these
algorithms which and i encourage you to
to look up if you're really if you're

222
00:25:34.000 --> 00:25:40.000
interested but you know what we need to
for our purposes you know what I what I
want to do is systems engineers be able

223
00:25:40.000 --> 00:25:48.000
to tweak my congestion control
algorithms to increase my performance
that's ultimately my goal so so let's

224
00:25:48.000 --> 00:25:55.000
let's do enough to do that well you know
a little bit of history the congestion
control algorithm in Linux before the

225
00:25:55.000 --> 00:26:03.000
268 Colonel was what was known as New
Reno these algorithms all have various
names there's one name Vegas you'll see

226
00:26:03.000 --> 00:26:11.000
that I you know I don't guess it was
designed maybe in a casino I don't know
but before 268 you know New Reno is the

227
00:26:11.000 --> 00:26:19.000
default in 26 data was changed to a
protocol called BIC and then most
recently you know or after 26 19 it was

228
00:26:19.000 --> 00:26:25.000
changed to cubic which remains the
default algorithm that we see in Ian's
amazon linux and other linux kernel's

229
00:26:25.000 --> 00:26:30.000
today but but the cool thing here is
that Linux has a pretty pluggable
architecture for the

230
00:26:30.000 --> 00:26:36.000
algorithm and in fact every single
socket can have a different algorithm
you can use different algorithms on

231
00:26:36.000 --> 00:26:43.000
different sockets if you desire so
there's there's a variety of algorithms
that are that are typically available

232
00:26:43.000 --> 00:26:50.000
and again if you use Amazon Linux these
are all available by default no code
compilation required you can actually

233
00:26:50.000 --> 00:26:58.000
turn these on there's a variety of other
protocols you know Vegas Illinois
westwood high speed scalable and others

234
00:26:58.000 --> 00:27:05.000
so how do I actually do that well
because you're going to see later
actually I did this and and in fact

235
00:27:05.000 --> 00:27:12.000
found some pretty amazing results by
able by tweaking my algorithm so to
figure out what you have currently

236
00:27:12.000 --> 00:27:19.000
loaded in the kernel and available you
can use the topsis CTL and that's just
interrogating the kernel to say what

237
00:27:19.000 --> 00:27:26.000
congestion control algorithms are
currently loaded if you want to find all
of the ones that you can load the second

238
00:27:26.000 --> 00:27:33.000
command just looks at you're literally
just looks at your kernel module
directories to see what's there looks

239
00:27:33.000 --> 00:27:40.000
for the tcp prefix which which will
indicate that it's a TCP congestion
algorithm and in this case I've decided

240
00:27:40.000 --> 00:27:48.000
I want to try out the TCP the Illinois
algorithm and and the reason I've chosen
this one in particular is that I've done

241
00:27:48.000 --> 00:27:53.000
a little bit of a research and reading
and identified that the Illinois
algorithm is one that was was actually

242
00:27:53.000 --> 00:28:02.000
designed to respond more favorably to
lost to to to react a little bit
differently and potentially better in

243
00:28:02.000 --> 00:28:09.000
cases where we're seeing a low level of
packet loss so I've gone ahead and used
the you know colonel the modprobe

244
00:28:09.000 --> 00:28:15.000
command just to load up by Illinois
module and now if i reek weary the
colonel you can see that it's telling me

245
00:28:15.000 --> 00:28:23.000
that hey now I have all three of these
algorithms available for my use in the
colonel all right so now what I've what

246
00:28:23.000 --> 00:28:31.000
I want to do you know again I indicated
that every every single socket can use a
different algorithm if you so choose but

247
00:28:31.000 --> 00:28:36.000
but typically I you know I I haven't
done a ton of research but I haven't
come across too many pieces of software

248
00:28:36.000 --> 00:28:43.000
or web servers or what have you that
they give me this ability to control it
because the way that the way that it

249
00:28:43.000 --> 00:28:50.000
would be controlled is by the
the software signalling as such like
doing the the right system call to

250
00:28:50.000 --> 00:28:56.000
change the algorithm and so you know I
haven't found much software that can
allows me to do this so what I'm going

251
00:28:56.000 --> 00:29:01.000
to do for my test today is I'm just
going to override it system-wide I'm
just going to tell my linux system I

252
00:29:01.000 --> 00:29:08.000
want all new connections by default to
be established using the Illinois
algorithm and I do that with the first

253
00:29:08.000 --> 00:29:16.000
command and and now now that's set every
single new connection that comes through
unless it's overridden again which most

254
00:29:16.000 --> 00:29:23.000
software does not unless it's overridden
it'll it'll go ahead and use Illinois
and then if I really want to force

255
00:29:23.000 --> 00:29:29.000
Illinois to be used permanently on my
system I do need to write it to disk so
that it gets set when the the colonel

256
00:29:29.000 --> 00:29:37.000
reboots but you know the first command
changes it in memory for for my current
run of the of the operating system and

257
00:29:37.000 --> 00:29:46.000
then i can write it to disk to to be
permanent all right so we're going to
see some results from that in a little

258
00:29:46.000 --> 00:29:52.000
bit but I have a few other things we
want to touch on before we get to the
results a little bit earlier I talked

259
00:29:52.000 --> 00:29:58.000
about the retransmission timer that's
the RTO value and so you know the
retransmission timer also plays a pretty

260
00:29:58.000 --> 00:30:04.000
critical role you know when you think
about latency of your application
because what this does is it controls

261
00:30:04.000 --> 00:30:12.000
when the the algorithm considers that a
packet has been lost and so the you know
you this is actually a little bit tricky

262
00:30:12.000 --> 00:30:19.000
to get set i mean again tcp manages this
dynamically for you and in most cases
does a fairly good job but again

263
00:30:19.000 --> 00:30:26.000
depending on your network application
may may not be the right setting for you
and so the retransmission timer if it

264
00:30:26.000 --> 00:30:34.000
gets too low you can lead to can lead to
a little bit you know what could lead to
a packet that's delayed just a little

265
00:30:34.000 --> 00:30:40.000
bit too long and triggering a
retransmission and also tricking the
algorithm to really close down and slow

266
00:30:40.000 --> 00:30:45.000
down you close the congestion window and
really slow your connection down so if
it's too low you know you run into the

267
00:30:45.000 --> 00:30:54.000
danger that that happens but if it gets
too high then you can be essentially
waiting around waiting for that timer to

268
00:30:54.000 --> 00:30:59.000
run out if you actually experience loss
of a package
so imagine that you have a

269
00:30:59.000 --> 00:31:06.000
retransmission timer that's one second
and you are you know sending to
something that's 10 milliseconds away

270
00:31:06.000 --> 00:31:12.000
and you experience loss well that loss
will happen within that first 10
milliseconds and then you're waiting

271
00:31:12.000 --> 00:31:18.000
another 990 milliseconds for that timer
to fire before you just do that one
retransmission and you know that might

272
00:31:18.000 --> 00:31:25.000
get your packet through and you'd be on
your way so if you you know if the timer
gets too high that can really increase

273
00:31:25.000 --> 00:31:33.000
the latency for your for your socket as
well you know when when you see loss now
in Linux you know just like we saw

274
00:31:33.000 --> 00:31:39.000
earlier with the initial congestion
window you can set the default minimum
retransmission timer on your sockets the

275
00:31:39.000 --> 00:31:45.000
default in or you can set the minimum
rather the default is 200 milliseconds
and it's set on the route level just

276
00:31:45.000 --> 00:31:53.000
like the congestion window so if you if
you look at your route list you won't
see anything for the default but you

277
00:31:53.000 --> 00:32:00.000
know but but if I if I want to change it
on my route list I look at my routes and
this is you know this is a default this

278
00:32:00.000 --> 00:32:07.000
is my default set of routes that I would
see you know running my instance in
virtual private cloud and the 10 dot 16

279
00:32:07.000 --> 00:32:13.000
dot 16 route is is a local route to
other things in the in the same subnet
that I'm in so you know I could set this

280
00:32:13.000 --> 00:32:21.000
as I could change that timer for my my
local for the route for this you know
the local subnet route I could change it

281
00:32:21.000 --> 00:32:26.000
from my default route or I could add a
route if I wanted to have a different
time around a specific set of

282
00:32:26.000 --> 00:32:33.000
destinations and so one of the things
I'm going to do here is I'm actually
just going to go ahead and change my

283
00:32:33.000 --> 00:32:39.000
minimum retransmission timer just for my
local subnet route because I know that
you know my local subnet route it's in

284
00:32:39.000 --> 00:32:49.000
the same availability zone and if I see
loss then I'm almost surely going to see
that within 10 milliseconds so having a

285
00:32:49.000 --> 00:32:56.000
200 millisecond timer you know may
simply cause me to wait an extra 190
milliseconds so I'm going to go ahead

286
00:32:56.000 --> 00:33:02.000
and change it using the route command
i'm going to add an RTO min of 10
milliseconds on my route and then if i

287
00:33:02.000 --> 00:33:10.000
reek weary the route you'll see that
then that is that shows up in the route
output so i can confirm it set

288
00:33:10.000 --> 00:33:17.000
alright so few other topics and then
we're going to some of the applications
another thing that that researchers have

289
00:33:17.000 --> 00:33:25.000
identified that can potentially be
problematic for for TCP connections is
essentially the the queuing that might

290
00:33:25.000 --> 00:33:33.000
happen along the intermediate network
path between these two hosts and so when
you when you look at particularly longer

291
00:33:33.000 --> 00:33:40.000
pads when you have multiple routers or
network devices between two hosts
typically those devices are built to

292
00:33:40.000 --> 00:33:48.000
have interface buffers you know
essentially data comes in one side and
then will be routed and store it in an

293
00:33:48.000 --> 00:33:54.000
interface buffer an output buffer just
waiting for available time on the the
port to transmit you know now obviously

294
00:33:54.000 --> 00:34:01.000
if the port has time right now that'll
just be transmitted out you'll be done
but as the load increases on an

295
00:34:01.000 --> 00:34:07.000
individual port you know that can lead
to that buffer increasing in length of
the queue length increasing and that is

296
00:34:07.000 --> 00:34:13.000
really where you then start to see
latency that's what introduces latency
on your on your path and again the

297
00:34:13.000 --> 00:34:21.000
problem here can be that a little bit of
latency if if particularly you know even
if you just have two computers you just

298
00:34:21.000 --> 00:34:27.000
have Jack and Jill sending data let's
say that jill has a bunch of data to
send just starts dumping that data on

299
00:34:27.000 --> 00:34:36.000
the network in one shot as fast as you
possibly can you know that could lead to
some intermediate port buffer filling or

300
00:34:36.000 --> 00:34:43.000
certainly being longer you know getting
longer which then increases latency for
those individual packets and then that

301
00:34:43.000 --> 00:34:50.000
might actually end up triggering it
could end up triggering a retransmission
timeout and so there's a there's a whole

302
00:34:50.000 --> 00:34:56.000
body of research around this but the
team actually the folks that have doing
doing some research here have actually

303
00:34:56.000 --> 00:35:03.000
done a pretty nice job of trying to
simplify this down to a set of or a
simple algorithm that you can apply

304
00:35:03.000 --> 00:35:08.000
again in Linux this is actually
available to you out of the box and
Linux you can turn this on it's called

305
00:35:08.000 --> 00:35:17.000
the coddle algorithm and what this does
is actually pieces the rate that packets
are put on to the network to try to

306
00:35:17.000 --> 00:35:23.000
avoid creating these intermediate
buffers
creating you know along gating the

307
00:35:23.000 --> 00:35:30.000
intermediate buffers along the path from
two to machines and so you'll see that
this is this command the first command

308
00:35:30.000 --> 00:35:37.000
is actually just to list what is my
current configuration for what linux
calls traffic control that is basically

309
00:35:37.000 --> 00:35:44.000
how it manages its outgoing Q is coming
off of the box and so I can just list
and this is really the default you'll

310
00:35:44.000 --> 00:35:50.000
see this top one and then if I want to
activate this coddle algorithm it's it's
literally just one command this this

311
00:35:50.000 --> 00:35:56.000
last command you see and that turns on
this coddle algorithm to try to pace how
quickly packets are written to the

312
00:35:56.000 --> 00:36:03.000
network and this whole sort of the soul
sort of space is called active queue
management and this is something that

313
00:36:03.000 --> 00:36:11.000
we'll see in a moment actually does
improve network performance in some
cases all right a few other a few of the

314
00:36:11.000 --> 00:36:16.000
things I want to cover before we get to
some of the applications so you know
most of my test is I've explained I've

315
00:36:16.000 --> 00:36:25.000
run been running these tests in ec2 and
Amazon ec2 I ran all of these tests on a
pretty modern machine that had enhanced

316
00:36:25.000 --> 00:36:30.000
networking configured if you're not
familiar with enhanced networking I
strongly recommend looking at one of

317
00:36:30.000 --> 00:36:39.000
there was a talk last year at me invent
this STD 419 talk they're really deep
Tove on unenhanced networking there's

318
00:36:39.000 --> 00:36:46.000
lots of documentation online as well but
enhance networking really is a in almost
every care in every case I've seen

319
00:36:46.000 --> 00:36:54.000
enhanced networking is an improvement it
reduces the cpu utilization of your
instance reduces latency and jitter and

320
00:36:54.000 --> 00:36:59.000
gives you overall hire packet rate
performance and when you have we have
enhanced networking available on all of

321
00:36:59.000 --> 00:37:06.000
our modern instance families as you can
see here the drivers it does require
special drivers to activate those

322
00:37:06.000 --> 00:37:12.000
drivers have been baked in Tammuz on
linux and windows for a while so really
using enhanced networking now is as

323
00:37:12.000 --> 00:37:18.000
simple as launching the right amies on
modern instance types but if you're not
you have any questions about this i

324
00:37:18.000 --> 00:37:27.000
definitely recommend taking a look at
one of the past talks another thing to
note is that mark more the modern

325
00:37:27.000 --> 00:37:33.000
instance types in easy to also have
increased the maximum transmission unit
so for those of you who aren't familiar

326
00:37:33.000 --> 00:37:38.000
with this
this concept you know historically on
the internet we've seen a 1500 bite em

327
00:37:38.000 --> 00:37:46.000
tu as kind of the standard default which
if you look at how much data is
available for tcp that ends up being

328
00:37:46.000 --> 00:37:52.000
1448 bytes so you end up with about a
three and a half percent overhead on
every single packet for all the headers

329
00:37:52.000 --> 00:37:59.000
on your mt you know conure on your
frames but with with it with enhanced
networking and some of the modern

330
00:37:59.000 --> 00:38:07.000
instance types we've pushed up the MTU
to 9001 bites and that obviously just
because that gives you more payload with

331
00:38:07.000 --> 00:38:12.000
no additional headers required that does
actually reduce the amount of overhead
you have and so that can be an

332
00:38:12.000 --> 00:38:21.000
improvement as well come as well see
again the good news here is envy pc with
with modern instance types this is just

333
00:38:21.000 --> 00:38:27.000
done by default for you the 9000 bite
9001 bite mt was just done by default
there's really nothing to think about

334
00:38:27.000 --> 00:38:34.000
but it's worth just understanding what's
going on there if i want to confirm this
i can actually just use the IP linked

335
00:38:34.000 --> 00:38:40.000
list command and i'll just confirm what
my MTU is so in this case I've looked
and I've confirmed that I have a 9000

336
00:38:40.000 --> 00:38:48.000
one bite MTU you can you can tune this
on an individual route level if you like
and again this isn't really required the

337
00:38:48.000 --> 00:38:55.000
the MTU will be signaled automatically
to you if you're sending data out to the
internet you will have to still use a

338
00:38:55.000 --> 00:39:03.000
1500 bite MTU but that will be signaled
when when you try to initiate a
connection to the internet but if you

339
00:39:03.000 --> 00:39:07.000
wanted to override if you wanted to
force it yourself you can actually do
that again through the IP route command

340
00:39:07.000 --> 00:39:12.000
just for completeness sake I because i
am going to be tuning this myself a
little bit and this just shows how you

341
00:39:12.000 --> 00:39:22.000
can tune that alright so at long last
you know I've been highlighting this for
the entire talk I want to apply some of

342
00:39:22.000 --> 00:39:27.000
our new knowledge and just see how we
can take everything we just talked about
which is I know a ton of content and

343
00:39:27.000 --> 00:39:33.000
very dense and we can apply it to
actually achieving some improvements in
you know some applications that we might

344
00:39:33.000 --> 00:39:38.000
have on the network so just for
background this is a little bit of
science here so I want to be really

345
00:39:38.000 --> 00:39:43.000
clear about how I've done this if you
want to replicate this or try it out
yourself you know I strongly encourage

346
00:39:43.000 --> 00:39:49.000
you if you're concerned about network
performance
heard you to set up a test bed and

347
00:39:49.000 --> 00:39:55.000
really experiment with this so you you
understand how the network is going to
perform with your application so what I

348
00:39:55.000 --> 00:40:02.000
did I did was I just had a pair of
instances I'm running you know the most
recent amazon Linux Tommy and for my

349
00:40:02.000 --> 00:40:09.000
tests i'm just using engine X as a web
server and i'm using the apache bench
tool as my client apache bench is a

350
00:40:09.000 --> 00:40:19.000
pretty simple tool that can concurrently
pull several requests and and repeat
requests i actually did this entirely

351
00:40:19.000 --> 00:40:26.000
well i have four applications three of
them i did with with HTTPS and i'll
explain the fourth in a minute but you

352
00:40:26.000 --> 00:40:32.000
know my theory here is that increasingly
everything needs to be encrypted that is
a meaningful part of our network

353
00:40:32.000 --> 00:40:38.000
connections we need to make sure that we
can not just transmit data but we can do
so securely with with things like TLS so

354
00:40:38.000 --> 00:40:45.000
I'm using that you know TLS v1 some
modern ciphers and for purposes of my
tests I'm just doing random data i'm

355
00:40:45.000 --> 00:40:51.000
transferring random data and I'm
ensuring that I'm not hitting any other
disk i/o I really wanted for this test I

356
00:40:51.000 --> 00:40:59.000
really want to just isolate the network
and so I'm even using a rama fest so all
of the data that's being transmitted is

357
00:40:59.000 --> 00:41:04.000
coming straight out of memory i'm
guaranteeing it's coming out of memory
and then once i'm receiving it I'm not

358
00:41:04.000 --> 00:41:09.000
trying to write it I'm just throwing it
away as I get it so really just to try
to isolate the network performance so

359
00:41:09.000 --> 00:41:16.000
that i can i can be clear what's going
on so if you if you do run apache bench
you'll see it gives you some output this

360
00:41:16.000 --> 00:41:21.000
is the this is just a snippet of the
output there's there's much more here as
well but you know this gives you some

361
00:41:21.000 --> 00:41:27.000
basic metrics about your transmission
how many requests were completed your
average time per request and your

362
00:41:27.000 --> 00:41:32.000
transfer rate so this is the kind of
output that I I was then using to
generate all the metrics you're going to

363
00:41:32.000 --> 00:41:39.000
see here in a moment all right so my
first application that I really wanted
to test was you know I want to see an

364
00:41:39.000 --> 00:41:45.000
example where there's a little bit of
loss you know we saw earlier that the
huge impact that loss can have on your

365
00:41:45.000 --> 00:41:53.000
tcp performance and you know certainly
AWS engineers are networks to have you
know very very low loss and very tight

366
00:41:53.000 --> 00:42:01.000
tolerances on the amount of loss that we
that we are we allow and and we monitor
very that very carefully

367
00:42:01.000 --> 00:42:06.000
but if I'm going out on the internet I'm
you know if I have mobile clients or I
have clients that might be in a remote

368
00:42:06.000 --> 00:42:13.000
geography I really want to make sure
that my application does well even with
a little bit of loss so for this test

369
00:42:13.000 --> 00:42:22.000
I'm basically simulating point2 percent
loss you know which again doesn't sound
very much but just wait so again I have

370
00:42:22.000 --> 00:42:28.000
a pair of instances I'm doing this
across an a path that has around an
email a second round trip time and I'm

371
00:42:28.000 --> 00:42:37.000
setting up this test to just parallel
pull 100 megabyte object you know a few
times and again in Linux I'm actually

372
00:42:37.000 --> 00:42:43.000
simulating the loss here just to make
sure that I am really getting my point
two percent loss that I want and linux

373
00:42:43.000 --> 00:42:48.000
actually makes it super easy if you want
to run this experiment and simulate some
loss and see what happens it's it's

374
00:42:48.000 --> 00:42:54.000
literally one command and you can do
that and so my goal here what I'm trying
to achieve is I want to I want to see

375
00:42:54.000 --> 00:43:00.000
what happens in a clean network and then
I want to add my loss and I want to try
to get my performance back up as close

376
00:43:00.000 --> 00:43:08.000
to that clean network performance as i
can because again point two percent loss
is pretty minimal but I do expect it you

377
00:43:08.000 --> 00:43:13.000
know if I'm serving mobile customers or
going to her mo geography this might
just be a fact of life that I need to

378
00:43:13.000 --> 00:43:21.000
live with all right so what are my
results well I started by doing you know
no loss just at all defaults out of the

379
00:43:21.000 --> 00:43:28.000
box no tweaking what am I getting and
with this test I was achieving around
four gigabits per second and the mean

380
00:43:28.000 --> 00:43:37.000
request time was you know around 28
seconds so pretty good then I introduced
my loss and boom 1.5 gigabits point two

381
00:43:37.000 --> 00:43:45.000
percent loss is all it took now I'm
seeing you know less than half the
performance and it is more than twice as

382
00:43:45.000 --> 00:43:53.000
long mean to to obtain this you know for
this for the requests so clearly loss is
a pretty big impact so I started

383
00:43:53.000 --> 00:43:59.000
experimenting let's I was I started
playing with these and I started I sort
of hypothesized what the problem might

384
00:43:59.000 --> 00:44:06.000
be played around with some settings to
see if it would improve it or not I you
know one of the first things i tried was

385
00:44:06.000 --> 00:44:11.000
i'm just going to increase my initial
congestion window i'm going to going to
try to get more data in the

386
00:44:11.000 --> 00:44:18.000
transmitted earlier maybe prime that
congestion we know it to be a little bit
higher turns out I was wrong it made my

387
00:44:18.000 --> 00:44:26.000
performance worse 1.3 gigabits per
second and now an 81 second meantime so
I got rid of that I scratch that off and

388
00:44:26.000 --> 00:44:32.000
I then I went to another approach I
decided well what if I were to just
double my my buffers on the server side

389
00:44:32.000 --> 00:44:39.000
you know and i will say in this test in
general i was trying to do things mostly
on the server side on the theory that

390
00:44:39.000 --> 00:44:44.000
you know it's way easier for me to tweak
something on the server side then go
tell a bunch of clients to make a change

391
00:44:44.000 --> 00:44:50.000
on their infrastructure so so i try to
do as much as i can on the server in
this case you know i tried to increase

392
00:44:50.000 --> 00:44:59.000
my tcp buffers just to see if you know
if making more data available to the bar
to to the tcp would would help and again

393
00:44:59.000 --> 00:45:08.000
the milk doesn't help ended up being a
little bit worse than my my default with
just some loss so now this this was a

394
00:45:08.000 --> 00:45:15.000
real interesting insight I or the
experiment i hypothesized that maybe if
i use this illinois congestion control

395
00:45:15.000 --> 00:45:23.000
algorithm which speaks to being able to
work around some loss more effectively
and maybe that would help and sure

396
00:45:23.000 --> 00:45:31.000
enough actually this helped a bunch so
again all i did was i swapped out the
default congestion control algorithm in

397
00:45:31.000 --> 00:45:39.000
my linux OS and sure enough got to
three-point got nearly 3.5 gigabits per
second and my meantime was within a

398
00:45:39.000 --> 00:45:47.000
second or actually 300 milliseconds of
where i was before so this this was
pretty meaningful i still have point two

399
00:45:47.000 --> 00:45:55.000
percent loss and my mean time is very
similar to what it was without any loss
so that's my that's my rabbit coming out

400
00:45:55.000 --> 00:46:01.000
of the hat that was my you know that the
congestion control algorithms a little
bit of magic and and I think I found a

401
00:46:01.000 --> 00:46:06.000
rabbit coming out of the Hat in this one
so you know in this case just by turning
this algorithm on using this by default

402
00:46:06.000 --> 00:46:15.000
i increased my performance from that
that initial baseline of loss by by one
hundred thirty-seven percent alright so

403
00:46:15.000 --> 00:46:20.000
so I was pretty excited about that
that's a pretty good result and so I
wanted to try some additional

404
00:46:20.000 --> 00:46:24.000
applications and just see you know what
else I could do with with other
applications so in this case

405
00:46:24.000 --> 00:46:31.000
this application I decided you know it's
a pretty common use case for someone to
want to just transmit a bunch of data

406
00:46:31.000 --> 00:46:36.000
you know bulk amount of data from one
hose to the other and just do it as fast
as possible right the previous one

407
00:46:36.000 --> 00:46:42.000
really was sort of simulating a lot of
clients this one is about you know
larger data set with fewer number of

408
00:46:42.000 --> 00:46:51.000
clients so again i reset my test again
in this test 0 loss so I'm just starting
with you know assuming the network is

409
00:46:51.000 --> 00:46:58.000
operating you know perfectly no laws I
know what can I do what can I achieve I
want to increase my throughput to to the

410
00:46:58.000 --> 00:47:08.000
max so out of the box default I ended up
with getting about two gigabits per
second in this application and it took

411
00:47:08.000 --> 00:47:15.000
about 30 seconds to transfer so I
decided okay I want to go ahead and try
to again I want to do as much as I can

412
00:47:15.000 --> 00:47:22.000
to to manage this from the server end so
I tried to increase my mighty CP buffers
on the server side ended up you know not

413
00:47:22.000 --> 00:47:30.000
doing not doing great i ended up going
backwards so yeah I reduced my
performance then I decided okay this is

414
00:47:30.000 --> 00:47:35.000
a kind of the example where I might have
control of both the sender and the
receiver if I'm actually trying to

415
00:47:35.000 --> 00:47:40.000
transfer a bulk amount of data from
sender to receiver you know maybe that
maybe I'll have control of the the

416
00:47:40.000 --> 00:47:47.000
receiver and can tweak things there so i
went ahead and i increased the tcp
buffers on the client side and again

417
00:47:47.000 --> 00:47:53.000
this would be increasing the receive
window and sure enough that actually
helped that helped quite a bit i got you

418
00:47:53.000 --> 00:48:00.000
know i got an extra 300 megabits per
second and decreased my my mean
transmission time as well so you know

419
00:48:00.000 --> 00:48:06.000
that's one setting that ended up you
know helping me out here I then decided
I wanted to try the active queue

420
00:48:06.000 --> 00:48:12.000
management you know i apophis eyes that
maybe my performance was being limited
by by the you know intermediate router

421
00:48:12.000 --> 00:48:18.000
buffers filling up wanted to give active
queue management a try now to be clear
I've tried this in isolation so this is

422
00:48:18.000 --> 00:48:24.000
active queue management in isolation
without the extra buffers on the client
side and and you can see that actually

423
00:48:24.000 --> 00:48:28.000
you know that that actually also
increases performance it actually does
increase performance over the baseline

424
00:48:28.000 --> 00:48:35.000
though not as much as doubling the TCP
buffers on the client and did as well so
I've got 22

425
00:48:35.000 --> 00:48:45.000
things now to changes that have both
increased performance and so then i said
ok well let's turn both on a natural

426
00:48:45.000 --> 00:48:50.000
next step is I got two things that seem
to do have a little bit of improvement
let's turn them both on together and

427
00:48:50.000 --> 00:48:57.000
sure enough fatty that increased my
performance even more now I'm up to 2.7
gigabits per second and 24 seconds of 24

428
00:48:57.000 --> 00:49:04.000
half seconds meantime using the two of
these settings together and then I
decided all right I've had some success

429
00:49:04.000 --> 00:49:08.000
with this Illinois congestion control
algorithm I don't have a bunch a loss
here but I'm curious what it does I

430
00:49:08.000 --> 00:49:14.000
turned that on as well with the client
buffers and queue management and now i'm
getting i got myself an extra hundred

431
00:49:14.000 --> 00:49:20.000
megabytes per second as i as i turned
this on and reduce my meantime even
further so now i'm actually transmitting

432
00:49:20.000 --> 00:49:28.000
the same amount of data in you know six
and a half seconds less than i did
previously sona again pulled my little

433
00:49:28.000 --> 00:49:35.000
rabbit out of the Hat and then finally I
decided well you know I got all three of
those turned on those are working pretty

434
00:49:35.000 --> 00:49:41.000
well what if I now increase the TCP
buffer on the server side just to give
myself a little bit of extra capacity on

435
00:49:41.000 --> 00:49:47.000
the server side for transmission and
that ended up in you know what with the
other parameters all turned on that

436
00:49:47.000 --> 00:49:52.000
ended up bumping me up just a little bit
more although it actually interesting
they bumped up my bandwidth performance

437
00:49:52.000 --> 00:49:58.000
but ended up my meantime actually snuck
up a little bit I think we saw a few
additional outliers in this because of

438
00:49:58.000 --> 00:50:06.000
turning on the server side buffers but
all told you know with with some tweaks
again none all of this is really out of

439
00:50:06.000 --> 00:50:13.000
the box i was able to tweak this out of
the box ended up increasing my network
performance by thirty two percent all

440
00:50:13.000 --> 00:50:20.000
right two additional applications that i
wanted to want to talk about zero wanted
to test so the next is I decided that I

441
00:50:20.000 --> 00:50:26.000
really wanted to test you know the
previous the previous example was over a
long rtt path as we talked about earlier

442
00:50:26.000 --> 00:50:33.000
that's where those extra buffers really
matter but what I wanted to do now was
to test it on a low RTT path where both

443
00:50:33.000 --> 00:50:41.000
instances might be in the same VPC and
so in this case I had two instances they
add about a 1.2 millisecond round trip

444
00:50:41.000 --> 00:50:47.000
time and I just wanted to see you know
really what what was the I expected that
the MTU would have a pretty significant

445
00:50:47.000 --> 00:50:52.000
impact
to your you know how big of an impact
with the mt you really have so what I

446
00:50:52.000 --> 00:50:58.000
did was I actually started by just
taking the defaults out of the box and
putting in place a 1500 bite em to you

447
00:50:58.000 --> 00:51:04.000
because you know again although this is
not the default at this point in v pc i
did want to see you know what's the

448
00:51:04.000 --> 00:51:10.000
difference in the MTU setting how does
that change things so out of the box
with 15 herb item to you it got pretty

449
00:51:10.000 --> 00:51:18.000
good performance already 8.8 gigabits
per second on an instance that has a 10
gig Nick that's that's pretty good and

450
00:51:18.000 --> 00:51:24.000
then if I go back to that 9001 bite em
to you though I see that now I do bump
up I do get a little bit of boost in

451
00:51:24.000 --> 00:51:31.000
performance by reducing the amount of
overhead in my in every single packet I
get my performance up to about 9.3

452
00:51:31.000 --> 00:51:38.000
gigabits per second and then I went
ahead and tried you know how much does
active queue management work when I'm

453
00:51:38.000 --> 00:51:44.000
you know operating in this low RTT
environment in the same data center and
as it turned out basically zero didn't

454
00:51:44.000 --> 00:51:51.000
didn't actually help me very much given
how you know these are two instances
with a low RTT operating in the same

455
00:51:51.000 --> 00:51:58.000
availability zone ended up active queue
management didn't play a cute huge role
here but you know again with with just

456
00:51:58.000 --> 00:52:03.000
by having that larger mt you ended up
with about a five percent increase in
performance over the the baseline 1500

457
00:52:03.000 --> 00:52:14.000
bytes so again although the 9000 one
byte MTU really is the default today in
some cases I know in some cases

458
00:52:14.000 --> 00:52:18.000
customers or engineers have thought
maybe I should tune back to 15 under
bite em to you since the sort of the

459
00:52:18.000 --> 00:52:24.000
Internet at large operates at that and
and you know that is in some cases that
might be the right decision but it does

460
00:52:24.000 --> 00:52:34.000
but we do end up with increased
performance using the larger 9001 bite
em to you all right so last last test I

461
00:52:34.000 --> 00:52:42.000
wanted to do now for this test i wanted
to simulate something that had simulate
a web service that was transmitting a

462
00:52:42.000 --> 00:52:48.000
very small amount of data where the
payload was very small the response size
was very small because particularly i

463
00:52:48.000 --> 00:52:52.000
wanted to see what are the impacts of
the initial congestion window and would
that really meaningfully change

464
00:52:52.000 --> 00:53:01.000
performances i as i tweaked that so you
know in the previous test I did HTTPS
the major so

465
00:53:01.000 --> 00:53:07.000
you do a lot of background reading on
https but essentially it boils down to
now being a case where HTTPS does

466
00:53:07.000 --> 00:53:13.000
require a few round trips on initial
connection establishment just to get
https established so I wanted to

467
00:53:13.000 --> 00:53:20.000
simulate something where there you know
literally the entire response size could
fit into the initial packets that are

468
00:53:20.000 --> 00:53:28.000
being sent from server to client and so
I couldn't do that with HTTPS I did that
with HTTP and interestingly enough hdtb

469
00:53:28.000 --> 00:53:36.000
2 point 0 seems like it will actually
allow me potentially to run the same
test with with HTTP with with TLS

470
00:53:36.000 --> 00:53:44.000
enabled but we're not quite there yet so
so I ran this with HTTP and real and
again so my object size was 10k I just

471
00:53:44.000 --> 00:53:51.000
want to transfer a 10k object as to as
many parallel clients in this case of
6,400 simulated parallel clients and I

472
00:53:51.000 --> 00:53:59.000
just want to transfer this with his low
latency as possible so again i started
with out-of-the-box ended up get seeing

473
00:53:59.000 --> 00:54:05.000
about 2.6 gigabits of total throughput
but really in this case I was focused on
the meantime of 195 milliseconds you

474
00:54:05.000 --> 00:54:15.000
know to to transmit them to respond to
my client like my my client simulated
clients so then I decided to go ahead

475
00:54:15.000 --> 00:54:21.000
and bump up the initial congestion
window again I wanted to see if my
default congestion window would be less

476
00:54:21.000 --> 00:54:27.000
than that 10k object size but now with
this this increase can initial
congestion window it will be larger my

477
00:54:27.000 --> 00:54:33.000
initial congestion window will be larger
than 10k and sure enough that actually
does mean you know does improve

478
00:54:33.000 --> 00:54:43.000
performance I my bandwidth increase to
2.7 gigabits and I shaved up five and a
half milliseconds off the response time

479
00:54:43.000 --> 00:54:50.000
so you know a win for my initial
congestion window I then decided I
wanted to go ahead and turn on I wanted

480
00:54:50.000 --> 00:54:55.000
to try my Illinois congestion control
algorithm see what sort of changed that
that had on it and although the end of

481
00:54:55.000 --> 00:55:02.000
it ended up reducing sort of the the
peak bandwidth we saw it did actually
reduce the the meantime I think we we

482
00:55:02.000 --> 00:55:09.000
pulled off some of the outliers on the
on the transfer time here by turning
this on so that was a little bit of a

483
00:55:09.000 --> 00:55:14.000
win and you know in total i achieved
about a four point six percent decrease
but

484
00:55:14.000 --> 00:55:21.000
tuning these two settings so you know
decreased our our response time bye bye
four point six percent and if you

485
00:55:21.000 --> 00:55:26.000
actually look at the meantime what's
interesting about this is again this was
done over an 80 ml a second round trip

486
00:55:26.000 --> 00:55:34.000
time so the the mean time is up is sort
of about two and a half round trips
which is pretty good I mean that implies

487
00:55:34.000 --> 00:55:41.000
that that I'm establishing a TCP
connection and transferring data and
closing that with about two and a half

488
00:55:41.000 --> 00:55:50.000
round trip times on average so pretty
happy with these results alright so that
was the these were the four experiments

489
00:55:50.000 --> 00:55:57.000
I ran and I just want to kind of
summarize all you know the content of
this talk you know hey oftentimes I

490
00:55:57.000 --> 00:56:02.000
think you know even I can think about
the networks as being a little bit of a
black box if I'm seeing a performance

491
00:56:02.000 --> 00:56:08.000
issue you know how do i really know
what's going on and and what's impacting
my performance and you know I think that

492
00:56:08.000 --> 00:56:13.000
as I showed with some of the tools
earlier the network really doesn't have
to be a black box you can interrogate it

493
00:56:13.000 --> 00:56:19.000
and get the you know figure out exactly
what's going on figure out what timers
are being used and what retransmissions

494
00:56:19.000 --> 00:56:25.000
are being seen and how that's impacting
your performance I you know and then I
think I hope I've demonstrated that

495
00:56:25.000 --> 00:56:31.000
through a few simple changes to the
parameters that are used with TCP you
can actually see meaningful improvements

496
00:56:31.000 --> 00:56:38.000
in performance and really what this you
know the way I did this was just by
setting up with some testbed testing it

497
00:56:38.000 --> 00:56:44.000
measuring my results and then tweaking
one parameter at a time and seeing you
know how my performance changed over

498
00:56:44.000 --> 00:56:48.000
time so I strongly encourage you you
know if you're if you're worried about
network performance you know understand

499
00:56:48.000 --> 00:56:56.000
what is your workload how does it use
the network and you know tune your
network stack accordingly so please

500
00:56:56.000 --> 00:57:01.000
complete the evaluations for this course
they help us improve the content and
keep it high quality at reinvent and
really appreciate you coming to the talk
so thank you