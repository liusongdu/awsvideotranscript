WEBVTT FILE

1
00:00:00.000 --> 00:00:07.000
hello welcome to this AWS webinar my
name's Ian Massingham I'm a Technical
Evangelist with amazon web services

2
00:00:07.000 --> 00:00:15.000
based in Europe I'm going to be hosting
today's session which is a master class
series webinar focusing on Amazon

3
00:00:15.000 --> 00:00:21.000
redshift so our fast petabytes scale
data warehouse service the master class
series of webinars are intended to

4
00:00:21.000 --> 00:00:28.000
provide a deep dive into specific AWS
services that complemented by another
series that we run called the journey

5
00:00:28.000 --> 00:00:35.000
through the cloud journey through the
cloud is a solutions orientated look at
how you can apply AWS services to solve

6
00:00:35.000 --> 00:00:41.000
particular use cases or deal with
different business challenges that you
might face you can find links to both

7
00:00:41.000 --> 00:00:48.000
webinar series in the links panel you
can see in the webinar interface if
you're watching this live or in the

8
00:00:48.000 --> 00:00:55.000
video description if you're watching
this webinar on the AWS webinars youtube
channel also if you're watching live you

9
00:00:55.000 --> 00:01:02.000
can see a files panel and there is
available a PDF of the materials from
today's session of advise downloading

10
00:01:02.000 --> 00:01:06.000
that if you're watching the session live
you'll find there were many links on
quite a few of the slides that we're

11
00:01:06.000 --> 00:01:13.000
going to be covering today where you can
find further information also links to
relevant documentation on the topics

12
00:01:13.000 --> 00:01:19.000
that we're going to be covering today
you can also find the materials on
SlideShare and if you're watching on

13
00:01:19.000 --> 00:01:26.000
demand you will find a link to the
relevant SlideShare presentation in the
video description on YouTube the other

14
00:01:26.000 --> 00:01:32.000
thing that you can see only in the live
interface is a Q&A panel and we're
unlikely to have time to answer

15
00:01:32.000 --> 00:01:39.000
questions at the end of today's session
so if you do have any questions please
do submit these using that Q&A panel and

16
00:01:39.000 --> 00:01:44.000
I will make sure that a member of our
solutions architecture team follows up
with you in email over the course of the

17
00:01:44.000 --> 00:01:51.000
next couple of weeks we do answer all
questions from all webinars so if you do
have questions don't worry if we don't

18
00:01:51.000 --> 00:01:56.000
get to them during today's session we
will cover them after the event also
show you a couple of social media links

19
00:01:56.000 --> 00:02:02.000
at the end of today's session where
you'll be able to stay up to date with
AWS globally and also AWS here in the UK

20
00:02:02.000 --> 00:02:06.000
and Ireland so have twitter ready for
that at the end of the session and
lastly at the very end of the session

21
00:02:06.000 --> 00:02:12.000
will also put the webinar into a Q&A and
feedback mode where you'll be able to
give us a rate

22
00:02:12.000 --> 00:02:17.000
from one to five with five being the
best let us know how we've done give us
some feedback and help us optimize this

23
00:02:17.000 --> 00:02:25.000
webinar for future audiences that might
watch it I just like to say that we also
present the two webinar series both the

24
00:02:25.000 --> 00:02:31.000
master class series and the journey
through the cloud series in conjunction
with Intel great AWS technology partner

25
00:02:31.000 --> 00:02:38.000
we'd like to thank Intel for their
support in helping us run this webinar
program during 2015 as I said if you're

26
00:02:38.000 --> 00:02:44.000
interested in the back webinars that
we've already recorded so far this year
take a look at the series pages that you

27
00:02:44.000 --> 00:02:49.000
can see in that links panel in the
webinar interface and catch up with the
stuff that we've already covered over

28
00:02:49.000 --> 00:02:55.000
the course of 2015 ok that's enough
housekeeping let's move on now to
today's content so this is a master

29
00:02:55.000 --> 00:03:02.000
class series webinar it's a technical
deep dive that's going to be a go beyond
the basics on one specific AWS service

30
00:03:02.000 --> 00:03:07.000
its intended to educate you have to get
the best out of the service that we're
covering show you how things work and

31
00:03:07.000 --> 00:03:15.000
illustrate how you can get things done
with one specific AWS surveys and today
we're covering Amazon redshift it's a

32
00:03:15.000 --> 00:03:22.000
fast fully managed petabyte scale data
warehouse that runs in the AWS cloud it
makes it simple and cost-effective for

33
00:03:22.000 --> 00:03:30.000
you to analyze all your data you can
continue to use existing business
intelligence ETL and visualization tools

34
00:03:30.000 --> 00:03:37.000
and it's a very scalable service that
enables you to start small and scale
well beyond the petabyte with current

35
00:03:37.000 --> 00:03:44.000
limits and it costs less than one
thousand dollars per terabyte per year
to operate this is roughly one-tenth the

36
00:03:44.000 --> 00:03:51.000
cost of traditional on-premises data
warehouse TCO it's a very
cost-effective way to get access to high

37
00:03:51.000 --> 00:03:58.000
performance and high quality analytical
services that you can use to turn your
organization into a data-driven

38
00:03:58.000 --> 00:04:04.000
enterprise the fundamental
characteristics of the survey as well as
you might expect for a data warehouse

39
00:04:04.000 --> 00:04:12.000
the services optimized precisely for the
data warehousing use case it's capable
of providing very high query performance

40
00:04:12.000 --> 00:04:18.000
on data sets that might range from 100
gigabytes to well over a petabyte it
uses modern data warehousing

41
00:04:18.000 --> 00:04:25.000
architectural approaches like column
storage data compression and zone
mapping to reduce the amount of I

42
00:04:25.000 --> 00:04:31.000
that is needed to perform queries and
can offer significantly about the better
performance than trying to put OLTP

43
00:04:31.000 --> 00:04:38.000
optimized where warehouses to use so
databases to use in this use case
provide an MPP architecture massively

44
00:04:38.000 --> 00:04:44.000
parallel processing architecture which
enables the system to paralyze and
distribute SQL operations taking

45
00:04:44.000 --> 00:04:49.000
advantage of all the resources that are
available in your cluster and the
underlying hardware itself is designed

46
00:04:49.000 --> 00:04:56.000
for high performance data processing
using features such as locally attached
storage to maximize throughput between

47
00:04:56.000 --> 00:05:03.000
CPUs and drives and a 10 gigabit
ethernet mesh network to maximize
throughput between nodes like all AWS

48
00:05:03.000 --> 00:05:12.000
services you can control amazon redshift
using the AWS api this is a feature that
you can use to very easily change the

49
00:05:12.000 --> 00:05:18.000
number or type of nodes in your data
warehouse in accordance with your
capacity needs there also a couple of

50
00:05:18.000 --> 00:05:22.000
different node types that we're going to
cover in a little bit more detail during
today's session that allow you to

51
00:05:22.000 --> 00:05:31.000
optimize around IO and cpu performance
or around capacity you can also resize
clusters as well show you towards the

52
00:05:31.000 --> 00:05:38.000
end of today's session on demand this is
a non-disruptive operation that allows
you to seamlessly migrate data between

53
00:05:38.000 --> 00:05:44.000
old and new cluster with different
dimensions doesn't interrupt query
performance while you do that it's also

54
00:05:44.000 --> 00:05:51.000
a cost-effective service once again like
all AWS services you can choose
on-demand pricing with no upfront costs

55
00:05:51.000 --> 00:05:57.000
or no long-term commitments or you can
optimize your costing so cost
significantly by using discounting

56
00:05:57.000 --> 00:06:02.000
that's available with reserved instance
pricing I'm going to cover pricing in
detail today but if you want to learn

57
00:06:02.000 --> 00:06:07.000
more about that visit the Amazon
redshift pricing page that will show you
towards the end of the session once

58
00:06:07.000 --> 00:06:13.000
again today simple service that enables
you to get started very quickly and it's
fully managed and this management

59
00:06:13.000 --> 00:06:20.000
service also covers aspects of fault
tolerance so if nodes in your cluster do
fail we will transparently replace those

60
00:06:20.000 --> 00:06:26.000
for you we take steps to protect your
data both with replication in the
cluster as well as with continuous and

61
00:06:26.000 --> 00:06:33.000
automated backups that are taking place
and will show you a process for
restoring from backup later in today's

62
00:06:33.000 --> 00:06:38.000
session the I encryption features so you
can take advantage of several different
AWS

63
00:06:38.000 --> 00:06:46.000
encryption services network isolation so
you can place your Amazon redshift
cluster inside an Amazon VPC or virtual

64
00:06:46.000 --> 00:06:53.000
private cloud connect it to your own
existing IT infrastructure using an
industry standard ipsec VPN or our

65
00:06:53.000 --> 00:07:00.000
direct connect service and then lastly
for compatibility it's an SQL data
warehousing solution li users industry

66
00:07:00.000 --> 00:07:09.000
standard ODBC and jdbc connections you
can download our custom JDBC and odbc
drivers from the client connect tab in

67
00:07:09.000 --> 00:07:15.000
the redshift console and from there
you'll be able to use popular business
intelligence ETL and visualization tools

68
00:07:15.000 --> 00:07:22.000
many of which have been certified by
their vendors to provide full support
for Amazon redshift there's integration

69
00:07:22.000 --> 00:07:29.000
with other AWS services as well as these
industry standard connectivity
mechanisms enabling you to parallel load

70
00:07:29.000 --> 00:07:38.000
data for example from s3 DynamoDB or
from other AWS data sources that you may
be taking advantage of so it's a very

71
00:07:38.000 --> 00:07:45.000
very capable service and it's a service
which has been around a little while we
launched amazon redshift during november

72
00:07:45.000 --> 00:07:54.000
2012 and as the original post here from
jeff bars blog back in 2012 where he
announced the launch of the service also

73
00:07:54.000 --> 00:08:02.000
explaining that customers like Netflix
nasa/jpl and Flipboard had been testing
redshift is part of a private beta and

74
00:08:02.000 --> 00:08:10.000
in November 2012 we launched a limited
public beta for the service so we've
been in operations with Amazon redshift

75
00:08:10.000 --> 00:08:19.000
for coming up to three years now and it
was at the time it was launched the most
rapidly adopted AWS service in history

76
00:08:19.000 --> 00:08:27.000
and that rapid adoption has enabled us
to iterate on the service pretty rapidly
and pretty successfully over the period

77
00:08:27.000 --> 00:08:34.000
since it was launched in 2012 we'll come
back to this most recent function most
recent feature on user-defined functions

78
00:08:34.000 --> 00:08:39.000
towards the end of the session to look
talk a little bit more about this but if
you are interested in keeping up with

79
00:08:39.000 --> 00:08:47.000
enhancements to Redshift over time you
may want to bookmark the URL you can see
on this slide which is the Amazon

80
00:08:47.000 --> 00:08:53.000
redshift category on the official AWS
blog and you'll find there are many
many updates to these services we add

81
00:08:53.000 --> 00:08:58.000
new features that have been requested
and back prioritized by customers and of
course if there's something that you

82
00:08:58.000 --> 00:09:04.000
feel is missing from Redshift today
then do contact us via your solutions
architect or via your account manager

83
00:09:04.000 --> 00:09:11.000
and let us know how we can improve the
service for you make it more suitable
for your specific use case always

84
00:09:11.000 --> 00:09:17.000
interested in getting feedback on how we
can improve existing services so what do
we have for you today in terms of the

85
00:09:17.000 --> 00:09:23.000
agenda first of all we're going to talk
about why you might run your data
warehouse on AWS as an alternative to

86
00:09:23.000 --> 00:09:29.000
other options that of course available
to you talk about getting started a
little bit on table design and data

87
00:09:29.000 --> 00:09:36.000
loading talk about how you can work with
data what kind of tools you can use for
that then we'll talk about backup and

88
00:09:36.000 --> 00:09:42.000
restoration upgrading and scaling
scaling and we'll close out by talking
about some resources that you can use to

89
00:09:42.000 --> 00:09:52.000
continue to build your knowledge after
today's session so why run your data
warehouse on AWS well in some ways there

90
00:09:52.000 --> 00:09:58.000
are parallels between traditional
database management services that you
might provide on your on-premises so on

91
00:09:58.000 --> 00:10:07.000
IT infrastructure that you operate and
with operating data warehouses in that
way they're typically expensive the

92
00:10:07.000 --> 00:10:13.000
difficult and time-consuming to
administer they are challenging to scale
and often scaling these systems can

93
00:10:13.000 --> 00:10:20.000
require very expensive upgrades as well
as a significant investment in
resourcing at your own resourcing or

94
00:10:20.000 --> 00:10:27.000
consulting resourcing to scale up and
replace data warehouses as they grow and
then fourthly there's quite a bit of

95
00:10:27.000 --> 00:10:33.000
locking with these systems part of that
is actually to do with cost customers
are often reluctant to change because of

96
00:10:33.000 --> 00:10:42.000
the significant capital investment
involved in acquiring implementing and
operating databases and most most more

97
00:10:42.000 --> 00:10:48.000
specifically data warehousing services
we're trying to tackle some of these
challenges when we create a damaged and

98
00:10:48.000 --> 00:10:54.000
Redshift in response to the fact the
customers were asking us to deliver data
warehousing in a way which was

99
00:10:54.000 --> 00:11:00.000
consistent with the way in which we
deliver the services so easy to
provision and easy to scale up

100
00:11:00.000 --> 00:11:07.000
eliminating those upfront costs and
available at a pay-as-you-go model with
of course the opportunity to further

101
00:11:07.000 --> 00:11:13.000
optimize costs if you want to make a
resource reservation with reserved
instances offering really fast

102
00:11:13.000 --> 00:11:19.000
performance and being open and flexible
with support for popular bi tools
enabling you to get your data in and out

103
00:11:19.000 --> 00:11:29.000
of Amazon redshift in a very simple and
familiar way and the architecture of the
system is such that Amazon redshift

104
00:11:29.000 --> 00:11:35.000
paralyzes and distributes all operations
that take place within the cluster the
cluster comprises a leader node where

105
00:11:35.000 --> 00:11:44.000
you make your JDBC or odbc connection
and then the leader node coordinates
activity Vivat e a cluster across a

106
00:11:44.000 --> 00:11:51.000
cluster comprising a number of compute
nodes and these compute nodes as we said
earlier are connected together by this

107
00:11:51.000 --> 00:12:00.000
10 Gigabit Ethernet mesh and take
advantage of local storage to deliver
optimal IO throughput and we provide a

108
00:12:00.000 --> 00:12:08.000
framework automation framework that
enables you to query load back up with
store and resize both the cluster and

109
00:12:08.000 --> 00:12:14.000
data that's stored within the cluster in
a very very seamless manner very simple
well we'll cover many of these back

110
00:12:14.000 --> 00:12:25.000
aspects in more detail during today's
session on scaling you have several
options firstly there are different

111
00:12:25.000 --> 00:12:33.000
types of compute nodes that you can use
to build your Amazon redshift cluster
these comprise dc1 nodes which are

112
00:12:33.000 --> 00:12:41.000
dense compute nodes available in two
sizes the dc1.large nodes
comprise 15 gigs of RAM, 2 virtual cores

113
00:12:41.000 --> 00:12:49.000
they have 10 Gigabit Ethernet
interconnect and they have 160 gigs of
solid-state capacity in each node so by

114
00:12:49.000 --> 00:12:56.000
using a single node you can access 160
gigabytes of SSD capacity and then
scaling with a cluster from 2 to

115
00:12:56.000 --> 00:13:03.000
32 nodes you can take advantage
of between 320 gigabytes and just over
5TB of solid-state capacity

116
00:13:03.000 --> 00:13:13.000
with those DC 1 nodes you then have the
dc1.8xlarge nodes here you have much
more memory 244 gigs per node, 32 virtual

117
00:13:13.000 --> 00:13:20.000
cores per node and the cluster
size is significant
large as well so you can go from 2 to 128

118
00:13:20.000 --> 00:13:28.000
nodes and 128 notes will give you up to
326TB of solid state storage
capacity within your cluster so you're

119
00:13:28.000 --> 00:13:35.000
going to find these are optimized for
smaller data sets and higher performance
in terms of data throughput off those

120
00:13:35.000 --> 00:13:43.000
drives solid-state drives by virtue of
their fundamental characteristics offer
higher performance and lower latency IO

121
00:13:43.000 --> 00:13:50.000
response then if you need to have a
larger cluster capable of taking more
data so working with larger data sets

122
00:13:50.000 --> 00:13:58.000
here you have the ds2.8xlarge nodes
these are rotational magnetic storage
equipped so I don't have solid state

123
00:13:58.000 --> 00:14:05.000
drives in them they have traditional
rotational hard disks in them still
carrying 244 gigs of ram but here you

124
00:14:05.000 --> 00:14:13.000
have 36 virtual cores to work with 10 GigE interconnect of course
and 16TB of magnetic rotational

125
00:14:13.000 --> 00:14:21.000
hard disk capacity per node you can
cluster between 2 and 128 nodes in
this instance which delivers up to two

126
00:14:21.000 --> 00:14:29.000
terabytes of magnetic rotational hard
disk capacity for your cluster so for
working with very large data sets the

127
00:14:29.000 --> 00:14:37.000
ds2.8xlarge is an excellent answer back
to that question we've done some work as
i said earlier to provide an automation

128
00:14:37.000 --> 00:14:44.000
framework which helps make it easier for
you to run an operate amazon redshift
clusters includes features like built-in

129
00:14:44.000 --> 00:14:51.000
security so in transit rest and also a
mag backed up using encryption back up
to amazon s3 which is continuous

130
00:14:51.000 --> 00:14:57.000
incremental and automatic and we'll show
you how to restore from backup slater on
in today's session this streaming

131
00:14:57.000 --> 00:15:05.000
restore methodology that's used and now
allows you to resume querying more
rapidly when restoring a cluster from

132
00:15:05.000 --> 00:15:14.000
backup and we also help deal with
failure modes both disk failure modes
and node failures recovering nodes and

133
00:15:14.000 --> 00:15:21.000
replacing drives automatically so that
you don't have to take any action in the
event of these failure scenarios so the

134
00:15:21.000 --> 00:15:27.000
typical architecture would be your
clients of course communicating with
your JDBC or odbc endpoint and then

135
00:15:27.000 --> 00:15:33.000
transparently in the background
Amazon redshift cluster is being
protected and data has been stored

136
00:15:33.000 --> 00:15:40.000
automatically on Amazon s3 with the
familiar 99.999999999% of durability that
that offers and of course the facility

137
00:15:40.000 --> 00:15:48.000
to keep your data in a
specific AWS region the specified so we
won't move data into regional su

138
00:15:48.000 --> 00:15:55.000
instruct us to do so using one of our
tools that make available and this of
course is a mechanism for ensuring that

139
00:15:55.000 --> 00:16:02.000
you're not moving data outside of
regions which may breach regulatory or
compliance obligations that are placed

140
00:16:02.000 --> 00:16:09.000
upon you so the standard AWS security
model applies to Amazon which redshift
just as it does to any other service

141
00:16:09.000 --> 00:16:17.000
that we provide to you use some
techniques that have become more common
now actually in columnar MPP data

142
00:16:17.000 --> 00:16:23.000
warehouses but we're actually pretty
revolutionary when they were first
dreamed up first invented so the idea

143
00:16:23.000 --> 00:16:32.000
that you can store data in a column not
orientated format using compression
using zone maps so this is a feature

144
00:16:32.000 --> 00:16:41.000
whereby each node knows the range of
values that are stored upon it and it
can therefore we only respond to queries

145
00:16:41.000 --> 00:16:47.000
needs to respond to queries where it's
been asked for data that is present on
the node and this can dramatically

146
00:16:47.000 --> 00:16:56.000
improve query performance by minimizing
IO IO however is accelerated through the
use of direct attached storage and also

147
00:16:56.000 --> 00:17:04.000
accelerated for large scale or large
reads through the use of
large data block sizes so whole system

148
00:17:04.000 --> 00:17:11.000
is optimized for read intensive scan
based workload such as the type that
are typical when performing queries on

149
00:17:11.000 --> 00:17:18.000
data so old systems built for that and
the fact that the system is built
specifically for optimizing query based

150
00:17:18.000 --> 00:17:23.000
workloads has led to some really good
results for customers this is a super
quiet really like this from jana Donovan

151
00:17:23.000 --> 00:17:31.000
was the CTO at the Financial Times talks
about implementing Amazon redshift as a
way for them to increase the speed

152
00:17:31.000 --> 00:17:36.000
performance and flexibility of data
analysis and when implementing redshift
they found that they achieved a

153
00:17:36.000 --> 00:17:42.000
ninety-eight percent performance
improvement in their query runtimes fact
the performance was so

154
00:17:42.000 --> 00:17:50.000
John and his team first glance thought
the system wasn't functioning because it
was so quick they then ran this system

155
00:17:50.000 --> 00:17:57.000
in parallel with their pre-existing
warehousing solution and they verified
that the queries were performing quickly

156
00:17:57.000 --> 00:18:02.000
they were just performing extremely fast
because of some of those optimization
techniques that I talked about a few

157
00:18:02.000 --> 00:18:08.000
minutes ago they're also able to reduce
their infrastructure costs by over
80% in comparison to a

158
00:18:08.000 --> 00:18:15.000
traditional data center based model so
real strong success story for the FT and
at the end of the session will show you

159
00:18:15.000 --> 00:18:22.000
a link where you can learn more about
this case study and also other customers
that have chosen to run data warehouses

160
00:18:22.000 --> 00:18:31.000
on Amazon redshift let's move
on now and take a look at getting
started and using the AWS console to

161
00:18:31.000 --> 00:18:38.000
access and create a redshift cluster is
very simple so in your typical AWS
console view you'll find that redshift

162
00:18:38.000 --> 00:18:43.000
the sub console for this specific
service is organized in the database
category and if you click on that you'll

163
00:18:43.000 --> 00:18:49.000
be taken to one or two views if you've
never created a redshift cluster before
you'll see this which tells you a little

164
00:18:49.000 --> 00:18:55.000
bit about the service and gives you
access to some resources that you can
use to learn a little bit more about how

165
00:18:55.000 --> 00:19:02.000
you might make use of Amazon redshift if
you have used the service before you'll
see this screen which is the cluster the

166
00:19:02.000 --> 00:19:08.000
redshift cluster dashboard and in either
event you're going to click on that
launch cluster button and you'll then be

167
00:19:08.000 --> 00:19:16.000
taken into the two familiar AWS
wizard-based work through to create a
new service instance he has to populate

168
00:19:16.000 --> 00:19:22.000
some information about your cluster and
identify a name for the database that
you wish to use the database port that

169
00:19:22.000 --> 00:19:30.000
you wish to make use of a user name and
a master user password that you're asked
to set click continue or into those

170
00:19:30.000 --> 00:19:37.000
fields rather and click continue and you
can then specify the dimensions of your
cluster by choosing your node type and

171
00:19:37.000 --> 00:19:45.000
by selecting whether you want to have a
single node or multi node cluster and
specifying the number of compute nodes

172
00:19:45.000 --> 00:19:51.000
that you wish to comprise your cluster
you can enter those values here click
continue once again and then you'll be

173
00:19:51.000 --> 00:19:58.000
asked to enter additional configuration
details and this is where you can do
things like set security parameters for

174
00:19:58.000 --> 00:20:05.000
your redshift cluster for example
specifying how you wish to have your
database encrypted you're going to 

175
00:20:05.000 --> 00:20:13.000
make use typically of the Amazon Web Services
key management service here or a cloudHSM
a hardware security module 

176
00:20:13.000 --> 00:20:21.000
if you have one of those configured and in use
in your account in the specific region
you've selected you can also use a hsm

177
00:20:21.000 --> 00:20:30.000
kms beyond the scope of this webinar if
you want to learn more about kms and HSM
visit the AWS Security Center at

178
00:20:30.000 --> 00:20:39.000
awsamazon.com/security and you can find
information about both of those services
there you're also configuring networking

179
00:20:39.000 --> 00:20:46.000
options here choosing which VPC you wish
to deploy the service into which will
place it into a specific subnet and also

180
00:20:46.000 --> 00:20:55.000
into a specific availability zone when
you select the specific subnet group
that you wish to deploy into you can

181
00:20:55.000 --> 00:21:01.000
specify whether you want the cluster to
be accessible from the public internet
or whether you want it to be accessible

182
00:21:01.000 --> 00:21:09.000
only from within your private VPC you
can specify a public IP address if you
wish to do so or not if you wish to keep

183
00:21:09.000 --> 00:21:16.000
your address privately rootable address
only and then you can specify the
availability zone that you wish to place

184
00:21:16.000 --> 00:21:23.000
in and then specify VPC security groups
that you wish to associate with this
cluster and these VPC security groups

185
00:21:23.000 --> 00:21:29.000
are important because these are the
security groups that will allow your
clients add to communicate with your

186
00:21:29.000 --> 00:21:36.000
cluster so you're going to set a
security policy on those security groups
that allows set of client hosts to

187
00:21:36.000 --> 00:21:43.000
communicate with your cluster and
exchange traffic with it this stage you
can also create and configure a cloud

188
00:21:43.000 --> 00:21:48.000
watch alarm if you select the option
you'll get this additional drop down
here and this allows you to specify a

189
00:21:48.000 --> 00:21:55.000
disk usage alarm for your cluster
publishing that to an existing SNS topic
and sending yourself

190
00:21:55.000 --> 00:22:02.000
an email when the cluster reaches the
particular disk usage threshold that
you've set it's a good idea to do that

191
00:22:02.000 --> 00:22:08.000
of course if you're going to load data
into your classics it will enable you to
manage scenarios

192
00:22:08.000 --> 00:22:15.000
where you run out of cluster storage and
need to potentially resize your cluster
if you hit continue at that point you'll

193
00:22:15.000 --> 00:22:22.000
be asked to review what you've specified
you'll get a warning here if you're not
applicable or eligible for the free

194
00:22:22.000 --> 00:22:31.000
trial and this will specify what your
hourly on demand rate for your cluster
will be also offers some guidance as the

195
00:22:31.000 --> 00:22:36.000
way you can access documentation about
reserve nodes in order to optimize your
costs if you're going to win a cluster

196
00:22:36.000 --> 00:22:42.000
you at all concerned about optimizing
your costs then check out reservation
options that are available to you to

197
00:22:42.000 --> 00:22:48.000
minimize that and also make sure you
familiar with the Amazon redshift
pricing that you can see linked out at

198
00:22:48.000 --> 00:22:55.000
the bottom of that screen as well once
you hit launch cluster you then told
that your cluster is being created by

199
00:22:55.000 --> 00:23:02.000
this pop-up and if you close that you'll
be taken to the clusters dashboard where
you can see the status of your cluster

200
00:23:02.000 --> 00:23:09.000
as it's being created you can also of
course at this point click on the
cluster name and here you can see

201
00:23:09.000 --> 00:23:15.000
additional information about the cluster
while it's under creation after a few
minutes and it typically takes around 10

202
00:23:15.000 --> 00:23:23.000
minutes to create an Amazon redshift
cluster your cluster status will switch
to available your DB health will switch

203
00:23:23.000 --> 00:23:29.000
to healthy and you'll show that you've
come out of maintenance mode and if you
click on the cluster again at this point

204
00:23:29.000 --> 00:23:38.000
you can expand out the cluster document
at cluster settings and you can see your
cluster database properties at this

205
00:23:38.000 --> 00:23:48.000
point and at that point you're ready to
start loading data using familiar jdbc
and odbc connectivity from the regular

206
00:23:48.000 --> 00:23:56.000
ETL or data management tools that you
use so it's very very simple to setup
and very very quick it's a good example

207
00:23:56.000 --> 00:24:03.000
actually of abstraction layer that makes
it much quicker to deliver a service
that's useful to you who are using AWS

208
00:24:03.000 --> 00:24:08.000
than it is building something similar
through more traditional approaches
where you might procure software and

209
00:24:08.000 --> 00:24:14.000
hardware and build a service yourself
off to describe Amazon redshift is a
great starting point for customers that

210
00:24:14.000 --> 00:24:19.000
aren't familiar with some of the
benefits of speed you can get if you
take advantage of the AWS cloud

211
00:24:19.000 --> 00:24:25.000
because it is so quick and simple to
provision a cluster using this console
based interface of course that's also

212
00:24:25.000 --> 00:24:33.000
available via the CLI and via the SDKs
that we provide as well some additional
stuff about getting started so the first

213
00:24:33.000 --> 00:24:38.000
of all is an excellent getting started
guide for Amazon redshift that you can
find if you visit the URL that's at the

214
00:24:38.000 --> 00:24:45.000
bottom right of this screen there's also
a two month free trial for set size
duster that you can take advantage of a

215
00:24:45.000 --> 00:24:51.000
very good getting started tutorial
there's an excellent system overview
there are guides for table design

216
00:24:51.000 --> 00:24:56.000
loading and query design which we're
going to cover in a little bit more
detail in a few minutes there's a

217
00:24:56.000 --> 00:25:03.000
description of how you can connect to
your database using these industry
standard connection protocols and

218
00:25:03.000 --> 00:25:09.000
there's also in that getting started
guide some really good pointers to BI
and ETL vendors have certified their

219
00:25:09.000 --> 00:25:15.000
tools for use with adam amazon redshift
so getting started guides for AWS
services are generally pretty good but

220
00:25:15.000 --> 00:25:20.000
the one for redshift in my view is one of the
best so really would advise if you've
got any interest in making use of the

221
00:25:20.000 --> 00:25:26.000
service you spend a few minutes
familiarizing yourself with that getting
started guide we're going to move on now

222
00:25:26.000 --> 00:25:32.000
take a look table design families and
redshift and three topics that we're
going to cover here firstly we're going

223
00:25:32.000 --> 00:25:37.000
to take a look at 
compression encodings and we'll take a
look at data types and lastly we'll take

224
00:25:37.000 --> 00:25:46.000
a look at distributing data across nodes
and sorting data to optimize performance
and a data warehousing system has very

225
00:25:46.000 --> 00:25:54.000
different design goals when you compare
them with traditional transaction
orientated RDBMS systems so OLTP

226
00:25:54.000 --> 00:26:02.000
applications that typically focused on
single row transactions inserts and
updates but data warehouses like Amazon

227
00:26:02.000 --> 00:26:09.000
redshift are optimized along a different
vector which is fast execution of
complex analytical queries against very

228
00:26:09.000 --> 00:26:17.000
large data sets because of the massive
amount of data involved in this kind of
workload you must specifically design

229
00:26:17.000 --> 00:26:23.000
your database to take full advantage of
performance optimizations is precisely
what we've done with Amazon redshift and

230
00:26:23.000 --> 00:26:31.000
the first important performance
optimization is compression this is a
column level operation that 

231
00:26:31.000 --> 00:26:37.000
reduces the size of data when it
stored because it reduces the size of
data when it's stored it has two effects

232
00:26:37.000 --> 00:26:45.000
firstly it conserves storage space and
secondly it reduces the size of data
that is read from storage and this can

233
00:26:45.000 --> 00:26:53.000
dramatically reduce the amount of disk
IO and therefore can dramatically
improve query performance to a plot you

234
00:26:53.000 --> 00:27:01.000
can apply compression encodings two
columns in tables manually based on your
own evaluation of data and that's an

235
00:27:01.000 --> 00:27:07.000
option that's open to you but you can
also use the copy command that's
provided to redshift and if you do this

236
00:27:07.000 --> 00:27:14.000
it will analyze and apply compression
automatically and we strongly recommend
using the copy command to apply

237
00:27:14.000 --> 00:27:22.000
automatic compression you can use
automatic compression when you create
and load a new table and 

238
00:27:22.000 --> 00:27:28.000
when you do this the copy command will perform that
compression arses you can also perform a
compression analysis 

239
00:27:28.000 --> 00:27:35.000
without loading data or changing the compressional
table by running a command called analyze
compression 

240
00:27:35.000 --> 00:27:41.000
against an already populated table so you can run this when you want
to analyze compression of the table for
future use while you preserve the

241
00:27:41.000 --> 00:27:50.000
existing data description language for that table 
data types
many different data types that are supported within

242
00:27:50.000 --> 00:27:57.000
Amazon redshift and one that's worth
calling out is the VARCHAR this
supports multi-byte characters but only

243
00:27:57.000 --> 00:28:03.000
up to a maximum of three bytes so 4 byte 
or longer characters are not
supported that's simple it's worth being

244
00:28:03.000 --> 00:28:10.000
aware of if you want to learn more about
supported data types in amazon redshift
visit the URL you can see bottom right

245
00:28:10.000 --> 00:28:16.000
here there's more comprehensive
discussion and description of the data
types that are available to you 

246
00:28:16.000 --> 00:28:24.000
when you load data into amazon redshift tables
schemas need to be expressed ideally in
third normal form and we're going to

247
00:28:24.000 --> 00:28:31.000
talk a little bit about how schema and
data distribution into play with one
another now and in distributing data

248
00:28:31.000 --> 00:28:38.000
it's important to remember that redshift
is a distributed system it's containing
a leader node which contains the query

249
00:28:38.000 --> 00:28:45.000
plan our and schedules queries and
aggregates results from queries but the
queries themselves take place across

250
00:28:45.000 --> 00:28:55.000
compute nodes and each compute node
contains slices one slice per core and a
slice is a logical container for data

251
00:28:55.000 --> 00:29:02.000
they're chosen on two types of
distribution round robin which can
be automated or on a distribution key

252
00:29:02.000 --> 00:29:09.000
where you define a column hash is
computed and data is distributed in
accordance with that hash this is very

253
00:29:09.000 --> 00:29:15.000
important because queries run on all
slices in parallel and optimal query
throughput can only be achieved when

254
00:29:15.000 --> 00:29:24.000
data is spread as close to evenly as
possible across slices what you want to
do ideally is have queries executed on

255
00:29:24.000 --> 00:29:32.000
cause working on data which is local to
them and in an optimized distribution of
the type that you can see here the

256
00:29:32.000 --> 00:29:38.000
distributed nature of the system can
cause significant performance overhead
here's a scenario where we've

257
00:29:38.000 --> 00:29:45.000
round-robin distributed item details
across slices but our
order details have been distributed

258
00:29:45.000 --> 00:29:51.000
independently so we have no distribution
key we're using default distribution
which is an optimized and 

259
00:29:51.000 --> 00:29:59.000
you can see that if we want to join orders with
items that were relevant to those orders
we'd have lots of internode traffic 

260
00:29:59.000 --> 00:30:06.000
web traffic for example from node one
relating to order one we'd have to make
a subquery into node three getting data

261
00:30:06.000 --> 00:30:12.000
from the second slice there about item
1.1 and joining it and also getting data
from the second slice on node two and

262
00:30:12.000 --> 00:30:20.000
joining that into the query relating to
the sub items on each order to help us
do the calculation you can 

263
00:30:20.000 --> 00:30:30.000
see at the top of this slide now with an optimized
distribution we're going to store the
sub items for each order adjacent 

264
00:30:30.000 --> 00:30:37.000
to them because we've used a customized
distribution we've ordered the
distribution key that we're using is

265
00:30:37.000 --> 00:30:48.000
items.orderID so that enables us to
colocate the item records together with
the order records on the same nodes 

266
00:30:48.000 --> 00:30:55.000
this leads to dramatically improve
performance because the zero internode
traffic in this model everything's 

267
00:30:55.000 --> 00:31:01.000
intra-node on the compute nodes themselves and
the aggregate data is passed back to the
leader nodes much much improved

268
00:31:01.000 --> 00:31:07.000
performance how can you choose
distribution keys when you want to look
for data that's frequently joined 

269
00:31:07.000 --> 00:31:15.000
by the most commonly run queries and you can
also evaluate looking at queries that
compute consume the most cpu and use

270
00:31:15.000 --> 00:31:22.000
that as a mechanism for identifying how
you should distribute data you want a
high cardinality so keys that have got a

271
00:31:22.000 --> 00:31:27.000
large number of discrete values because
we're using hashing here we want good
hash distribution and then you want to

272
00:31:27.000 --> 00:31:34.000
have low skew you don't want to have hot
spots and you can use query STV block
list identify skew factor in a data set

273
00:31:34.000 --> 00:31:43.000
a sorting here we're talking about
sorting data by a sort key and trying to
choose a sort key that is frequently

274
00:31:43.000 --> 00:31:51.000
used in your key query so as a query
predicate like the date range perhaps or
an identifier or was a join parameter

275
00:31:51.000 --> 00:31:58.000
and it can also be the hash key and this
sort key allows us to use those zone
maps to avoid reading entire blocks

276
00:31:58.000 --> 00:32:07.000
based on predicates so for example if
you had a table that contained a
timestamp sort key you were looking for

277
00:32:07.000 --> 00:32:15.000
a recent date range you'd entirely skip
reading any blocks that contained old
data where the zone map showed that

278
00:32:15.000 --> 00:32:22.000
these blocks did not include data that
was relevant to your query and again
this can minimize intra-node traffic so

279
00:32:22.000 --> 00:32:28.000
internode traffic and dramatically
improve query performance so sorting is
critically important for query

280
00:32:28.000 --> 00:32:36.000
performance optimization as well and
then lastly schema design you want to
optimize your data for querying it's

281
00:32:36.000 --> 00:32:42.000
pretty obvious for query orientated
workloads so you want to colocate
frequently join tables using those

282
00:32:42.000 --> 00:32:48.000
distribution keys to avoid data
transfers between nodes we said and you
want to use sort keys on joined columns

283
00:32:48.000 --> 00:32:54.000
allowing for fast merge joins and also
the use of zone maps and this
compression can allow you to denormalize

284
00:32:54.000 --> 00:33:02.000
data without heavy overhead or penalty
and incremental storage allowing you to
simplify queries and limit joins and

285
00:33:02.000 --> 00:33:09.000
here's an example of square query Optima
optimization on that data sent that we
talked about before we're going to

286
00:33:09.000 --> 00:33:18.000
denormalize commonly joined attributes
into a large table in this case bringing
inquiries enquiry airports and 

287
00:33:18.000 --> 00:33:27.000
enquiry airport groups together into one large
fact table using a star schema using
inquiry ideas our distribution key and

288
00:33:27.000 --> 00:33:35.000
our sort key and then we're going to
cash the other services using DISTSTYLE ALL
which is a attribute that you can

289
00:33:35.000 --> 00:33:44.000
effect when creating a table if you set
specified this style all when creating a
table this will lead to a copy of the

290
00:33:44.000 --> 00:33:55.000
entire table being distributed to every
node in your cluster it's a great help
in boosting performance for small for

291
00:33:55.000 --> 00:34:02.000
small tables that you want to have cache
locally all the rows in these tables for
any join will be available on every node

292
00:34:02.000 --> 00:34:08.000
obviously this increases storage
requirements and it also increases load
and maintenance time for the table but

293
00:34:08.000 --> 00:34:15.000
it can significantly improve execution
time when used with certain dimension
tables where key distribution is not

294
00:34:15.000 --> 00:34:21.000
appropriate but you need to weigh
performance improvements against the
maintenance and storage overhead costs

295
00:34:21.000 --> 00:34:29.000
so it's good for small tables that sit
around the edge of your star schema that
are frequently looked up and joined when

296
00:34:29.000 --> 00:34:34.000
executing queries on that large fact
table look caught that forms the core of
the schema so might want to consider

297
00:34:34.000 --> 00:34:41.000
that as an option the last thing to talk
about on table design not so much about
design is more about maintenance really

298
00:34:41.000 --> 00:34:47.000
there's a vacuuming operation which
needs to be used to clean up tables
after a bulk delete load or series of

299
00:34:47.000 --> 00:34:55.000
incremental updates this is quite
important amazon redshift does not
automatically reclaim the new space that

300
00:34:55.000 --> 00:35:01.000
is free when you delete rows and update
rose to perform an update amazon
redshift deletes the original row and

301
00:35:01.000 --> 00:35:08.000
appends the updated row so every update
is effectively delete followed by an
insert Rosa marked for deletion but not

302
00:35:08.000 --> 00:35:15.000
removed and the query processor will
scan deleted rows as well as those that
are undeleted so if you have too many

303
00:35:15.000 --> 00:35:22.000
deleted rose you're going to cost
yourself overhead cycles and unnecessary
processing you can resolve this by

304
00:35:22.000 --> 00:35:29.000
vacuuming tables or even the entire
database when you've concluded a
significant number of updates or deletes

305
00:35:29.000 --> 00:35:36.000
this will reclaim space it will purge
those deleted rows and it will improve
query performance it's an important

306
00:35:36.000 --> 00:35:43.000
aspect of operating clusters if you're
going to run them for a long period of
time and make significant updates or

307
00:35:43.000 --> 00:35:49.000
deletes to them want to learn more about
table design there's some excellent
resources that you can take advantage of

308
00:35:49.000 --> 00:35:54.000
in the redshift documentation which is a
really very comprehensive documentation
set the table design docs that you can

309
00:35:54.000 --> 00:36:03.000
see there and a table design tutorial
and this table design tutorials great
actually it takes you through an example

310
00:36:03.000 --> 00:36:12.000
of how you can optimize table design in
this tutorial you will start by creating
tables based on a star schema benchmark

311
00:36:12.000 --> 00:36:18.000
schema without sort keys without
distribution styles and without
compression encodings you'll then load

312
00:36:18.000 --> 00:36:24.000
the tables and test data and system
performance then you'll apply best
practices recreating the tables using

313
00:36:24.000 --> 00:36:32.000
sort keys and distribution styles
you'll reload the tables with test data
using automatic compression and your

314
00:36:32.000 --> 00:36:38.000
test performance again and if you do
this you'll be able to see firsthand the
performance benefits of well-designed

315
00:36:38.000 --> 00:36:45.000
tables the whole tutorial costs about a
dollar to run and takes about an hour to
run so really would recommend if you've

316
00:36:45.000 --> 00:36:51.000
not already worked through that as part
of your experimental work with Amazon
redshift you pick that tutorial up and

317
00:36:51.000 --> 00:36:57.000
take advantage of it it'll get you
hands-on with the system with a real
data set and they'll allow you to see in

318
00:36:57.000 --> 00:37:04.000
practice some of the benefits that you
can achieve through using these
optimization techniques next thing we're

319
00:37:04.000 --> 00:37:11.000
going to talk about is data loading and
just to provide you a quick overview
there's a very rich ecosystem of AWS

320
00:37:11.000 --> 00:37:17.000
services that you can take advantage of
to help you load data into amazon
redshift clusters we have integration

321
00:37:17.000 --> 00:37:24.000
with many AWS persistent services 
like RDS Oracle or MySql 
that you might be running on

322
00:37:24.000 --> 00:37:33.000
instances that you manage yourself data
volumes Amazon Elastic MapReduce and
glacier our low-cost archiving service

323
00:37:33.000 --> 00:37:39.000
you can also bring data in from
corporate data center resources
running on your own premises or in

324
00:37:39.000 --> 00:37:45.000
current location facilities using a
variety of different options actually
Direct Connect which is our low latency

325
00:37:45.000 --> 00:37:52.000
high bandwidth direct connection service
enabling you to connect your data center
or network directly to an AWS region

326
00:37:52.000 --> 00:37:58.000
using one gigabit 10 gigabit or
fractional bandwidth provided by
partners so if you regularly loading

327
00:37:58.000 --> 00:38:05.000
data into amazon redshift that could be
a good option for you can also use a VPN
connection which will terminate inside

328
00:38:05.000 --> 00:38:11.000
your VPC on a VPN gateway and you can
connect to from your own site that
allows you to secure the transportation

329
00:38:11.000 --> 00:38:19.000
of data across the internet using
encryption and authorization techniques
and then we have the s3 public API

330
00:38:19.000 --> 00:38:25.000
endpoint you can write to using s3
multi-part upload it's a very high
performance and effective way to get

331
00:38:25.000 --> 00:38:31.000
data into Amazon s3 that you can then
unload from there into redshift and we
have several options now for

332
00:38:31.000 --> 00:38:38.000
import/export so these this is a
mechanism for getting data into AWS
using physical media either the AWS

333
00:38:38.000 --> 00:38:45.000
import/export disk service where you can
send media of your choice to many
different AWS regions around the world

334
00:38:45.000 --> 00:38:51.000
we will load that I'll load the data
that is on those media into s3 and make
it available to you in your account or

335
00:38:51.000 --> 00:38:59.000
in North America you can take advantage
of the AWS import/export snowball it
allows you to import up to 50 terabytes

336
00:38:59.000 --> 00:39:07.000
of data with a single device we provide
the device which is a organized
data transport device you can use that

337
00:39:07.000 --> 00:39:15.000
currently in the one region in us-west
and in the us-east-1 region to load
data into Amazon s3 at really high scale

338
00:39:15.000 --> 00:39:21.000
very very rapidly so you've got large
data sets that you wish to move into
Amazon redshift those AWS import export

339
00:39:21.000 --> 00:39:29.000
options can be a good solution for you
you can also share data the other way so
you can dump data from your redshift

340
00:39:29.000 --> 00:39:38.000
database storing it in encrypted CSV
files you can upload that data to s3 and
you can share that date with other AWS

341
00:39:38.000 --> 00:39:44.000
accounts of course so you can take data
out of redshift as well as taking it
into redshift and s3 is a common

342
00:39:44.000 --> 00:39:50.000
mechanism for getting data into these
services we've already said for best
performance it's an excellent idea to

343
00:39:50.000 --> 00:39:57.000
split data files into a number of different
sub files and then you can parallel load

344
00:39:57.000 --> 00:40:05.000
these using the copy command across all
the available nodes in your cluster here
you can see we're using file suffixes so

345
00:40:05.000 --> 00:40:13.000
we're copying customer from s3 providing
a prefix and all files that have that
prefix will be evaluated and loaded into

346
00:40:13.000 --> 00:40:19.000
Amazon redshift in parallel providing a
set of credentials which have
appropriate access authority to the

347
00:40:19.000 --> 00:40:26.000
Amazon s3 endpoint so that we can read
data from that specific bucket and read
these specific objects and then we're

348
00:40:26.000 --> 00:40:33.000
specifying a delimiter and there are
actually many good examples of different
approaches the loading data that you can

349
00:40:33.000 --> 00:40:40.000
find if you check out the URL you can
see on the screen in the bottom left as
many different data loading scenarios

350
00:40:40.000 --> 00:40:48.000
that are explored with worked examples
of that URL some issues that you should
try to avoid when loading data many of

351
00:40:48.000 --> 00:40:52.000
these are quite obviously it's
worthwhile running through them anyway
so firstly avoid mismatches make sure

352
00:40:52.000 --> 00:40:59.000
that data types in tables and values in
input data fields match each other make
sure you have the right number of

353
00:40:59.000 --> 00:41:05.000
columns make sure that you have
mismatched quotes so you don't have
mismatched quotes redshifts support

354
00:41:05.000 --> 00:41:10.000
single and double quotes quotes but the
most these must be balanced
appropriately so red double quote must

355
00:41:10.000 --> 00:41:15.000
match a double crow in a single quote
which map must match single quote dates
and times obviously to be in the correct

356
00:41:15.000 --> 00:41:22.000
format where you have numeric columns
with range control you need to make sure
that all entries are within the range

357
00:41:22.000 --> 00:41:29.000
which is valid and where yet may have an
issue where the number of distinct
volumes values for a column may exceed

358
00:41:29.000 --> 00:41:35.000
the limitation which compression
encoding again the reason it's a good
idea to use automatic compression

359
00:41:35.000 --> 00:41:45.000
encoding using the copy command you can
also load data using direct SQL all
direct SQL commands go via the leader

360
00:41:45.000 --> 00:41:54.000
node so this is not a parallel load
model but you can use it for for example
joining staging tables with pre-existing

361
00:41:54.000 --> 00:42:00.000
target tables and in this scenario it
will update where rows exists and insert
where no rows exists there are

362
00:42:00.000 --> 00:42:06.000
performance limitations here because
you're relying upon the leader
no it should not really be used for

363
00:42:06.000 --> 00:42:13.000
large-scale data loading operations but
it is available for you for example if
you wanted to load in some of those

364
00:42:13.000 --> 00:42:21.000
smaller tables that we described as
being around the outside of that star
schema that might be a useful way to do

365
00:42:21.000 --> 00:42:28.000
that I recommend loading large data sets
using the copy command as you can see
here it's definitely best practice use a

366
00:42:28.000 --> 00:42:35.000
single copy command which will allow you
to parallel load rather than load in
series compress your data files with

367
00:42:35.000 --> 00:42:44.000
gzip once again to optimize the speed at
which data files can be ingested by the
nodes if you cannot use copy you can use

368
00:42:44.000 --> 00:42:51.000
multirow inserts using SQL as we've
described already so bulk insert
operations they provide high performance

369
00:42:51.000 --> 00:42:59.000
data insertion also some further best
practices load your day to insert key
order to avoid the need to vacuum at the

370
00:42:59.000 --> 00:43:06.000
end of the process and organize your
data as a series of time series tables
each table is identical but contains

371
00:43:06.000 --> 00:43:13.000
different data for time ranges then
those can be parallel loaded by
different load different nodes in your

372
00:43:13.000 --> 00:43:21.000
cluster you can use staging tables to
perform an upset and as we said earlier
run the vacuum command whenever you've

373
00:43:21.000 --> 00:43:27.000
added deleted or modified a large number of rows
unless you load your data in sort key order 
you can increase memory

374
00:43:27.000 --> 00:43:35.000
available to copy or vacuum 
and you can run the analyze command whenever you've
made changes to your data to ensure that

375
00:43:35.000 --> 00:43:40.000
your table statistics are current so and
follow those best practices and
obviously this further best practice

376
00:43:40.000 --> 00:43:47.000
guidance in the documentation for this
service so once you've got your data
into amazon redshift how do you go about

377
00:43:47.000 --> 00:43:53.000
working with it there's several
different usage scenarios here but a
common usage scenario is staging data

378
00:43:53.000 --> 00:44:01.000
that you might be creating in an online
transaction processing ERP system you're
going to store that information in an

379
00:44:01.000 --> 00:44:09.000
RDBMS on your premises and this is quite
a common use case by no means the only
use case another common flow might be

380
00:44:09.000 --> 00:44:17.000
unstructured data pre-processed and
enriched using for example EMR our
elastic MapReduce

381
00:44:17.000 --> 00:44:24.000
managed service running register running
Hadoop in the AWS cloud it's also a
common workflow that customers will use

382
00:44:24.000 --> 00:44:32.000
using Hadoop to pre-process data and
then loading it into amazon redshift by
creating files they staged an amazon s3

383
00:44:32.000 --> 00:44:40.000
that they can a little load in using
that copy command in the use case of
data integration between existing

384
00:44:40.000 --> 00:44:48.000
RDBMS systems though it's quite common
for customers to use one of the data
integration partners from the Amazon

385
00:44:48.000 --> 00:44:57.000
redshift ecosystem you
can use VPC an encryption to make sure
that your data remains secure all times

386
00:44:57.000 --> 00:45:04.000
if you do this 
several different options but those provided by our data integration partners so a good option

387
00:45:04.000 --> 00:45:11.000
here these technologies work to connect
operational data stores with redshift
they make transporting data over

388
00:45:11.000 --> 00:45:20.000
distance very simple and it makes it
easy to transfer data between different
database types with little operational

389
00:45:20.000 --> 00:45:26.000
effort a little operational overhead you
can find out more about these partners
if you visit the URL that you see there

390
00:45:26.000 --> 00:45:33.000
but we have a variety of different
partners that you might want to take
advantage for data integration obviously

391
00:45:33.000 --> 00:45:39.000
when you've got your data into the
system you're then interested in
business intelligence and visualization

392
00:45:39.000 --> 00:45:47.000
of that data this is where you make a
take advantage of those standard JDBC
and odbc connectivity drivers that we

393
00:45:47.000 --> 00:45:54.000
provide again there's a rich ecosystem
of different partners available to help
you with this and you can find those at

394
00:45:54.000 --> 00:46:02.000
the same URL it's actually very broad
different ecosystem a very very broad
ecosystem different types of provider

395
00:46:02.000 --> 00:46:09.000
that are operating to provide
visualization services many of these bi
and visualization tools are available

396
00:46:09.000 --> 00:46:17.000
through something called the AWS
marketplace and this is an e-commerce
platform that provides ISVs with an

397
00:46:17.000 --> 00:46:25.000
opportunity to market and sell software
products to AWS customers and if you
visit this URL 

398
00:46:25.000 --> 00:46:32.000
aws.amazon.com/marketplace/redshift
you'll find a wide variety of marketplace products 
so 20 different products that are aligned

399
00:46:32.000 --> 00:46:38.000
directly to the redshift use case and
if you visit the business intelligence
section on the market place you'll find

400
00:46:38.000 --> 00:46:44.000
over 200 different software products
that have business intelligence features
so have many different options open to

401
00:46:44.000 --> 00:46:51.000
you here are three of the options that
are available under the analytics
category from looker from tableau and

402
00:46:51.000 --> 00:46:58.000
from tibco jaspersoft with a variety of
different licensing options variety
different cost points the great thing

403
00:46:58.000 --> 00:47:06.000
about the AWS marketplace is many of the
ISVs that have products listed there
provide free trials and many of these

404
00:47:06.000 --> 00:47:12.000
free trials can be bootstrap directly
from the marketplace without having to
speak to or make contact with the ISVs

405
00:47:12.000 --> 00:47:18.000
in question so it's an excellent way for
trying out different visualization tools
different bi tools that were available

406
00:47:18.000 --> 00:47:25.000
to I really would recommend that
customers do take a look at it we also
recently announced that the AWS reinvent

407
00:47:25.000 --> 00:47:32.000
a just a couple of weeks ago a brand-new
AWS business intelligence service called
Amazon quick sight this is a fast cloud

408
00:47:32.000 --> 00:47:39.000
platt easy to use P I service that makes
it easier to build visualizations
perform ad hoc analysis and quickly get

409
00:47:39.000 --> 00:47:45.000
insights from data here we're
integrating with AWS data stores like
amazon redshift as well as other stores

410
00:47:45.000 --> 00:47:53.000
like flat files and third-party data
sources and we provide an in-memory
query engine that means it's possible to

411
00:47:53.000 --> 00:48:00.000
get your first analyses from your day to
in less than 60 seconds very beautiful
and dynamic data visualizations sharing

412
00:48:00.000 --> 00:48:06.000
features so you can share analytics with
people in or outside your organization
the capability as I said to integrate

413
00:48:06.000 --> 00:48:14.000
with data sources on AWS and it comes at
about the tenth the cost of traditional
ly licensed old guard business

414
00:48:14.000 --> 00:48:21.000
intelligence tools I'm going to say a
whole lot more about Amazon quick site
during today's session but if you want

415
00:48:21.000 --> 00:48:28.000
to see the launch it reinvent and also
see a demo from my colleague Matt would
talking about capabilities of quick site

416
00:48:28.000 --> 00:48:35.000
and visually illustrating these in the
form of a demo visit the youtube link
that you can see at the bottom right of

417
00:48:35.000 --> 00:48:41.000
this slide you can see under Jesse's
announcement of the surveys followed by
matt's demo from reinvent just a couple

418
00:48:41.000 --> 00:48:46.000
of weeks ago so that might be something
that you also
consider as we make it generally

419
00:48:46.000 --> 00:48:54.000
available over the coming months back up
and restoration this is very important
feature of Amazon redshift and the first

420
00:48:54.000 --> 00:49:00.000
thing to say is that back up stammers
and s3 automatic continuous and
incremental so it's not something that

421
00:49:00.000 --> 00:49:08.000
you need to configure other than
configuring the snapshot interval and
the snapshot retention period so how

422
00:49:08.000 --> 00:49:18.000
often do you wish to store a snapshot of
your data and how long do you wish to
retain these snapshots for those two

423
00:49:18.000 --> 00:49:26.000
variables by the way default to a
one-day snapshot interval and a 35-day
retention period so your recovery point

424
00:49:26.000 --> 00:49:33.000
objective for this service is 24 hours
essentially with the default settings
you can also create user driven

425
00:49:33.000 --> 00:49:40.000
snapshots at any time you simply
freezing the cluster state triggering a
snapshot via the console or API and that

426
00:49:40.000 --> 00:49:48.000
frozen cluster state will be retained
unless you explicitly delete it you have
control on how your clusters backups are

427
00:49:48.000 --> 00:49:54.000
stored in that particular scenario any
point you can restore a cluster from a
snapshot and make it available for

428
00:49:54.000 --> 00:50:02.000
querying as it was in the state at the
time the snapshot was taken and it's a
very very simple process here's an

429
00:50:02.000 --> 00:50:10.000
example of how you go about restoring
from a snapshot so I'm looking here at
the cluster console in the redshift

430
00:50:10.000 --> 00:50:18.000
console and I'm going to click into the
detail of the my cluster cluster that's
running and at this point i have several

431
00:50:18.000 --> 00:50:29.000
drop downs available to me i can click
the create snapshot button and i'll be
given the option at this point to

432
00:50:29.000 --> 00:50:37.000
restore from a snapshot if i click that
restore from snapshot option and as to
define the characteristics of the new

433
00:50:37.000 --> 00:50:44.000
cluster that i wish to create so for
example can restore on to a different
node type here and i do have to change

434
00:50:44.000 --> 00:50:50.000
the name of my cluster identifier when I
do this restoration so I'm going to
modify my cluster identifier here and

435
00:50:50.000 --> 00:50:56.000
then restore that cluster from a
snapshot with the name restored dash my
cluster

436
00:50:56.000 --> 00:51:05.000
hit restore at the bottom and at this
point my cluster my new cluster will go
into creation and restoration mode and

437
00:51:05.000 --> 00:51:12.000
actually at the point at which creation
has been completed but restoration is
still underway at this point I can begin

438
00:51:12.000 --> 00:51:18.000
to make queries against my newly
restored cluster this is where that
background streaming of the restoration

439
00:51:18.000 --> 00:51:24.000
process comes into effect and I can
actually work with data in my cluster
while it's still in the process of being

440
00:51:24.000 --> 00:51:31.000
restored I'll be given a new JDBC odbc
endpoint that can work with for that
restored snapshot version of my cluster

441
00:51:31.000 --> 00:51:37.000
which is distinct from the current
cluster that have running in my console
you can already see that and that's how

442
00:51:37.000 --> 00:51:43.000
simple it is so it's one two three four
five step process to restore cluster
from a snapshot that you've previously

443
00:51:43.000 --> 00:51:50.000
created very very simple so close to
date your application automated backups
on s3 and note monitoring all

444
00:51:50.000 --> 00:51:58.000
availability features that are intended
to make the redshift service durable and
ensure that it meets the availability

445
00:51:58.000 --> 00:52:05.000
level that you need ok what about
upgrading and scaling clusters did just
say at the top of the session that this

446
00:52:05.000 --> 00:52:11.000
was a very simple process and
essentially what happens here is you can
resize a cluster while it remains online

447
00:52:11.000 --> 00:52:19.000
so what will happen when you go to
resize is it will put your cluster into
read-only mode ok this is a very simple

448
00:52:19.000 --> 00:52:25.000
process so you're going to specify
resize switch your existing cluster into
read only or provision a new clerk

449
00:52:25.000 --> 00:52:32.000
cluster in the background will copy data
in parallel from node to node in us in a
seamless and transparent manner and

450
00:52:32.000 --> 00:52:38.000
during this process you will only be
charged for the source cluster at the
point at which the new cluster is

451
00:52:38.000 --> 00:52:45.000
available the SQL endpoint will switch
over via dns so your tools will continue
to work but you'll look be looking at

452
00:52:45.000 --> 00:52:51.000
the new and large cluster you've been
read write mode at this point and then
in the background we will decommission

453
00:52:51.000 --> 00:52:55.000
the source cluster and charging
obviously we will flick over onto the
new cluster at the point at which that

454
00:52:55.000 --> 00:53:02.000
endpoints which is over so taking you
through that in the console once again
we're in the cluster console here we can

455
00:53:02.000 --> 00:53:08.000
click on our cluster drop down you'll
see there's a resize option there that
appears if we select that you'll see

456
00:53:08.000 --> 00:53:14.000
this resize cluster dialog
you can change the node type the cluster
type and the number of nodes here I've

457
00:53:14.000 --> 00:53:21.000
increased my node count to eight I've
gone off single node operation on to
eight nodes and I'm then into a

458
00:53:21.000 --> 00:53:28.000
transparent operation so I don't create
a new endpoint in this scenario in the
background the cluster will be switched

459
00:53:28.000 --> 00:53:36.000
into read-only mode you can view resize
progress in the cluster status tab there
and obviously the time it takes to

460
00:53:36.000 --> 00:53:40.000
resize of cluster will depend on the
amount of data that's in each node
typically it takes from a couple of

461
00:53:40.000 --> 00:53:47.000
hours up to a day of their clusters with
large amounts of data might take even
longer than that this is because data is

462
00:53:47.000 --> 00:53:54.000
copied in parallel from each node on the
source cloud each node on the source
cluster to each node on the target okay

463
00:53:54.000 --> 00:54:02.000
there's much more information about
resizing that you can see if you follow
the URL you can see at the bottom left

464
00:54:02.000 --> 00:54:09.000
of this slide so check out cluster
resizing that the end of the process the
endpoint will be switched over and the

465
00:54:09.000 --> 00:54:16.000
service will be made available for me once more 
take a look now quickly at
some recently announced features for

466
00:54:16.000 --> 00:54:21.000
Amazon redshift just to bring you right
up to date with new stuff that's been
announced for this service the first

467
00:54:21.000 --> 00:54:26.000
desired trail at the top of the session
is something called scalar user defined
functions you can write these

468
00:54:26.000 --> 00:54:36.000
user-defined functions using Python 2.7
syntax largely identical to the postgres
SQL UDF syntax for security reasons we

469
00:54:36.000 --> 00:54:43.000
prohibit system and network calls within
udfs but you can basically take
advantage of pre-installed Python

470
00:54:43.000 --> 00:54:52.000
modules pandas numpy and scifi to
perform analytic data analytic
processing of your data in Python and

471
00:54:52.000 --> 00:55:01.000
this analytic processing takes place in
parallel across your cluster so like a
regular redshift query it will take

472
00:55:01.000 --> 00:55:07.000
advantage of IO CPU and memory capacity
in each one of the nodes in your cluster
there's a very very powerful feature

473
00:55:07.000 --> 00:55:15.000
there's much more on that in Jeff bars
recent blog post that you can find if
you visit the Amazon redshift category

474
00:55:15.000 --> 00:55:22.000
on the official AWS blog it's a very
powerful feature it enables you to
simplify data manipulation as well as an

475
00:55:22.000 --> 00:55:29.000
excellent example of
working with URLs in data using UDFs
in Jeff's blog so check that of you

476
00:55:29.000 --> 00:55:36.000
interested in user-defined functions in Python 
last announcement before that was
the announcement of the new ds2 instance

477
00:55:36.000 --> 00:55:41.000
family which I already talked about the
top of the session when we were
describing the different nodes that you

478
00:55:41.000 --> 00:55:50.000
can use to construct your redshift
cluster this is a pretty big upgrade
actually delivering twice the memory and

479
00:55:50.000 --> 00:55:57.000
compute power of their previous dense
storage pretty assessors which used to
call the dw1 stain same storage

480
00:55:57.000 --> 00:56:03.000
capacity this node type also supports
enhanced networking and provides fifty
percent more disks throughput than the

481
00:56:03.000 --> 00:56:11.000
ds1 so it was a big performance bump for
Amazon redshift when we release that
back in June this year this also details

482
00:56:11.000 --> 00:56:19.000
on that post on cost optimization using
reserved instances I would recommend
taking a look at that post if you

483
00:56:19.000 --> 00:56:27.000
interested in getting up to date with
reserved instance pricing for Amazon
redshift also okay resources that you

484
00:56:27.000 --> 00:56:34.000
can use to learn more about Amazon
redshift the first resource of course is
the Amazon redshift product detail page

485
00:56:34.000 --> 00:56:40.000
you can find many of the resources that
I've talked about during the session
linked off this page including the

486
00:56:40.000 --> 00:56:49.000
documentation sets for Amazon redshift
for some of the best documentation of any
AWS service in my view getting started

487
00:56:49.000 --> 00:56:56.000
guide that we touched upon earlier in
the session please do check that out
also check out the table design tutorial

488
00:56:56.000 --> 00:57:02.000
you can find a link to that gang started
guide now trail but early but it really
is excellent and well worth taking a

489
00:57:02.000 --> 00:57:08.000
look at one dollar one hour and you can
learn hands-on how to optimize table
design with Amazon redshift you can find

490
00:57:08.000 --> 00:57:13.000
the Financial Times reference that I've
talked about as well as many other
examples of customers that are running

491
00:57:13.000 --> 00:57:22.000
analytics workloads in the AWS cloud at
awsamazon.com/solutions/case-studies/analytics around 30 or so

492
00:57:22.000 --> 00:57:27.000
different customer case studies are
there worth taking a look at you may
very well find someone that's got a very

493
00:57:27.000 --> 00:57:34.000
similar use case to you they've already
implemented using Amazon redshift
it's also an awesome session from AWS

494
00:57:34.000 --> 00:57:43.000
reinvent 2015 just a couple of weeks old
where we dive deep into Amazon redshift
tuning and best practices it's with Eric

495
00:57:43.000 --> 00:57:50.000
Ferreria from AWS and airy Miller from
tripadvisor you can find that if you
click the youtube link that you can see

496
00:57:50.000 --> 00:57:56.000
at the bottom of the text on this
particular slide and if you do that
you'll also see a playlist pop up in

497
00:57:56.000 --> 00:58:05.000
YouTube that playlist contains all other
big data and analytics sessions from AWS
reinvent including additional content on

498
00:58:05.000 --> 00:58:14.000
amazon redshift and on building data
pipelines to use Redshift and on using
EMR and other analytic services have

499
00:58:14.000 --> 00:58:21.000
been used and built by customers using
the AWS cloud so I really would advise
you to check out both this video and the

500
00:58:21.000 --> 00:58:29.000
playlist you can find that I've got as
my colleague two switches into Q&A mode
in just a second so you can rate us when

501
00:58:29.000 --> 00:58:34.000
we wrap up the webinar just remind you
that we are accepting your questions
during today's session and we'll come

502
00:58:34.000 --> 00:58:39.000
back to you with feedback over the
course of the next week or two if you do
submit a question during today's session

503
00:58:39.000 --> 00:58:44.000
so if there's anything that you'd like
to know more about that we've covered
the day or any other Amazon redshift or

504
00:58:44.000 --> 00:58:49.000
other Amazon topics for that matter
amazon web services topic so that one
that you interested in please ask a

505
00:58:49.000 --> 00:58:54.000
question using that Q&A panel and we'll
get back to you training and
certification if you want to build your

506
00:58:54.000 --> 00:59:01.000
AWS knowledge more generally and check
out these self-paced labs training and
certification options the AWS amazon com

507
00:59:01.000 --> 00:59:08.000
/ training URL contains details of all
AWS training offerings and is well worth
look if you've not already taken a look

508
00:59:08.000 --> 00:59:15.000
at it that concludes all the content
that we have for today's session and ask
my colleague to switches into Q&A mode

509
00:59:15.000 --> 00:59:21.000
now so you can give us a rating please
give this webinar a rating between one
and five with five being the best if you

510
00:59:21.000 --> 00:59:27.000
want to leave us quality feedback about
how we've done of course you can do that
using the Q&A panel as well as well as

511
00:59:27.000 --> 00:59:31.000
continuing to submit your questions
going to leave the webinar open for 10
minutes or so so make sure we've got all

512
00:59:31.000 --> 00:59:37.000
of your questions today and give you an
opportunity to rate us also like to
encourage you to follow a double us on

513
00:59:37.000 --> 00:59:44.000
social media on Twitter actually stay up
to date with us globally aw
cloud stay up to date with AWS news and

514
00:59:44.000 --> 00:59:53.000
education events here in the UK and
Ireland AWS underscore UK I and you can
find me on twitter at enm as well like

515
00:59:53.000 --> 00:59:58.000
to thank you for giving up your time to
join today's session and do appreciate
you giving you time to learn a little

516
00:59:58.000 --> 01:00:04.000
bit more about AWS hope it was a useful
session for you today please keep
submitting your questions and I'll see
you on another AWS webinar very very
soon thanks very much