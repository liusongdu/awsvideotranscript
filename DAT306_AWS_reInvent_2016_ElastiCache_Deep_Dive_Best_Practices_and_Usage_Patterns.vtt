WEBVTT FILE

1
00:00:00.000 --> 00:00:08.000
hello and welcome to Amazon Elastic ash
deep dive my name is Michael abebe I'm a
specialist Solutions Architect here at

2
00:00:08.000 --> 00:00:15.000
AWS and I'm delighted to share the stage
today with Brian Kaiser CTO of huddled
who will be presenting afterward now we

3
00:00:15.000 --> 00:00:21.000
have a lot of content that we're going
to present today so please save your
questions to the end of the session and

4
00:00:21.000 --> 00:00:30.000
we'll stick around and take those
questions alright so today we're going
to be talking about the value of a key

5
00:00:30.000 --> 00:00:37.000
value store we're going to dive into
Amazon Elastic ash we're going to look
at the various usage patterns that you

6
00:00:37.000 --> 00:00:43.000
could use em is on elastic ash with
we'll talk about how you can scale your
data using Redis cluster we'll look at

7
00:00:43.000 --> 00:00:54.000
best practices and at that point I'm
going to hand it over to Brian all right
so we are headed into a the midst of a

8
00:00:54.000 --> 00:01:03.000
massive shift toward real-time data and
if you think about the the need for you
know real-time analytics the data

9
00:01:03.000 --> 00:01:14.000
velocity the data value it really
created an emerging trend for this fast
data so in our session today what we're

10
00:01:14.000 --> 00:01:20.000
going to do is we're going to talk about
how Amazon Elastic ash can power those
various workloads outside of caching

11
00:01:20.000 --> 00:01:29.000
what is Amazon Elastic ash so it's a
managed service it supports the two most
popular key value store engines which is

12
00:01:29.000 --> 00:01:35.000
reticent memcache d it's fully managed
so what that means is you don't have to
worry about anything besides your data

13
00:01:35.000 --> 00:01:42.000
and the size of your cluster it is
highly available reliable and it's
managed and hardened by amazon so we're

14
00:01:42.000 --> 00:01:50.000
going to talk about what that exactly
means in a later slide now if you were
to think about your data as a

15
00:01:50.000 --> 00:01:56.000
temperature gauge you would want that
hot data to be readily available you
would want it to support extremely high

16
00:01:56.000 --> 00:02:03.000
request rates you'd want it to support
extremely low latency that's where
Amazon Elastic a schvitz you might also

17
00:02:03.000 --> 00:02:09.000
have cold data you might also have warm
data and for those data for those data
needs you'll have data different data

18
00:02:09.000 --> 00:02:14.000
data stores that can augment your
solution and so for your cold data
you might want to put that in Amazon

19
00:02:14.000 --> 00:02:20.000
glacier which you can art archive and do
something else with it and the same
thing is true for those other data

20
00:02:20.000 --> 00:02:30.000
stores in between and it all depends on
your use case so I mentioned there are
two popular key value stores that have

21
00:02:30.000 --> 00:02:37.000
supported with elastic ash the first one
being memcache d so memcache d has been
available and around since 2003 it's

22
00:02:37.000 --> 00:02:44.000
been the gold standard of caching for
many years if you think about the
capabilities of memcache d it's really

23
00:02:44.000 --> 00:02:51.000
like a flat cash it supports a data
structure which is a string you can
support up to one megabyte in that value

24
00:02:51.000 --> 00:02:59.000
it has no persistence so if you're
adding shards in a memcache d cluster
and you lose the data in a particular

25
00:02:59.000 --> 00:03:07.000
node you lost that data and for a
caching use case that's a that's okay
and if it's not okay this is kind of

26
00:03:07.000 --> 00:03:14.000
where Redis fits in and we'll talk about
that other things with memcache d is
very easy to scale you can add nodes and

27
00:03:14.000 --> 00:03:23.000
your key space will kind of distribute
across those no it's pretty easily and
and it's insanely fast right so I mean

28
00:03:23.000 --> 00:03:30.000
we're dealing with micro second
performance right us on the other hand I
like to think of is a superset to

29
00:03:30.000 --> 00:03:38.000
memcache d why do I say that well it
supports the string data structure so
you can have the string value except

30
00:03:38.000 --> 00:03:45.000
from a one megabyte instead of having
data up to one megabyte you can store up
to 512 megabytes worth of data there are

31
00:03:45.000 --> 00:03:51.000
other data structures which we're going
to dive into it has persistence so if
you care about that data and you want to

32
00:03:51.000 --> 00:03:57.000
have you know support rtos and our POS
you can do that with Redis we'll talk
about the different options you have

33
00:03:57.000 --> 00:04:05.000
it's oddly available you can have a
master and if that primary node fails
you can have a read replica which will

34
00:04:05.000 --> 00:04:13.000
be promoted to be the new new master
it's very powerful there's over 200
commands and Redis you have Lua

35
00:04:13.000 --> 00:04:20.000
scripting which you can build some
business logic and you can have that
logic execute in memory and it's all so

36
00:04:20.000 --> 00:04:26.000
simple and that's important to mention
because syntactically it's very easy to
use

37
00:04:26.000 --> 00:04:35.000
we'll see some examples later in this
presentation so when we talk about data
structures I like to start with the

38
00:04:35.000 --> 00:04:42.000
basic so the basic data structure is a
string now in Redis this is supported
both in memcache tea and Redis amend

39
00:04:42.000 --> 00:04:49.000
Redis it's a you know it supports up to
512 megabytes its binary safe so what
does that mean what it means is you can

40
00:04:49.000 --> 00:04:57.000
essentially put anything that fits into
that space in that value that could be
HTML code it could be a JSON object it

41
00:04:57.000 --> 00:05:05.000
could be a image of picture and there's
also a cool capability where if you have
an integer representation in that string

42
00:05:05.000 --> 00:05:13.000
value you can increment and decrement
that value and use it as a counter where
it gets interesting with Redis is these

43
00:05:13.000 --> 00:05:19.000
additional data structures the first one
we'll talk about is a set so if you
remember if you came from a dev

44
00:05:19.000 --> 00:05:27.000
background a set is a collection that
allows you to have unique values or
elements within that collection so say

45
00:05:27.000 --> 00:05:35.000
for example you have a customer you want
a group maybe your customer IDs your key
maybe that customer the customer list

46
00:05:35.000 --> 00:05:42.000
and in every value that you have in that
set might be a customer ID this is great
why because you don't want to have

47
00:05:42.000 --> 00:05:49.000
duplicate customer IDs so this is
managed for you in memory it's lightning
fast it's micro second performance and

48
00:05:49.000 --> 00:06:02.000
it's a great way to group your data
together now a sorted set is a set so it
maintains those unique values within the

49
00:06:02.000 --> 00:06:09.000
set but it also has an interesting
parameter which is score now a score
allows you to sort the data based on a

50
00:06:09.000 --> 00:06:17.000
particular value right so take for
example you are building a game and your
key might be a leader board and then

51
00:06:17.000 --> 00:06:25.000
your users are the value and you want to
sort these users based on something
right so in a game you're actually

52
00:06:25.000 --> 00:06:34.000
sorting them by the score so again this
has happened for you in an in-memory
performance engine and you pass in those

53
00:06:34.000 --> 00:06:39.000
values they'll maintain the uniqueness
and it will sort it for you
automatically and

54
00:06:39.000 --> 00:06:45.000
you can retrieve it in a number of ways
one way that you can retrieve it is you
know in a synchronous order and a

55
00:06:45.000 --> 00:06:52.000
reverse order and you can pull in a
range of information so again this is
great for D duping a information for

56
00:06:52.000 --> 00:06:57.000
grouping information and sorting
information and we'll take a look at
other use cases where you can use a

57
00:06:57.000 --> 00:07:08.000
sorted set a list is a collection that
allows you to capture the elements that
are inserted in that order so there is

58
00:07:08.000 --> 00:07:14.000
no particular order that this is
maintained and but it's great for
pushing and popping elements either from

59
00:07:14.000 --> 00:07:21.000
the head or the tail of this list so a
lot of common patterns that are built
using a list could be you know a

60
00:07:21.000 --> 00:07:29.000
timeline and so you can have a timeline
that might be your key and all the
elements that you put in that list could

61
00:07:29.000 --> 00:07:38.000
be an event based on that timeline so
this is a common pattern and a usage for
a list and in hashes they are my

62
00:07:38.000 --> 00:07:47.000
favorite so what a hash allows you to do
is it's the it's it's a data structure
that is suited for object representation

63
00:07:47.000 --> 00:07:54.000
so take for example you have a customer
record now that customer has attributes
that attribute could be you know the

64
00:07:54.000 --> 00:08:02.000
customer name could be a customer
address so if you're using a hash the
key maybe your customer ID and in all

65
00:08:02.000 --> 00:08:09.000
the fields and values associated with
that customer are the attributes related
to that customer now why is the cash why

66
00:08:09.000 --> 00:08:16.000
is a hash cool you can create a JSON
object and you can store this in the
string but what I like about hashes is

67
00:08:16.000 --> 00:08:25.000
that you can do operations on individual
fields so you can set the data in a hash
its memory efficient in Redis and then I

68
00:08:25.000 --> 00:08:33.000
can query individual elements so maybe
within my customer key or my customer
list or my individual customer I just

69
00:08:33.000 --> 00:08:41.000
want to know his address I can just
query for that individual field so it's
great one of the questions that I get a

70
00:08:41.000 --> 00:08:49.000
lot from customers who are new to you
know caching is which one should i use
memcache tea or redness and so one of

71
00:08:49.000 --> 00:08:54.000
the things I like to do is kind of think
backwards
if you are just doing a caching use case

72
00:08:54.000 --> 00:09:02.000
and I'd like to start with what you know
what languages are you using you might
be using a language that you know has

73
00:09:02.000 --> 00:09:08.000
you know sophisticated support for
memcache d maybe it's baked into the
framework and if you're just using

74
00:09:08.000 --> 00:09:17.000
caching maybe memcache d is good enough
however if you are needing to do caching
but you think that there might be other

75
00:09:17.000 --> 00:09:23.000
use cases for your data I'll tell you to
use ritis I think Redis and a lot of
ways does what memcache do you can do

76
00:09:23.000 --> 00:09:34.000
but it can support additional use cases
and we'll take a look at those so some
of the value propositions that you get

77
00:09:34.000 --> 00:09:40.000
with elastic ash I mentioned the first
one it's fully managed what does that
mean you just have to worry about the

78
00:09:40.000 --> 00:09:47.000
data that you put in a cluster and the
size of your actual cluster as far as
patching as far as fail overs all those

79
00:09:47.000 --> 00:09:54.000
additional processes which we call heavy
lifting it's kind of removed from your
plate the other thing is that it's open

80
00:09:54.000 --> 00:10:01.000
source compatible so if you're already
thread a have code written using ritis
on ec2 you can easily port that code

81
00:10:01.000 --> 00:10:08.000
over to lassic ash same is true for
memcache d so we support all the open
source protocols there is no cross AZ

82
00:10:08.000 --> 00:10:16.000
data transfer so this is sometimes
overlooked so say for example you're
running Redis on ec2 and you're in a

83
00:10:16.000 --> 00:10:24.000
multi AZ environment you may have you
know up your your nodes communicating
with each other and if you're doing that

84
00:10:24.000 --> 00:10:31.000
there is an additional charge so if you
take a look at the actual charge for
your ec2 instance plus the data out

85
00:10:31.000 --> 00:10:38.000
you're in a same ballpark of using
elastic ash might as well just use the
last Akash and this is especially true

86
00:10:38.000 --> 00:10:47.000
when your node size of your cluster size
is large the other thing is that the
enhanced Redis engine comes with us and

87
00:10:47.000 --> 00:10:53.000
so I'll talk about that in the next
slide and this is really some of the
lessons learned that we've heard from

88
00:10:53.000 --> 00:11:02.000
our customers that we built into service
so the first one is the first feature is
that we've heard from customers that

89
00:11:02.000 --> 00:11:08.000
memory management can be challenging
with Redis so one
Apple is say for example you're running

90
00:11:08.000 --> 00:11:15.000
Redis and you have background processes
like doing snapshots and sinking
especially with snapshots depending on

91
00:11:15.000 --> 00:11:24.000
how much rights you have occurring on
the primary Redis may take up up to you
know it may double your memory footprint

92
00:11:24.000 --> 00:11:29.000
on that instance and if you don't have
enough memory for those background
processes the throw you want to swap now

93
00:11:29.000 --> 00:11:36.000
you don't want to be in swap when you're
an in-memory database system so what we
have is we have enhancements that detect

94
00:11:36.000 --> 00:11:41.000
that situation we'll look at how much
memory you have available on that on
your instance and it will put you in a

95
00:11:41.000 --> 00:11:49.000
forklift backup scenario the other one
is a right throttling so if you have a
lot of rights hitting your primary we've

96
00:11:49.000 --> 00:11:55.000
heard from customers that this could be
challenging because your read replicas
may fall out of sync and so we have

97
00:11:55.000 --> 00:12:01.000
controls that will detect that scenario
and it will throttle some of those
rights to make sure that your cluster is

98
00:12:01.000 --> 00:12:10.000
in sync the last one I'm going to talk
about is a smoother failover process so
say for example you have a primary you

99
00:12:10.000 --> 00:12:16.000
have a replica actually have a couple
replicas and then your primary fails one
of your replicas will be elected to be

100
00:12:16.000 --> 00:12:23.000
the new primary but any other replicas
that you have the data will be flushed
and this is in this as if you're running

101
00:12:23.000 --> 00:12:30.000
it on easy easy to what we have is we've
kind of enhanced that process to make
sure the whole failover process runs

102
00:12:30.000 --> 00:12:36.000
smoother so we don't flush the data in
the other replicas we just make sure
that it's in sync with the newly elected

103
00:12:36.000 --> 00:12:46.000
the newly elected primary talk about
some usage patterns the most popular one
is caching right so there's a couple

104
00:12:46.000 --> 00:12:52.000
reasons a couple drivers that you want
to do caching the first one is you want
to alleviate some of the pressure to

105
00:12:52.000 --> 00:12:58.000
your database now that pressure could be
maybe you maybe can't your database
can't scale it doesn't matter what's

106
00:12:58.000 --> 00:13:05.000
your database is it could be Cassandra
can be DynamoDB it could be Mongo it
could be you know our DBMS base database

107
00:13:05.000 --> 00:13:12.000
it could be anything the other reason
the other driver is maybe the
performance that you're getting out of

108
00:13:12.000 --> 00:13:19.000
your database isn't good enough right
you want to lower that leg and see when
a few lines of code you can

109
00:13:19.000 --> 00:13:25.000
you can you can augment your
architecture and add a caching layer
right and so what that caching layer is

110
00:13:25.000 --> 00:13:31.000
going to give you automatically it's
going to give you a higher level of
throughput up to 20 million reads per

111
00:13:31.000 --> 00:13:39.000
second up to 4.5 million writes per
second that's crazy the second one is
it's cost effective why do I say that

112
00:13:39.000 --> 00:13:45.000
because if you were trying to scale your
database a lot of times the cost of
scaling your back-end database which

113
00:13:45.000 --> 00:13:52.000
you'll never get the lower latency
compared to a caching system the cost of
scaling your database is a lot higher

114
00:13:52.000 --> 00:13:59.000
than adding a caching layer and then the
third one is better performance while
you're getting a micro second speed and

115
00:13:59.000 --> 00:14:08.000
today is a day and age you want to have
that fast response times the power your
applications now if you're using

116
00:14:08.000 --> 00:14:15.000
DynamoDB what's cool about this is that
you can have an automatic trigger which
is automatically going to going to

117
00:14:15.000 --> 00:14:22.000
populate the data in Alaska cash so you
can have a trigger based on an update
that's hitting your DynamoDB table that

118
00:14:22.000 --> 00:14:28.000
will put that update in a dynamo DB
stream where a lambda function will be
triggered off of that stream and it

119
00:14:28.000 --> 00:14:35.000
populates your data into elastic cash
now outside of caching this is great for
decorating your data because you might

120
00:14:35.000 --> 00:14:42.000
not want to put that data in elastic ash
in the same way that it's stored into
dynamo you might want to augment that

121
00:14:42.000 --> 00:14:48.000
data you might want to enhance or enrich
that data you can just code that into
your function now once you set this

122
00:14:48.000 --> 00:14:56.000
function it's just they're just going to
work for you this isn't it this is a way
of a right through pattern now I said

123
00:14:56.000 --> 00:15:02.000
earlier in a few lines of code you can
see you can augment your solution by
adding a cash let's see if I'm lying so

124
00:15:02.000 --> 00:15:08.000
from a right through if you see there's
two lines that are highlighted here
essentially how right through works is

125
00:15:08.000 --> 00:15:15.000
you you are writing to your system of
record to your database and it and after
you write to your database you

126
00:15:15.000 --> 00:15:23.000
immediately write that data to your cash
now what's great about a right through
pattern is that you are proactively

127
00:15:23.000 --> 00:15:31.000
filling your cat cast you're hydrating
your cash with data that is that you
think is usable now the con with doing

128
00:15:31.000 --> 00:15:37.000
this is that you are you have the
potential of putting data into the cache
and using more memory than you then you

129
00:15:37.000 --> 00:15:43.000
probably need on the other hand is
another common pattern that's lazy
loading so the way lazy loading works is

130
00:15:43.000 --> 00:15:49.000
you check your cash to see if a value is
there if it is not there you retrieve it
from your system of record your database

131
00:15:49.000 --> 00:15:59.000
and then you you set that data into your
cash now the value with that is that you
are setting the data that you know your

132
00:15:59.000 --> 00:16:06.000
application actually needs right so the
con is you have a higher chance of
hitting getting a Miss which is you know

133
00:16:06.000 --> 00:16:13.000
the data is not in your cash and you
know that's that might not be best for
your performance in practice people

134
00:16:13.000 --> 00:16:22.000
typically use both of these patterns and
they augment with a TTL and expire
parameter based on the data frequency

135
00:16:22.000 --> 00:16:27.000
the change of their data in their
database so you have to understand how
your data changes in your system of

136
00:16:27.000 --> 00:16:36.000
record and then apply a TTL that
corresponds to that data okay so we're
talking about caching another example

137
00:16:36.000 --> 00:16:42.000
I'll quickly go over a session caching
now when you're in a distributed
environment you have web applications

138
00:16:42.000 --> 00:16:50.000
it's important to to you know abstract
your sessions and put them in a
distributed cache right so this is great

139
00:16:50.000 --> 00:16:58.000
especially if you have a fleet of
servers that can you know that can grow
and shrink and based on your usage so

140
00:16:58.000 --> 00:17:06.000
based on a lot of the frameworks that
you're using say for example is PHP
example I can augment my solution with

141
00:17:06.000 --> 00:17:12.000
just changing a couple configuration
changes and I don't have to create a
session manager or do anything else I

142
00:17:12.000 --> 00:17:19.000
could just make these changes and now my
application is using in this example
memcache tea this is also true for Redis

143
00:17:19.000 --> 00:17:25.000
and it's also true for a variety of
programming languages so this is just
one example you can take a look at that

144
00:17:25.000 --> 00:17:34.000
github repo for how you can actually do
this IOT is an emerging kind of need
that we're seeing with customers so

145
00:17:34.000 --> 00:17:42.000
imagine you have a solution that you
know you have devices and you're
capturing say sensor information one of

146
00:17:42.000 --> 00:17:46.000
the ways that you could do this there's
a variety of ways that you could do it
but one of the ways that you could do it

147
00:17:46.000 --> 00:17:54.000
is you
create an AWS IOT rule that rule will
trigger a lambda function and and after

148
00:17:54.000 --> 00:17:59.000
that lambda function is triggered you
can have whatever sensor information
that's coming in you can have that

149
00:17:59.000 --> 00:18:08.000
basically persisted into your Amazon
Elastic cash engine now why is that good
it's good because you're not paying for

150
00:18:08.000 --> 00:18:14.000
requests rates you're not paying for
throughput you are not paying for
anything from up from a cost-effective

151
00:18:14.000 --> 00:18:22.000
perspective you're only paying for that
instance type that you've selected and
you can support the 20 million reads and

152
00:18:22.000 --> 00:18:30.000
the 4.5 million writes per second that's
that's fast that's going to support all
your device data now say for example you

153
00:18:30.000 --> 00:18:36.000
want to capture that data in another
repository maybe you want to have longer
retention you can always augment your

154
00:18:36.000 --> 00:18:43.000
solution and dump that data in DynamoDB
you can also create a data lake put that
data in s3 and you can do you know EMR

155
00:18:43.000 --> 00:18:51.000
jobs on top of that data or you can
archive that data into amazon glacier if
we look at this particular example for

156
00:18:51.000 --> 00:18:57.000
the IOT rule you'll see that it's pretty
simple this is a nodejs example actually
have the code checked into github as

157
00:18:57.000 --> 00:19:05.000
well you can play around with that but
the ziad is essentially how you add data
to a sensor to a sorted set so the sort

158
00:19:05.000 --> 00:19:14.000
of set is called sensor data and the
date is my score so we talked earlier
about what a score does in you know with

159
00:19:14.000 --> 00:19:21.000
Redis with a sort of set so what I want
to do here is I'm going to capture time
series data and it's important for me to

160
00:19:21.000 --> 00:19:28.000
have the actual time or the date when
that event actually occurred now when I
query this data out of the sort of side

161
00:19:28.000 --> 00:19:36.000
I'm going to do the reverse order so I
know all the values that happened in and
you know the most frequent data that I'm

162
00:19:36.000 --> 00:19:46.000
going to have that returned back to me
now the HM set is how you you persist
data into a hash and the first is my the

163
00:19:46.000 --> 00:19:53.000
first value is my key so the device ID
is my key and all those additional
attributes and values are the fields and

164
00:19:53.000 --> 00:19:59.000
values that I have associated to my hash
now I'm wrapping this into a multi
command because I just want to queue up

165
00:19:59.000 --> 00:20:07.000
these commands
is execute that transaction with retta's
another popular use case is streaming

166
00:20:07.000 --> 00:20:15.000
data so we have amazon kinesis streams
and with kanisa streams you could have a
AWS lambda function trigger as soon as

167
00:20:15.000 --> 00:20:22.000
records are unmet stream and then as i
mentioned earlier as that data is coming
out of that stream you can decorate that

168
00:20:22.000 --> 00:20:28.000
data you can do something with that data
and then you can persist that data into
Amazon Elastic ash you can always have

169
00:20:28.000 --> 00:20:34.000
an ec2 instance sitting on the right
hand side here that can query elastic
ash maybe you just want to see that

170
00:20:34.000 --> 00:20:39.000
moving data moving maybe you want to do
something with that data you can always
do that and the same as true as I

171
00:20:39.000 --> 00:20:47.000
mentioned before you can always meant
that solution and store that data in
another system of record streaming data

172
00:20:47.000 --> 00:20:53.000
enrichment is another interesting
pattern that we're seeing customers use
so imagine you have that data coming in

173
00:20:53.000 --> 00:21:02.000
to stream now you may have various data
sources that are populating that stream
with data that stream is a raw stream

174
00:21:02.000 --> 00:21:08.000
and it might not be a cleanse stream so
you want to do something with that data
before you start you know using it so

175
00:21:08.000 --> 00:21:15.000
what you can do is you can collect that
data from the stream have an AWS lambda
function trigger when records are in the

176
00:21:15.000 --> 00:21:22.000
stream persist that data in serratus say
you wanted to do dee doop de doop data
you can throw that data in a set say you

177
00:21:22.000 --> 00:21:28.000
wanted to decorate that data you can
take data that's already persisted into
read us and then based on the records

178
00:21:28.000 --> 00:21:34.000
that are coming in you can you can you
know decorate that data you can enrich
that data you could do a lot of things

179
00:21:34.000 --> 00:21:41.000
with that data and then once your data
is cleanse throw that data in a cleanse
stream and then and then you can run any

180
00:21:41.000 --> 00:21:51.000
type of operations on that stream you
can even do sequel in that stream using
Kinesis analytics spark streaming with

181
00:21:51.000 --> 00:21:59.000
Retta is an interesting use case so you
know typically when you are doing data
analytics you are you know you have

182
00:21:59.000 --> 00:22:05.000
maybe data coming in from Kinesis stream
you have maybe a spark streaming jobs
that's pulling data out of that stream

183
00:22:05.000 --> 00:22:09.000
it's summarizing that data it's
augmenting that data it's dumping that
data in

184
00:22:09.000 --> 00:22:17.000
to s3 then once it's an s3 your data
lake you could have maybe a redshift or
another EMR job pick up that data and do

185
00:22:17.000 --> 00:22:25.000
something with it now what's what's an
interesting trend here is that if you
augment your spark job with Redis you

186
00:22:25.000 --> 00:22:34.000
will by orders of magnitude speed up
that performance why because number one
polling data out of an in-memory system

187
00:22:34.000 --> 00:22:42.000
is faster than pulling data from a you
know file based system right or a SSD
based system the second reason is that

188
00:22:42.000 --> 00:22:49.000
if you think about the kind of code that
you typically write in spark you're
usually you know sorting data you're

189
00:22:49.000 --> 00:22:55.000
aggregating data you are doing some sort
of function that a lot of these advanced
data structures and reddest can help

190
00:22:55.000 --> 00:23:05.000
reduce your code complexity so from both
angles it's an interesting project to
take a look at so Retta sin a multi AZ

191
00:23:05.000 --> 00:23:11.000
environment let's take a look at that so
the first thing that I'll call out is
this is in a non-clustered environment

192
00:23:11.000 --> 00:23:16.000
so we'll talk about clustered
environments a little later in this
presentation but typically what happens

193
00:23:16.000 --> 00:23:25.000
is you have rights that you want issued
to your primary your primary is a
synchronously communicating to your read

194
00:23:25.000 --> 00:23:35.000
replicas there's a to set for multi AZ
and when you do multi AZ we will put
your read replicas in different az's and

195
00:23:35.000 --> 00:23:43.000
will enable that failover and when a
failover happens will basically take the
DNS name from your primary and will

196
00:23:43.000 --> 00:23:51.000
propagate that two to one of your read
replicas now we'll select the read
replica that has the lowest replication

197
00:23:51.000 --> 00:23:58.000
lag in your cluster now one thing that
we always recommend as well is that when
you do snapshots do them on a read

198
00:23:58.000 --> 00:24:05.000
replica so you don't interrupt or
interrupt your your master or your
primary let's take a look at that let's

199
00:24:05.000 --> 00:24:10.000
visualize it so you have your
applications they are talking to your
primary which has a border around it

200
00:24:10.000 --> 00:24:18.000
something happened to that primary dns
replication will happen the read replica
will be the new primary and then we'll

201
00:24:18.000 --> 00:24:23.000
replace that replica and in this
particular example you'll see there's
another data store

202
00:24:23.000 --> 00:24:29.000
in this case it's DynamoDB it is showing
you that you can your application can
talk to a variety of databases it

203
00:24:29.000 --> 00:24:37.000
doesn't really matter that the databases
themselves don't talk to each other now
one question that I get a lot is how do

204
00:24:37.000 --> 00:24:44.000
I know what my read replicas are
especially in an environment where your
rear up bcuz can change well we provide

205
00:24:44.000 --> 00:24:51.000
an API that allows you to query your
replication group and you can pass in a
replication ID you can easily get all

206
00:24:51.000 --> 00:24:58.000
the attributes associated to your
cluster and then once you have for
example will your replicas you can use

207
00:24:58.000 --> 00:25:04.000
them right so you can issue reads
against your replica and take advantage
of one so an example of how you could do

208
00:25:04.000 --> 00:25:13.000
that I checked into that github repo
alright so what's new so two months ago
roughly two months ago we announced a

209
00:25:13.000 --> 00:25:22.000
new feature which is Redis cluster we
support up to 3.5 terabytes and run us
we as I mentioned earlier up to 20

210
00:25:22.000 --> 00:25:30.000
million reads per second up to 4.5
writes per second all the enhancements
that we talked about earlier are rolled

211
00:25:30.000 --> 00:25:38.000
into this version it's up to four times
faster than version 2.8 and that's
because it's not based on DNS and we'll

212
00:25:38.000 --> 00:25:45.000
talk about how actually works cluster
level backup you don't have to back up
individual nodes and we support up to 15

213
00:25:45.000 --> 00:25:50.000
shards within your cluster the other
thing that I'll mention it's fully
supported by idiot with AWS

214
00:25:50.000 --> 00:25:55.000
CloudFormation if you're familiar with
that it's a template engine it allows
you to build up build environments and

215
00:25:55.000 --> 00:26:05.000
it's supported in all our AWS regions
the other thing is that there's two
additional data types that are supported

216
00:26:05.000 --> 00:26:12.000
with version 3.2 one is the bit field
command and then the other one is
geospatial I'll of geospatial I think

217
00:26:12.000 --> 00:26:19.000
it's awesome it's an awesome way to
build data aware of code or geo aware
code so say for example you have a

218
00:26:19.000 --> 00:26:26.000
mobile application and in that mobile
application you want to advertise
particular points of interest to your

219
00:26:26.000 --> 00:26:31.000
customers those points of interest could
be anything right there could be
restaurants that could be anything that

220
00:26:31.000 --> 00:26:37.000
you really want to advertise well in
that mobile application you can pass up
the longitude

221
00:26:37.000 --> 00:26:44.000
latitude of that position of that
customer and based on geo ads which are
those points of interests that you added

222
00:26:44.000 --> 00:26:53.000
in the cluster you can find all the the
points of interest within a particular
radius right so I can say all right this

223
00:26:53.000 --> 00:27:00.000
customer is in this particular location
let me find everything in a you know a
mile away from this customer and let me

224
00:27:00.000 --> 00:27:07.000
send those recommendations up to this
mobile app so we can see it now again as
I mentioned Redis is an in-memory system

225
00:27:07.000 --> 00:27:14.000
we're dealing with micro second
performance this is why this is awesome
because realistically when you have an

226
00:27:14.000 --> 00:27:21.000
application like that you want to be
able to you know to market those
advertisements as quick as possible now

227
00:27:21.000 --> 00:27:27.000
in addition to finding all the points of
interest within a particular you know
radius you can also do other things like

228
00:27:27.000 --> 00:27:35.000
what's the distance between two points
as well as other things scaling with
Redis cluster so how do you tell the

229
00:27:35.000 --> 00:27:41.000
engine that you want a horizontally
scale the first thing you do is you
check cluster mode otherwise if you

230
00:27:41.000 --> 00:27:46.000
don't check that will assume that you
want that primary and in that Reed
replica kind of vertically scale

231
00:27:46.000 --> 00:27:54.000
architecture that we looked at earlier
and before we kind of dive into how
sharding looks like let's talk a little

232
00:27:54.000 --> 00:28:04.000
bit about the client so essentially the
way sharding works is you have 16384
total hash slots now those hash slots

233
00:28:04.000 --> 00:28:13.000
are distributed across your charts now
by default that distribution is an equal
distribution so if you create 5 shards

234
00:28:13.000 --> 00:28:22.000
those hash ranges will distribute
distribute that be distributed across
those those charts now your actual

235
00:28:22.000 --> 00:28:30.000
client my client being the driver or the
client code that you're using the
connector etis has a map of every shard

236
00:28:30.000 --> 00:28:41.000
that has those hash ranges as well as
any read replicas for that individual
shark now what's also great about a

237
00:28:41.000 --> 00:28:50.000
client is that a lot of clients do they
load balance reads for you the other
thing that I'll mention about the client

238
00:28:50.000 --> 00:28:55.000
is
that because all that information is in
a map within a client you don't need DNS

239
00:28:55.000 --> 00:29:03.000
propagation all those IP addresses and
all that cluster information is built
into the map and so the client itself

240
00:29:03.000 --> 00:29:10.000
knows where to route the traffic now
this of course is much better than you
know doing things like a proxy or

241
00:29:10.000 --> 00:29:18.000
something else why because that map is
all right there with your code and so
you can eliminate your talking directly

242
00:29:18.000 --> 00:29:27.000
with the cluster and you can eliminate
any other network hop between your code
and actual the Redis engine all right

243
00:29:27.000 --> 00:29:34.000
let's visualize this so with the outside
blue border is your wettest cluster now
that cluster again can be up to 15

244
00:29:34.000 --> 00:29:44.000
shards in this example we have three
shards the the nodes that have the gray
border those are your primary charts now

245
00:29:44.000 --> 00:29:55.000
any other node that has the same range
in this example is a rear uploader so
our first our first shard has a slot

246
00:29:55.000 --> 00:30:03.000
range of 0 to 54 54 and then you can see
which are the read replicas of this
example they're in different az's and I

247
00:30:03.000 --> 00:30:11.000
have two other shards with a different
hash range associated to them now with
elastic ash you could have up to five

248
00:30:11.000 --> 00:30:21.000
read replicas for each shard and as I
mentioned up to 15 total sharks every
one of these nodes together are your

249
00:30:21.000 --> 00:30:29.000
total cluster size in this example we
have nine how do you do that right so in
the console the first thing that you do

250
00:30:29.000 --> 00:30:37.000
is you give your cluster a name so this
example my reddit Custer the second
thing you do you select the engine type

251
00:30:37.000 --> 00:30:48.000
3.24 then you will start selecting a
node sighs this is the node size for
your shark so in that case 13.5 x 3

252
00:30:48.000 --> 00:30:57.000
that's the total memory space for my
cluster and then i'm saying i want two
shards or two replicas for my shard so

253
00:30:57.000 --> 00:31:03.000
in total
nine total sharps now I mentioned
earlier by default will assume you want

254
00:31:03.000 --> 00:31:12.000
equal distribution so we have that total
hash slop range and we will divide that
range across each individual shard but

255
00:31:12.000 --> 00:31:19.000
you may have a key or something that's a
hotkey say for example you know you're
always reading from an individual key so

256
00:31:19.000 --> 00:31:27.000
we'll give you the capability to change
the slot and a key space in orientation
of your of your cluster and it will also

257
00:31:27.000 --> 00:31:34.000
by default spreads your your nodes
across a ZZZ unless you have a
particular use case where you want your

258
00:31:34.000 --> 00:31:48.000
ear your your nose in a particular AZ so
get failure scenarios so assume that
your primary fails this is an easy

259
00:31:48.000 --> 00:31:55.000
scenario right so what we do is we will
promote one of your read replicas
they'll be a new primary your previous

260
00:31:55.000 --> 00:32:05.000
you know your primary will be a rear up
waka and will will repair it your your
client code your client is aware of this

261
00:32:05.000 --> 00:32:12.000
and this typically happens within 30
seconds as far as your reads there's no
interruption assuming you have read

262
00:32:12.000 --> 00:32:19.000
replicas your application can continue
reading from the cluster you might have
some right interruption in the process

263
00:32:19.000 --> 00:32:29.000
of making that new read replica the new
primary but that's up to 30 seconds
another example is when you have two

264
00:32:29.000 --> 00:32:36.000
primaries fail within your cluster and
in this example are two or more
primaries fill in your cluster this

265
00:32:36.000 --> 00:32:43.000
example we have three told shards so two
primary shards of failing and why this
is challenging is that if you're running

266
00:32:43.000 --> 00:32:52.000
this on ec2 and we've heard customers
tell us if the majority of your
primaries fail it causes a problem and

267
00:32:52.000 --> 00:32:59.000
why does this cause a problem because
you need a majority of primaries to be
available to elect new primaries from

268
00:32:59.000 --> 00:33:06.000
read replicas we have our controls
around this so essentially we'll see you
know we'll look at your entire cluster

269
00:33:06.000 --> 00:33:13.000
health and you know if you're in a
scenario where
you don't have the majority to elect new

270
00:33:13.000 --> 00:33:18.000
primaries we have controls that can
resolve that problem builds in the
engine you don't have to do anything we

271
00:33:18.000 --> 00:33:27.000
will do this for you how do you get from
a non-clustered environment to a
clustered environment so it's pretty

272
00:33:27.000 --> 00:33:36.000
easy you essentially you take a snapshot
of your cluster and then you restore
that snapshot on each one of your

273
00:33:36.000 --> 00:33:42.000
individual shards and your cluster now
the way cluster works there's a
particular hash range right that we

274
00:33:42.000 --> 00:33:51.000
mentioned earlier on each 180 shards so
we'll discard any keys that aren't
applicable to that hash range now if you

275
00:33:51.000 --> 00:33:57.000
wanted to you know migrate from a
non-clustered to a non-clustered 3.2
version that seamless right there's a

276
00:33:57.000 --> 00:34:04.000
couple clicks and then that just that
just works the other thing that I'll
call out is that it's important to make

277
00:34:04.000 --> 00:34:10.000
sure that your client support sweaters
cluster and so you can just look up the
client that you're using and just make

278
00:34:10.000 --> 00:34:20.000
sure it supports whereas 3m cloud
formation is fully supported out of the
box so essentially building your cluster

279
00:34:20.000 --> 00:34:27.000
you know how many we've replicas you
want all that is supported so you want
to augment or automate your environments

280
00:34:27.000 --> 00:34:32.000
maybe build up an environment for tests
and dev and you know dis terminate it
you can do all that through cloud

281
00:34:32.000 --> 00:34:44.000
formation okay so best practices kind of
go through a few of these so we can
leave time here in the presentation

282
00:34:44.000 --> 00:34:52.000
these are just a few that we snuck into
the presentation first one is avoid
really short key names so I know a lot

283
00:34:52.000 --> 00:34:58.000
of people who are you know they try to
be extremely memory efficient they want
to have like the smallest you know

284
00:34:58.000 --> 00:35:03.000
abbreviated key name possible so what
they'll do is they'll pick a key name
that doesn't make sense for an

285
00:35:03.000 --> 00:35:10.000
application developer right so pick
something that has a you know a logical
schema name that's easy to code against

286
00:35:10.000 --> 00:35:19.000
the second thing is you know use hashes
lists and sets when possible these are
memory efficient collections so one

287
00:35:19.000 --> 00:35:23.000
thing that you can easily do so if you
think about it this way if you have
maybe five keys

288
00:35:23.000 --> 00:35:31.000
and you had a you know a hash that had
five values in it the hash has a smaller
memory footprint than those five

289
00:35:31.000 --> 00:35:38.000
individual keys and then the last thing
is I see some people you know they have
the keys command coded into their

290
00:35:38.000 --> 00:35:46.000
application don't do that right that is
a you know that's a blocking command
instead use like scans and you know just

291
00:35:46.000 --> 00:35:55.000
kind of iterate through that the results
that you get all right so a few things I
talked about a lot of these things just

292
00:35:55.000 --> 00:36:02.000
kind of mentioned a few of them from a
Redis cluster standpoint have a odd
number of shards now I talked about

293
00:36:02.000 --> 00:36:09.000
earlier that even in a situation where
the majority of your shards fail we will
fix that scenario we have controls

294
00:36:09.000 --> 00:36:15.000
around that but it's still good to have
an odd number because it just speeds up
the overall failure scenario the other

295
00:36:15.000 --> 00:36:23.000
thing is you know for critical workloads
you want to have a few read replicas you
know associated you know for swap uses

296
00:36:23.000 --> 00:36:29.000
you'd never want to see that you never
want to be in swap swap memory so you
always want to see that at least 02 very

297
00:36:29.000 --> 00:36:38.000
low and let's see what else for reserve
memory I mentioned in if you're running
this on ec2 that typically this can

298
00:36:38.000 --> 00:36:43.000
double your total memory footprint with
elastic ash will just recommend you know
twenty five to thirty percent reserve

299
00:36:43.000 --> 00:36:48.000
memory just to make sure you know
there's there's additional memory for
those background operations and read us

300
00:36:48.000 --> 00:36:57.000
a few things i'll call out here are some
cloud watch metrics now every cloud
watch metric you can have an alarm set

301
00:36:57.000 --> 00:37:04.000
up the first one cpu you know typically
don't go past 90 remember Retta
single-threaded so you wanted to find

302
00:37:04.000 --> 00:37:11.000
that by the number of number of cores
that you have swap usage low again this
is an in-memory system you never want to

303
00:37:11.000 --> 00:37:17.000
be in swap cache misses the hits you
want to have more hits right so if
you're getting the value out of your

304
00:37:17.000 --> 00:37:25.000
cash you want to be you want to be
finding data in the cache evictions this
is when you know Redis you know memcache

305
00:37:25.000 --> 00:37:32.000
d kind of kind of just jumps in and
starts a victim keys because you're poor
memory management you never want to

306
00:37:32.000 --> 00:37:37.000
really run into evictions unless this is
unintentional maybe you're following a
particular

307
00:37:37.000 --> 00:37:46.000
rhythm that you know like a Russian doll
caching algorithm where you want to do
this I wouldn't recommend it and there

308
00:37:46.000 --> 00:37:53.000
are addiction policies that you can you
know that you can look at and I would
select one in the case you are in an

309
00:37:53.000 --> 00:38:01.000
eviction select one that makes sense for
your application the other thing I'll
mention is for an Maxo for the clients

310
00:38:01.000 --> 00:38:09.000
you can have up to 65,000 connections
per node but it's good to have
parameters around timeout and TCP keep

311
00:38:09.000 --> 00:38:17.000
alive and make sure you're killing dead
connections or idle connections just get
rid of those and that's pretty much all

312
00:38:17.000 --> 00:38:24.000
I'm going to cover for this session the
other thing just to kind of recap amazon
elastic ash supports of variety of use

313
00:38:24.000 --> 00:38:29.000
cases we're not talking about just
cashing although that's what the name
kind of sounds like we're talking about

314
00:38:29.000 --> 00:38:36.000
a lot of use cases that support that
fast data and fast moving data second
thing is as you saw with a few lines of

315
00:38:36.000 --> 00:38:42.000
code it's very easy to augment your
solution you know you whether use a
memcache dior Redis they were lazy

316
00:38:42.000 --> 00:38:50.000
loading right through very easy a lot of
frameworks already support these caching
solutions so in some cases is just the

317
00:38:50.000 --> 00:38:58.000
configuration changes and then lastly
you can support you know terabytes worth
of data with millions of I ops so to

318
00:38:58.000 --> 00:39:07.000
really power your architectures it's an
interesting you know it's an it's an
interesting and also high ROI to augment

319
00:39:07.000 --> 00:39:18.000
that solution with putting a elastic ash
as part of that thank you guys that's
all I have for this presentation

320
00:39:18.000 --> 00:39:33.000
hey all right thanks Michael hey
everyone my name is Brian Kaiser I'm the
CTO of huddle so today I'm going to talk

321
00:39:33.000 --> 00:39:41.000
about what huddle is kind of how we got
into basic cashing our journey from
memcache d2 Redis an elastic ash and

322
00:39:41.000 --> 00:39:48.000
then some best practices we learned
along the way so huddle is a sports
platform that really allows coaches and

323
00:39:48.000 --> 00:39:55.000
lists and athletes to win with video and
analytics we were founded by myself and
two partners about 10 years ago really

324
00:39:55.000 --> 00:40:02.000
focusing on football at the professional
level nowadays we were broadly across
sports from soccer basketball football

325
00:40:02.000 --> 00:40:09.000
from kind of peewee and youth teams all
the way up to the NBA NFL in english
premier league in fact over ninety eight

326
00:40:09.000 --> 00:40:15.000
percent of american football teams use
our product and the entire English
Premier League calls us a customer now

327
00:40:15.000 --> 00:40:22.000
so it really is broadly applicable both
domestically and internationally this is
just a cool example I think of some of

328
00:40:22.000 --> 00:40:27.000
the more advanced analysis that we're
seeing the English Premier Li do around
player tracking data the really kind of

329
00:40:27.000 --> 00:40:34.000
cutting edge in the space of sports
analytics as you probably read about a
lot online so in some quick fun facts on

330
00:40:34.000 --> 00:40:41.000
our platform we have over a hundred and
thousand 130,000 teams international
using the product that I Quast over four

331
00:40:41.000 --> 00:40:48.000
and a half million active users we
actually store and serve over two
billion videos on s3 so needless to say

332
00:40:48.000 --> 00:40:56.000
we have a lot of video on s3 really
likes our usage we actually ingested in
a code over 35 hours of HD video per

333
00:40:56.000 --> 00:41:03.000
minute during our primary sports season
get that encoded and sort of back out
and we're servicing over 15,000 API

334
00:41:03.000 --> 00:41:08.000
requests per second during that same
time span and every one of those API
requests is actually multiple cache hits

335
00:41:08.000 --> 00:41:15.000
as well kind of talk about so we've been
on Amazon pretty much since the start of
huddle and in fact if you look at the

336
00:41:15.000 --> 00:41:20.000
data it's the start of Amazon is right
about the time that huddle actually
started and it really made sense for us

337
00:41:20.000 --> 00:41:26.000
right we needed the ability to scale
very quickly we handle very seasonal
traffic workloads which is a great fit

338
00:41:26.000 --> 00:41:30.000
for Amazon and we need the ability to
deliver high-performance the teams no
matter what region of the world there

339
00:41:30.000 --> 00:41:37.000
and Amazon checked all those boxes for
us we run a fairly standard micro
services architecture at huddle so we

340
00:41:37.000 --> 00:41:43.000
use an EOB as our primary entry point
that's actually spread out to our
routing layer which is really engine

341
00:41:43.000 --> 00:41:50.000
xboxes in each availability zone that
talk to eureka and eureka is a service
discovery system written by netflix it's

342
00:41:50.000 --> 00:41:56.000
a wonderful piece of open-source
software we use arica to see what
services are online what servers are

343
00:41:56.000 --> 00:42:02.000
available and what routes we need to do
and then route down to the appropriate
squad cluster in the micro service

344
00:42:02.000 --> 00:42:08.000
cluster in each applicable availability
zone now the entry point that is of
course our web tier pretty standard we

345
00:42:08.000 --> 00:42:15.000
run I is for our primary web server and
it's just an auto scaling group across
the three azs each squad is able to

346
00:42:15.000 --> 00:42:23.000
determine the supporting services to
meet their needs whether its elastic ash
dynamodb SQS whatever it may be they

347
00:42:23.000 --> 00:42:28.000
actually service their needs each squad
is able to use those amazon services and
then MongoDB is our primary data store

348
00:42:28.000 --> 00:42:36.000
at the bone at the bottom layer now
we've had caching for a long long time
and we got started with couchbase on top

349
00:42:36.000 --> 00:42:43.000
of memcache d and honestly there's a
pretty logical fit for us hopefully it's
obvious from this presentation what

350
00:42:43.000 --> 00:42:48.000
Michaels talked about that caching is
easy implement and it's very high impact
and we recognize that early on and

351
00:42:48.000 --> 00:42:53.000
honestly for us it's not just about
performance it's also about some of the
things that are a little less obvious so

352
00:42:53.000 --> 00:42:59.000
we noticed that it helped us smooth out
volatilities and things like Ibiza that
might spike or network blips that

353
00:42:59.000 --> 00:43:05.000
happened in our internal infrastructure
we also found as a very effective way
for us as a smaller startup to scale

354
00:43:05.000 --> 00:43:11.000
cost effectively without having to
increase our database you should
significantly we started making the

355
00:43:11.000 --> 00:43:17.000
transition to read us a couple years ago
and it's been quite impactful for us I
like to think of it now is kind of the

356
00:43:17.000 --> 00:43:23.000
Swiss Army knife of a Swiss Army knife
of our infrastructure not only created
that very basic key value store but also

357
00:43:23.000 --> 00:43:29.000
has so many advanced capabilities and
data structures we use it for queuing
and pub sub and now we serve over eighty

358
00:43:29.000 --> 00:43:36.000
thousand requests per second through our
through our edits clusters and preparing
for this presentation I did some

359
00:43:36.000 --> 00:43:43.000
calculations and found that our average
latency is less than one millisecond
which is pretty astounding right even in

360
00:43:43.000 --> 00:43:50.000
raised me that's one to two orders of
magnitude quicker than what we found
from our traditional database fetches so

361
00:43:50.000 --> 00:43:55.000
now I'm going to walk through some kind
of basic use cases of how we use Redis
at huddle I'm going to start with the

362
00:43:55.000 --> 00:44:01.000
most simple one which is the lazy
loading basic data caching now we put
this data caching at the lowest level

363
00:44:01.000 --> 00:44:06.000
possible right in front of the database
calls in the database functions and we
do that to allow for very easy cache

364
00:44:06.000 --> 00:44:12.000
invalidation we found that if our
caching code was spread out broadly or
up at the service layer and validation

365
00:44:12.000 --> 00:44:19.000
became very difficult so in our opinion
we like to have it as low as possible
for those in validation so here's a very

366
00:44:19.000 --> 00:44:25.000
basic get utility function that we wrote
and this isn't dotnet code one of the
things that I think is somewhat unique

367
00:44:25.000 --> 00:44:31.000
is the first chunk where it says
underscore register naval value huddle
has a system of feature toggles where we

368
00:44:31.000 --> 00:44:39.000
can turn on and off different pieces of
our system and architecture dynamically
on a per cluster basis so for example in

369
00:44:39.000 --> 00:44:44.000
Redis if we need to do maintenance on a
certain cluster if we're having any kind
of connectivity issues we can hit one

370
00:44:44.000 --> 00:44:51.000
toggle we actually use SNS for this
propagation we can turn off Redis access
across that entire cluster now we have

371
00:44:51.000 --> 00:44:57.000
some pieces in our system that are
extremely high volume and just kind of
turning off Redis buyin airily would

372
00:44:57.000 --> 00:45:02.000
cause the Thundering Herd affect
potentially in a massive low in our
database so in those cases it's much

373
00:45:02.000 --> 00:45:07.000
more of a broad range toggle where it
might be between 0 and 100 we can ramp
up and down our usage of these features

374
00:45:07.000 --> 00:45:13.000
depending on the maintenance and the
time of day things like that the next
line you see in bold there is just where

375
00:45:13.000 --> 00:45:18.000
we actually get a value from the
database this is coming back as a byte
array very very simple and then we're

376
00:45:18.000 --> 00:45:24.000
deserialising it in the bottom line in
this case we're using protobuf so we're
taking that byte array and we're

377
00:45:24.000 --> 00:45:29.000
deserializing internet object again this
is incredibly simple but i just want to
show how simple this actually is and

378
00:45:29.000 --> 00:45:35.000
this is the code we use for it on the
flip side of it here's our put method so
you can see again that toggle at the top

379
00:45:35.000 --> 00:45:41.000
that allows us to control access to
Redis we actually sir lies the object
into a byte array and then we pop it

380
00:45:41.000 --> 00:45:47.000
into the database with a TTL on it and
how do well it really depends on what
the frequency of access on our objects

381
00:45:47.000 --> 00:45:53.000
are it could be anywhere from five
minutes to an hour on the low end up to
1 27 to 30 days or the amount of time

382
00:45:53.000 --> 00:45:58.000
that we put objects in the cash for a
slightly more complicated
Apple is a utility functionary wrote to

383
00:45:58.000 --> 00:46:04.000
kind of enforce this lazy loading
pattern so this is our get input so
again we have the toggle at the top we

384
00:46:04.000 --> 00:46:11.000
go to attempt to get that key out of
cash if it's a cache hit we go ahead and
return that value right away if it's a

385
00:46:11.000 --> 00:46:17.000
Miss we're able to pass in an accessor
function which may be a DB access or
maybe a call to another microservice and

386
00:46:17.000 --> 00:46:23.000
then we put that cash key in again this
is ultra simple right we write these
functions to help enforce a good pattern

387
00:46:23.000 --> 00:46:29.000
and make sure we don't have any problems
or bugs down the road so what are some
examples of ways that we use this well

388
00:46:29.000 --> 00:46:34.000
the most common one for us is the auth
token so every call that comes in to
huddle gets authenticated right that

389
00:46:34.000 --> 00:46:40.000
auth tokens in the cache we check it
every single time so these Keys alone
are over 15,000 requests per second just

390
00:46:40.000 --> 00:46:46.000
for our auth tokens the next big one is
user information this is a perfect
example as Michael mentioned where a

391
00:46:46.000 --> 00:46:53.000
hash is a wonderful fit in Redis we can
store the relevant information for a
user like their jersey their email

392
00:46:53.000 --> 00:46:58.000
address what teams are a part of in a
simple hash function in Redis and either
get that whole object out which is

393
00:46:58.000 --> 00:47:05.000
actually those common thing for us or
individual pieces as we need it likewise
we store information on our teams this

394
00:47:05.000 --> 00:47:10.000
might be the sport they were part of the
level whether it's a high school
collegiate protein and a very simple

395
00:47:10.000 --> 00:47:17.000
hash function in Redis it just makes a
whole lot of sense so let's move on to a
little more advanced example this is our

396
00:47:17.000 --> 00:47:23.000
news feed so you can think of the news
feed kind of like well a facebook news
feed right this is essential source of

397
00:47:23.000 --> 00:47:30.000
information for our users and teams and
a good example would be a coach post a
new highlight reel or playlist or video

398
00:47:30.000 --> 00:47:36.000
for a team to watch and you can imagine
it populating into the newsfeed this is
a function that makes incredibly heavy

399
00:47:36.000 --> 00:47:42.000
use of Redis force in fact it's such a
heavy user of Redis that if Redis has a
problem or goes down for any reason we

400
00:47:42.000 --> 00:47:46.000
have to turn off this piece of
functionality our system because it
would quickly overload our database I

401
00:47:46.000 --> 00:47:52.000
think it'll make sense in a second when
I talk about the access patterns so at
the top of the page here we use a hash

402
00:47:52.000 --> 00:47:57.000
so we're storing things like the
background image for the team the
location of the team things like the

403
00:47:57.000 --> 00:48:03.000
view count for this page or for
highlight reels the number of followers
of this team and again this is a

404
00:48:03.000 --> 00:48:08.000
wonderful use of a hash often as an
example in this page load we're
retrieving that entire thing out we can

405
00:48:08.000 --> 00:48:13.000
atomically increment things like view
count and
lowers right it's very very simple but

406
00:48:13.000 --> 00:48:19.000
highly effective and highly efficient
way of storing this data and kind of the
bulk of the page here is the actual feed

407
00:48:19.000 --> 00:48:24.000
itself and I think this is a somewhat
novel approach that one of our
developers came up with for this

408
00:48:24.000 --> 00:48:32.000
essentially this is a single Redis list
of feed IDs and a little bit of metadata
around them what happens is let's say a

409
00:48:32.000 --> 00:48:38.000
coach in that previous example is
publishing a video we will go through
every single user on that team we will

410
00:48:38.000 --> 00:48:45.000
push that ID and information onto the
Redis list for that user it's very very
quick operation and then we'll trim the

411
00:48:45.000 --> 00:48:50.000
list to make sure it's at a constant
length and so in our case we keep this
list at right around five hundred items

412
00:48:50.000 --> 00:48:56.000
now when the user is scrolling through
this list we know the offset so we can
actually grab a range off that reddish

413
00:48:56.000 --> 00:49:03.000
list of the ids and information and then
we can hydrate that with those items out
of the cash in parallel with a multi get

414
00:49:03.000 --> 00:49:08.000
so i know that's a lot there but
effectively what that means is we're
able to generate this news feed and

415
00:49:08.000 --> 00:49:13.000
literally a couple milliseconds on page
load it's a very very efficient
operation for us for something that

416
00:49:13.000 --> 00:49:18.000
would be very complicated right and this
is a wonderful example of how we can use
lists and coupled with the caching and a

417
00:49:18.000 --> 00:49:26.000
multi get so let's talk about an another
example distribute a caching this is
kind of a dirty word often don't

418
00:49:26.000 --> 00:49:31.000
encourage you to use this driven caching
but when you need it it's really nice to
have a very reliable and consistent way

419
00:49:31.000 --> 00:49:37.000
to do it atomically and read us provides
that so in this example we have a lot of
coaches they're at their game and

420
00:49:37.000 --> 00:49:44.000
they're filming it with iPads fragments
of this video is flowing directly to s3
so little snippets of the game live why

421
00:49:44.000 --> 00:49:50.000
it's happening well then making calls
into a queue which get put into a very
massive worker farm we have that does

422
00:49:50.000 --> 00:49:55.000
that video encoding we talked about that
35 hours of video per minute right
that's what these jobs come in we're

423
00:49:55.000 --> 00:50:00.000
grabbing those chunks of video or
encoding the minimal qualities were
sanitizing them we're getting ready for

424
00:50:00.000 --> 00:50:06.000
streaming the challenge comes this is
all happening in parallel at a very
broad scale so these jobs come back not

425
00:50:06.000 --> 00:50:12.000
only we're writing them to MongoDB out
of this queue but we also have to use a
distributed lock to make sure that we're

426
00:50:12.000 --> 00:50:18.000
coordinating delivery and finalization
of this video and it's a small piece and
it's very very quick but it's incredibly

427
00:50:18.000 --> 00:50:22.000
important to get right so they know when
this video is ready for the user and
that's what we actually use last to cash

428
00:50:22.000 --> 00:50:27.000
for and again this is a thing that
staccato is extremely extremely well in
our experience and if you have to

429
00:50:27.000 --> 00:50:34.000
distribute locking I'd recommend you
checking it out so we're going to answer
the question why do we use elastic ash

430
00:50:34.000 --> 00:50:39.000
versus just running Redis or self and
Michael gave some great examples of the
benefits and I can tell you from

431
00:50:39.000 --> 00:50:45.000
personal experience that we've seen that
for us it's also an operational question
and an operational answer so we are

432
00:50:45.000 --> 00:50:50.000
organized as a company in a product team
like Spotify we pretty blatantly ripped
it off and I appreciate all the

433
00:50:50.000 --> 00:50:56.000
documentation that done this in fact
these are screenshots from a slide that
they've posted online and what this

434
00:50:56.000 --> 00:51:02.000
model is it's all about tribes and
squads and pushing down decision making
an autonomy as low as possible and that

435
00:51:02.000 --> 00:51:07.000
includes not just requirements gathering
and working as customers but also the
operational side whether it being what

436
00:51:07.000 --> 00:51:13.000
services you want to run how you want to
run the servers monitoring scaling and
all that so having a managed service

437
00:51:13.000 --> 00:51:21.000
like it last Akash is really critical to
allow our squads to operate autonomy
it's a perfect fit each squad can

438
00:51:21.000 --> 00:51:25.000
determine how they want to integrate
that into their cluster do they want to
use Redis cluster is it a single node

439
00:51:25.000 --> 00:51:31.000
what size do they want to be what
eviction policy makes sense for them and
how do they want to manage their

440
00:51:31.000 --> 00:51:37.000
serialization and elastic ash takes care
of all the operational complexity and it
just works and they can focus on how

441
00:51:37.000 --> 00:51:43.000
they actually want to use that service
on top of it it really just makes sense
so we had an analogy we were prepping

442
00:51:43.000 --> 00:51:48.000
for this where someone said it's kind of
like a slam dunk isn't it that's the
cheesy sports analogy but I think it

443
00:51:48.000 --> 00:51:54.000
really resonates here so I'd be remiss
if I didn't talk about Redis cluster
briefly this is a really new addition

444
00:51:54.000 --> 00:52:00.000
it's been incredibly impactful for us
I'll tell you that we don't need to use
it broadly it's kind of like a hammer

445
00:52:00.000 --> 00:52:05.000
and often it's overkill for us but we do
need it it's incredibly important it is
the last piece of the puzzle for us to

446
00:52:05.000 --> 00:52:10.000
migrate all of our caching onto elastic
cash I give this a screen shot up here
of that feed that I talked about earlier

447
00:52:10.000 --> 00:52:17.000
that is running on Redis cluster in
production today and the performance on
it is is really incredible this is 1

448
00:52:17.000 --> 00:52:23.000
screenshot that I took just prepping for
this presentation this is one single
shard out of that feed cluster and you

449
00:52:23.000 --> 00:52:29.000
can see it's running right around 1.2
percent CPU and it's doing a hundred
thousand operations per minute by

450
00:52:29.000 --> 00:52:35.000
without per second sorry buzz per minute
but on the single shard at one point to
CPU I mean that's that's really really

451
00:52:35.000 --> 00:52:41.000
incredible we're not even pushing this
operating in chard capacity all right so
it's time for me to wrap up I want to

452
00:52:41.000 --> 00:52:46.000
talk about some best practices that
we've learned over the past couple years
in elastic ash first and foremost and I

453
00:52:46.000 --> 00:52:53.000
cannot stress this enough if you're in
production use a multi easy replica
honestly you just have to at the end of

454
00:52:53.000 --> 00:52:58.000
the day you know you're running on an
infrastructure where nodes need
maintenance nodes will fail that we

455
00:52:58.000 --> 00:53:05.000
network problems and in our experience
running multi AZ has been a very very
efficient way to provide uptime our

456
00:53:05.000 --> 00:53:10.000
failure is a failover is often in the
order of magnitude of seconds I know
they talk about 30 seconds for us that's

457
00:53:10.000 --> 00:53:15.000
been a very high bar it's often much
much quicker than that multi izzy is
critical now obviously in you know dev

458
00:53:15.000 --> 00:53:22.000
and test we run single node right but
productions always multi easy Michael
had a slide up there that he went

459
00:53:22.000 --> 00:53:26.000
through kind of quickly talking about
setting up alerting that's been critical
for us and early on we didn't do a very

460
00:53:26.000 --> 00:53:32.000
good job with this and we paid the price
for it so some of the alerts that we
found very valuable swap usage we've had

461
00:53:32.000 --> 00:53:37.000
some cases where we didn't manage our
memory properly swap usage got out of
hand and all of a sudden you see

462
00:53:37.000 --> 00:53:43.000
horrible performance is a very critical
alert to set up cpu and evictions are
the two others i'd recommend alert on

463
00:53:43.000 --> 00:53:50.000
from our point of view so very important
i think it's worth taking the time to
understand available eviction policies

464
00:53:50.000 --> 00:53:55.000
and Redis depending on your use case
different policies make sense for you
there's six of them out there there's a

465
00:53:55.000 --> 00:54:00.000
really really good documentation online
so I don't need a waste your time
talking about it but for example there's

466
00:54:00.000 --> 00:54:06.000
one called all keys LRU and if you're
just going to have a cluster that only
does caching it'll take care of the

467
00:54:06.000 --> 00:54:11.000
eviction for you based on least
requested automatically you basically
don't even need ttls it's a very easy

468
00:54:11.000 --> 00:54:17.000
way to phase in whereas we use volatile
TTL for clusters that we have mixed
things like we have some distributed

469
00:54:17.000 --> 00:54:24.000
locks some caching some more persistent
objects and if we use the wrong one we
might get the wrong objects evicted from

470
00:54:24.000 --> 00:54:30.000
memory or the wrong access patterns
might be evicted it's very very
important to get that right my final one

471
00:54:30.000 --> 00:54:35.000
it's worth the time to learn and
understand ritas advanced data
structures I talked a little bit about

472
00:54:35.000 --> 00:54:41.000
our news feed when we first got into
caching we were just doing the basic
object caching key value and that is

473
00:54:41.000 --> 00:54:45.000
impactful and it makes a huge
performance difference but what really
happened in our company was almost like

474
00:54:45.000 --> 00:54:52.000
a cultural shift to take the time to
learn to understand sorted sets
choose lists hyper hyper logs and how

475
00:54:52.000 --> 00:54:57.000
those could benefit our ability to
provide higher access to our data for
our customers and things that just

476
00:54:57.000 --> 00:55:02.000
aren't possible in on traditional
database it really kind of opened our
eyes to what we could do I also think

477
00:55:02.000 --> 00:55:08.000
it's important to understand the Big O
complexity of the operations on those
data structures so when we're looking at

478
00:55:08.000 --> 00:55:13.000
design the newsfeed we had to decide did
we want to use a sorted set based on
date or did we want to use a more

479
00:55:13.000 --> 00:55:19.000
traditional list we looked at the Big O
complexity of the operations we need to
run on that object to understand what

480
00:55:19.000 --> 00:55:24.000
the performance implications are and
again this is all documented on the
reddest site very very clearly there's

481
00:55:24.000 --> 00:55:29.000
wonderful examples on there I encourage
you to check it out so with that I
really appreciate you talking any time

482
00:55:29.000 --> 00:55:36.000
I'm going to be available afterwards
answer questions I encourage you all to
fill out the recommendation forms thank
you and have a good one