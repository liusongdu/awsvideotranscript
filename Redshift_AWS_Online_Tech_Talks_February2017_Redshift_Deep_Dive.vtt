WEBVTT FILE

1
00:00:00.000 --> 00:00:08.000
welcome to today's webinar deep dive on
Amazon redshift our presenter today is
Tony Gibbs tony is a member of the big

2
00:00:08.000 --> 00:00:16.000
data specialist team at awf where he
focuses on data warehousing solutions
for Amazon redshift prior to joining the

3
00:00:16.000 --> 00:00:23.000
big data specialist team Tony work to
the software development engineer at
Amazon destination Tony holds a

4
00:00:23.000 --> 00:00:30.000
bachelor's degree in computer science
and economics from the university of
british columbia today with we also have

5
00:00:30.000 --> 00:00:37.000
Co Huong and Peter Dalton who will be
our webinar moderators today after
answering your questions through the Q&A

6
00:00:37.000 --> 00:00:48.000
panel Tony welcome the floor is now
yours hi I'm Tony Gibbs there yeah I'm a
solution architect on data warehousing

7
00:00:48.000 --> 00:00:56.000
for Amazon redshift and today I'm going
to be talking you guys a little bit
about query lifecycle and parallelism

8
00:00:56.000 --> 00:01:05.000
and kind of the storage subsystem in red
shift so just get started here just kind
of looking over the agenda we're going

9
00:01:05.000 --> 00:01:11.000
to start out just kind of with a kind of
an overview of the history and
development of redshift just to kind of

10
00:01:11.000 --> 00:01:18.000
give you guys a bit of just a
overview really then we're going to jump
into the cluster architecture so you

11
00:01:18.000 --> 00:01:24.000
guys kind of a get a bit of an
understanding of how redshift works then
we're going to jump in a kind of concept

12
00:01:24.000 --> 00:01:32.000
terminology so that we can kind of build
up towards the rest of the webinar then
we're going to jump into the kind of

13
00:01:32.000 --> 00:01:37.000
parallelism and that's kind of where we
get into the query lifecycle and all
that good stuff and then at the end I

14
00:01:37.000 --> 00:01:45.000
put in for you guys a bit of the new and
upcoming features that are coming to redshift
and we'll cover that as well then

15
00:01:45.000 --> 00:01:54.000
we'll wrap up with a QA so just starting
with the redshift development in history
it really kind of starts with PostgreSQL

16
00:01:54.000 --> 00:02:01.000
which is you know if you connect to
redshift you'll see a connection string
come back and it will actually say that

17
00:02:01.000 --> 00:02:09.000
it's Postgres 8.0 and so that's
kind of a bit of the foundation it looks
and feels a lot like postgres the SQL

18
00:02:09.000 --> 00:02:17.000
is postgres-like SQL it's very similar to
PostgreSQL it is an see compatible so it's

19
00:02:17.000 --> 00:02:23.000
all the sequel you you know you love and
have probably used in the past if you've
used other relational databases

20
00:02:23.000 --> 00:02:36.000
obviously redshift is a lot more than
just postgres you know we've
wrapped it all up in AWS there's

21
00:02:36.000 --> 00:02:45.000
we've added a ton of features to it
these are like analytics features we've
made it MPP we have also at

22
00:02:45.000 --> 00:02:50.000
made it column or storage so that's
really the biggest changes is you know
the column or storage and kind of wrap

23
00:02:50.000 --> 00:03:01.000
all this together and that's really what
gives you amazon redshift since we
released a redshift which was actually

24
00:03:01.000 --> 00:03:12.000
on Valentine's Day in February 2013 just
a little over 4 years ago we've done a
hundred patches now or more than

25
00:03:12.000 --> 00:03:19.000
these not just like
little bug fixes and things like that we
actually release new features every two

26
00:03:19.000 --> 00:03:25.000
weeks to redshift and we're
going to cover some they recently
released features at the end of this

27
00:03:25.000 --> 00:03:31.000
webinar and I'll kind of go through what
we've recently released but there's just
you know we're constantly innovating and

28
00:03:31.000 --> 00:03:37.000
adding more to redshift and these are
you know we roll the patches out you
don't have to do any maintenance

29
00:03:37.000 --> 00:03:43.000
yourself it's all taken care of by the
redshift team so really over the last
for years redshift is just

30
00:03:43.000 --> 00:03:51.000
simply gotten bigger and better so we'll
jump into the cluster architecture this
is kind of just like high level here to

31
00:03:51.000 --> 00:03:57.000
give you guys some context of
how it works if you've never used redshift
before so redshift is split into

32
00:03:57.000 --> 00:04:05.000
kind of two parts there's the part that
you interact with mainly which is the
leader node that's the bit that looks

33
00:04:05.000 --> 00:04:13.000
and feels like postgres if that's what
you connect to it your JDBC or
odbc drivers you can technically connect

34
00:04:13.000 --> 00:04:20.000
with any postgres compatible drivers such as
lid PQ or if you want to use P sequel if you're used

35
00:04:20.000 --> 00:04:27.000
to that you know those all those drivers
connect to the leader note
behind the leader node is where all

36
00:04:27.000 --> 00:04:36.000
the data sets in other nodes that we
call compute nodes and we store the data
down in these nodes and when you issue a

37
00:04:36.000 --> 00:04:41.000
query all of these nodes are the ones
that actually do all of the work and
resent up kind of the results of the

38
00:04:41.000 --> 00:04:50.000
leader node the important concept here
is that redshift is a share nothing
MPP architecture and what that means is

39
00:04:50.000 --> 00:04:58.000
all of these compute nodes work together
all at the same time all in parallel and
it's really about the parallelism

40
00:04:58.000 --> 00:05:04.000
that's really important so there's one
takeaway from this it's kind of
understand that this is an MPP parallel

41
00:05:04.000 --> 00:05:16.000
architecture jumping into kind of the
components and what makes up these what
makes up kind of the bits of these

42
00:05:16.000 --> 00:05:24.000
compute and leader nodes is as I kind of
mentioned on the leader node you know we
have like the parser that's where the

43
00:05:24.000 --> 00:05:29.000
queries of parts that's where we rewrite
and you know the planner and the
optimizer and all that good stuff sits

44
00:05:29.000 --> 00:05:36.000
we also have all the postgres catalog
tables so if you're used to the
PG tables and the kind of postgres

45
00:05:36.000 --> 00:05:45.000
catalog those actually also sit on the
leader known as well then down on the
compute nodes as I mention this is where

46
00:05:45.000 --> 00:05:51.000
all the data sits in this webinar we're
going to focus quite a bit on this
section which is kind of the storage

47
00:05:51.000 --> 00:06:02.000
subsystem and we're going to go through
this more in depth so just to kind of
fill in the concepts of you know column

48
00:06:02.000 --> 00:06:07.000
architecture if that's not something
you're familiar with we're just going to
kind of step through comp how you know

49
00:06:07.000 --> 00:06:13.000
what makes redshift unique it's are
compared to say like a traditional
database that will

50
00:06:13.000 --> 00:06:21.000
a SQL database and maybe like postgres or
oracle or something like that so the big
thing is that it's a column our

51
00:06:21.000 --> 00:06:30.000
architecture what that means is that we
actually store the data on disk column
by column rather than row by row this

52
00:06:30.000 --> 00:06:37.000
has an advantage in that if say you only
wanted to read a single column off disk
maybe just want in this

53
00:06:37.000 --> 00:06:43.000
particular case only to read just the
date-time for example off of disk what
would end up happening is we don't have

54
00:06:43.000 --> 00:06:50.000
to read just that data and this
basically results in a reproduction in
Aisle I mean in this case you're only

55
00:06:50.000 --> 00:06:57.000
going to end up having to read one third
of the data in this particular example
and that's one of

56
00:06:57.000 --> 00:07:04.000
the hugest benefits to redshift for
the types of queries that you you know
run on a data warehouse the next is data

57
00:07:04.000 --> 00:07:13.000
compression because we store the data
column by column it makes it really easy
to compress it effectively we can apply

58
00:07:13.000 --> 00:07:20.000
different compression settings to the
different data types for each column
independently and they can be

59
00:07:20.000 --> 00:07:27.000
compressed is effectively as they can be
so if you kind of look in the top right
corner there where we have the ddl for

60
00:07:27.000 --> 00:07:35.000
the create table you'll notice there's
the encode statements that are now
saying code lzo in code bite dick and

61
00:07:35.000 --> 00:07:43.000
code run length those are encoding types
that's that State the compression so
that's how you put that into your

62
00:07:43.000 --> 00:07:49.000
compression now the next is is that we
have what are called zone maps and I'm
going to cover this a little bit more in

63
00:07:49.000 --> 00:07:58.000
depth because it's a very important
concept but just high level what it is
is we store the min and Max values for

64
00:07:58.000 --> 00:08:05.000
what we call blocks and we're going to
get all into blocks on stuff later but
what a block is is if you look at those

65
00:08:05.000 --> 00:08:11.000
orange and white kind of squares there
that are underneath each of the
columns each one of those is meant to

66
00:08:11.000 --> 00:08:19.000
represent a block which is one Meg
chunks of data for each column and then
what we do is is in memory we store min

67
00:08:19.000 --> 00:08:25.000
and Max values for each of them when a
query comes in say a predicate like
we're going to filter some data we can

68
00:08:25.000 --> 00:08:35.000
check those min and Max values in memory
before we ever hit disk and that allows
us to eliminate data or reduce IO I

69
00:08:35.000 --> 00:08:43.000
should say and just to kind of give 
a bit of a better example or to
illustrate this imagine we have some

70
00:08:43.000 --> 00:08:50.000
table that has these four
blocks in this particular case with some
data it's just written arbitrarily

71
00:08:50.000 --> 00:08:56.000
whatever order what would happen is if we 
came along with some SQL query that looks

72
00:08:56.000 --> 00:09:04.000
something like this and we just want to
know do a count where this data
equals jun 2013 we can check

73
00:09:04.000 --> 00:09:11.000
this in-memory data structure and we can
immediately prune out large important
part of the data or well a little bit in

74
00:09:11.000 --> 00:09:20.000
this case this leads to sorting which is
a different construct and redshift that
will touch on a little bit but what it

75
00:09:20.000 --> 00:09:27.000
does is if you imagine you sorted this
your data by this timestamp you might
end up with some data that looks

76
00:09:27.000 --> 00:09:34.000
something like this now running that
same query against this sorted data we
can these zone maps become more

77
00:09:34.000 --> 00:09:43.000
effective and we can basically eliminate
all about one block so that's one of the
ways that redshift can get really good

78
00:09:43.000 --> 00:09:55.000
performance picking a sort key is really
kind of based a bit around your business
requirements your your query

79
00:09:55.000 --> 00:10:01.000
patterns that sort of thing in a lot of
cases if you have time series data
you're typically filtering between two

80
00:10:01.000 --> 00:10:07.000
different time stamps you
usually have some sort of maybe
it's a clickstream data or something

81
00:10:07.000 --> 00:10:14.000
like that or maybe some some
other time series data maybe sales data
that's based on time and a lot of times

82
00:10:14.000 --> 00:10:21.000
you're going to end up filtering it off
of a timestamp so in a lot of cases that
first column which is your timestamp

83
00:10:21.000 --> 00:10:29.000
will end up being your sort key and the
goal here really is to make the zone
maps more effective and to reduce IO

84
00:10:29.000 --> 00:10:39.000
that's kind of the big thing about
what sorting does for you and how
it kind of relates to is on maps next

85
00:10:39.000 --> 00:10:48.000
we're going to talk about is a
concept that we have called slices you
can think of a slices a virtual compute

86
00:10:48.000 --> 00:10:54.000
node and redshift I kind of
mention you and we have the leader knows
we have the compute node we split those

87
00:10:54.000 --> 00:11:00.000
compute nodes up into what we call
slices which are kind of virtual compute
nodes and that's how we get the 

88
00:11:00.000 --> 00:11:08.000
parallelism within each compute node
they're basically a collection of
their own software

89
00:11:08.000 --> 00:11:16.000
processes we store data by slice so the
data is written sliced by slice so one
slice doesn't access on other slice of

90
00:11:16.000 --> 00:11:25.000
data it just gets its own the next is 
depending on the size of the
compute node we know thus the compute

91
00:11:25.000 --> 00:11:32.000
nodes you to come in 2, 16 or 32 slices
it's not configurable it's kind of hard
coded for the type of compute nodes that

92
00:11:32.000 --> 00:11:45.000
you pick this kind of leads to data
distribution how do you distribute data
across all these slices in the cluster

93
00:11:45.000 --> 00:11:52.000
there's three different ways that we
have to do this in redshift the first
is to distribute by key what this

94
00:11:52.000 --> 00:12:01.000
essentially does is you pick a column in
your table hopefully a high cardinality
column and we basically hash that value

95
00:12:01.000 --> 00:12:07.000
and obviously send it down to
do like a module kind of
operator on it and send it down to a

96
00:12:07.000 --> 00:12:14.000
particular slice in the cluster so for
example like if you we're going to go
through kind of an example of how that

97
00:12:14.000 --> 00:12:23.000
works in a couple of slides so don't go
too far much further than that the next
is EVEN even is a distribution style

98
00:12:23.000 --> 00:12:30.000
that really just round robins the data
across all the slices
in the cluster so if you for example

99
00:12:30.000 --> 00:12:36.000
where like I don't have a very good disk
key to distribute on I don't have
something that's high cardinality I

100
00:12:36.000 --> 00:12:43.000
don't have a unique key you can just
pick distribution style EVEN and we'll
take care of it for you if for example

101
00:12:43.000 --> 00:12:51.000
in the next case we also have what's
called distribution style ALL which is
base makes a full copy of the table on

102
00:12:51.000 --> 00:12:59.000
each node in the cluster typically used
for dimension tables and stuff so just
to kind of illustrate these three

103
00:12:59.000 --> 00:13:07.000
different keys we're going to kind of
jump into some examples
the first example that we're going to

104
00:13:07.000 --> 00:13:14.000
end up doing here is going to be
distribution style even so if you look
on the top right hand side up there

105
00:13:14.000 --> 00:13:20.000
you're going to see a bunch of insert
statements there and that's kind of the
fake fictitious data we're going to use

106
00:13:20.000 --> 00:13:26.000
and then we have the create table
statement you can see on the left there
we have what's called DISTSTYLE EVEN

107
00:13:26.000 --> 00:13:33.000
there that's how you can put
that distribution style into the DDL
that you're normally would with your

108
00:13:33.000 --> 00:13:41.000
create table statement so just moving
through this example with DISTSTYLE EVEN
what's going to end up

109
00:13:41.000 --> 00:13:46.000
happening is as i mentioned we're
just going to round robin the data
through the cluster and it's just going

110
00:13:46.000 --> 00:13:55.000
to move through in a very nice even
fashion if we pick distribution style
key for example you can kind of see in

111
00:13:55.000 --> 00:14:03.000
the red there that's highlighted to our
to highlight a pic distribution key
location so what that is is if you look

112
00:14:03.000 --> 00:14:11.000
over in the data that we're inserting we
have you know SFO JFK SFO JFK
some airport codes it looks like if we

113
00:14:11.000 --> 00:14:20.000
distributed on this what's going to
happen is we're going to get something
that looks a bit like this this is an

114
00:14:20.000 --> 00:14:26.000
example of picking a bad distribution
key what's ended up happening here is
because there's only two unique values

115
00:14:26.000 --> 00:14:36.000
in that incoming data we can only
distribute to two of the four slices in
this cluster what that means is if

116
00:14:36.000 --> 00:14:45.000
say you were to run a query against this
this cluster you'd end up only using one
of your two nodes so it's kind of a

117
00:14:45.000 --> 00:14:51.000
distribution keys are the one thing
you want to make sure you get right over
anything else or you want to make sure

118
00:14:51.000 --> 00:14:59.000
you don't get them wrong i should say if
you're in doubt just pick distribution
style EVEN moving on i'm going to try

119
00:14:59.000 --> 00:15:07.000
what if we pick you the primary
key here this 'aid' which is
unique values assuming the hashing

120
00:15:07.000 --> 00:15:12.000
worked out well we're going to get
something like this the data is
going to be evenly distributed across

121
00:15:12.000 --> 00:15:21.000
the cluster
now distribution style ALL on the other hand
how it works is 

122
00:15:21.000 --> 00:15:31.000
we're going to actually load the data all 
up kind of something like this 
so basically there's a complete copy of it on each node in the

123
00:15:31.000 --> 00:15:43.000
cluster 
so just kind of recap and
explain where you use each of these
different distribution styles you use

124
00:15:43.000 --> 00:15:50.000
distribution style key if you want it
has to meet the criteria that
it results in an even distribution

125
00:15:50.000 --> 00:15:59.000
this is a high cardinality value
something that's you know not got like a
kind of hotspots in it 
if that 

126
00:15:59.000 --> 00:16:06.000
criteria is met then you can kind of continue
down looking for you no data to what
kind of distribution style you want to

127
00:16:06.000 --> 00:16:13.000
look at or you can look at further down
key so then at that point you would
decide would you for example

128
00:16:13.000 --> 00:16:20.000
are you joining this table with another
large table if that's the case and that
particular key that you're joining on

129
00:16:20.000 --> 00:16:28.000
happens to give this nice even
distribution then that's a really good
key to key off of the next is if you're

130
00:16:28.000 --> 00:16:36.000
also doing say like group bys on that
key that can also help with performance
or if you're optimizing for emergent

131
00:16:36.000 --> 00:16:42.000
which is kind of a special case which
I'm not going to get into in this but
it's basically if you have two large

132
00:16:42.000 --> 00:16:47.000
fact tables that you were just say
joining together and this would be like
maybe hundreds of billions of

133
00:16:47.000 --> 00:16:55.000
rows joined with another table hundreds of billions of rows 
so kind of a special case 
our guidance for distribution style

134
00:16:55.000 --> 00:17:01.000
ALL is if the table has less than two or
three million rows you can go
ahead and use distributions that all if

135
00:17:01.000 --> 00:17:10.000
none of the above ply then go with
distribution style even as I mentioned
before so we're going to get into the

136
00:17:10.000 --> 00:17:20.000
storage system and redshift one thing
that I don't think we really advertise
this in any way mainly because we try

137
00:17:20.000 --> 00:17:27.000
and keep redshift really simple you know
to use but if you're going to say
migrating from maybe some on Prem data

138
00:17:27.000 --> 00:17:32.000
warehouse or something like that it kind
of
you know just to kind of think of it

139
00:17:32.000 --> 00:17:40.000
like this we actually have two and a
half to three times more storage on the
nodes than we advertised one of the

140
00:17:40.000 --> 00:17:48.000
reasons why we do that is that we mirror
the data in red shift so we always keep
two copies of your data so when you

141
00:17:48.000 --> 00:17:56.000
write which say you do like an insert
into redshift just you know you insert
some data when that commit happens your

142
00:17:56.000 --> 00:18:04.000
data is safely on two different nodes in
the cluster so that's why we allocate
that extra space then we also have all

143
00:18:04.000 --> 00:18:11.000
the space allocated for you know the
operating system and temp scratch space
and things like that so that's why we

144
00:18:11.000 --> 00:18:19.000
have the two-and-a-half to three times
more storage than we advertise this
space that we advertise for redshift

145
00:18:19.000 --> 00:18:26.000
like say for example in our smallest
know type which is you know DC one large
is 160 gigs that's the usable space that

146
00:18:26.000 --> 00:18:39.000
you get the next bit that we're going to
talk about is what are called blocks and
I've already kind of mentioned them a

147
00:18:39.000 --> 00:18:44.000
couple times because it's actually hard
to have the conversation about redshift
and storage and how data is stored

148
00:18:44.000 --> 00:18:53.000
without talking about blocks the blocks
in red shifter they're basically one Meg
mutable chunks of data are stored column

149
00:18:53.000 --> 00:19:00.000
by column and as I already mentioned you
know we store the in-memory bits about
them in metadata essentially which is

150
00:19:00.000 --> 00:19:08.000
the min and max value and stuff like
that these blocks can be compressed you
know we have this one of its actually 11

151
00:19:08.000 --> 00:19:15.000
encodings now which we'll get to and
they recently release we just released a
new encoding type so you can set the

152
00:19:15.000 --> 00:19:22.000
encoding type for each columns of all of
the blocks for that particular column
will be encoded with that type and then

153
00:19:22.000 --> 00:19:28.000
you know then you can store up to you
know we can throw up to 8.4 million
values into a single block I think

154
00:19:28.000 --> 00:19:38.000
that's it you're storing boolean for
example
then kind of that kind of leads to well

155
00:19:38.000 --> 00:19:45.000
you have these blocks well what about if
you have a whole bunch of them what we
do is we call that the block chain so

156
00:19:45.000 --> 00:19:53.000
there's basically a blockchain for each
column on each slice there's technically
to block chains if you have a sort key

157
00:19:53.000 --> 00:20:00.000
on a table 14 the sorted region 14 the
unsorted region and then these columns
can all grow independently one block at

158
00:20:00.000 --> 00:20:08.000
a time from other columns depending on
you know the types of compression that
you have set it's also important to note

159
00:20:08.000 --> 00:20:16.000
that redshift is you know acid compliant
and all those sorts of things so you
know everything you know we use MVCC and

160
00:20:16.000 --> 00:20:23.000
redshift and so there's three other
columns to help with that that you
actually don't see although you can kind

161
00:20:23.000 --> 00:20:30.000
of you can see that they exist but
they're not queryable and those are the
row ID the deleted ID and the

162
00:20:30.000 --> 00:20:37.000
transaction ID so we keep track of that
as well so it's kind of important to
know that there are three extra columns

163
00:20:37.000 --> 00:20:49.000
and redshift that when you create a
table so because redshift is a data
warehouse it it is optimized for you

164
00:20:49.000 --> 00:20:56.000
know batch processing you know you can
load massive amounts of data you know
small right and redshift is cost

165
00:20:56.000 --> 00:21:03.000
somewhat similar to a large right most
people will usually be loading you know
on the order of millions of records per

166
00:21:03.000 --> 00:21:11.000
copy when copying data in and that's
just being the way it's designed you
know it's designed for really big data

167
00:21:11.000 --> 00:21:21.000
you can scale redshift all the way up to
you know storing to petabytes of
compressed data so typically with our

168
00:21:21.000 --> 00:21:30.000
compression ratio that we get it's about
six petabytes of data in a single
redshift cluster now just kind of to

169
00:21:30.000 --> 00:21:38.000
note a little bit about updates and
deletes and because of the fact that it
is MVCC is is that an update essentially

170
00:21:38.000 --> 00:21:46.000
is a delete and then an insert at the
end of the table so that means that
records do deleted records do need to be

171
00:21:46.000 --> 00:21:57.000
cleaned up and we have a
there's a process for that called out
vacuum so just a little bit kind of into

172
00:21:57.000 --> 00:22:06.000
the design considerations here the
column properties you know that the bits
that really influenced performance are

173
00:22:06.000 --> 00:22:16.000
the first thing is is dis keys they have
the largest or can have the largest
impact on performance the next sort keys

174
00:22:16.000 --> 00:22:23.000
they would be the next and then
compression after that so if for example
you're tuning redshift you you know

175
00:22:23.000 --> 00:22:28.000
maybe running a POC some sort of bake
off and it's just not performing
anything like what you're expecting

176
00:22:28.000 --> 00:22:35.000
definitely take a look at the
distribution keys and because that would
be likely the case that could that would

177
00:22:35.000 --> 00:22:41.000
be wrong or it can be and then the next
would be sort keys but those are the two
that have orders of magnitude difference

178
00:22:41.000 --> 00:22:50.000
in performance so we're going to jump
into kind of the parallel ISM side of
redshift now we've kind of built up this

179
00:22:50.000 --> 00:22:57.000
foundation of how storageworks and all
of that sort of stuff I kind of
mentioned this we kind of saw this slide

180
00:22:57.000 --> 00:23:04.000
earlier before but didn't really go too
much into it and what we're going to do
is in the next little bit we're going to

181
00:23:04.000 --> 00:23:10.000
talk about the query lifecycle how
queries move through redshift how data
kind of comes from the compute nodes and

182
00:23:10.000 --> 00:23:19.000
back up so this kind of bit here is
already talking about the what happens
in redshift is when you throw a query at

183
00:23:19.000 --> 00:23:25.000
redshift the parser obviously takes the
query parses the query we actually
rewrite the query once this is kind of

184
00:23:25.000 --> 00:23:32.000
to rip out things like sequel syntactic
sugar and things like that we then feed
this rewritten query to our planner

185
00:23:32.000 --> 00:23:39.000
which the planner basically picks out
gets a whole bunch of plans the
optimizer using statistics that are in

186
00:23:39.000 --> 00:23:45.000
the database as long as you're keeping
statistics up-to-date in the database as
a command for that call to analyze

187
00:23:45.000 --> 00:23:52.000
statistics or just analyze but you want
to analyze statistics and well what that
does is it gives things like the

188
00:23:52.000 --> 00:24:01.000
cardinality you know of the column kind
of samples a bit of data about it and
that allows the optimizer

189
00:24:01.000 --> 00:24:08.000
to you know pick the correct or
hopefully the correct plan that is going
to fulfill the query get the results as

190
00:24:08.000 --> 00:24:16.000
fast as possible we then pass that
through to a code generator which
generates a bunch of C++ code and when

191
00:24:16.000 --> 00:24:22.000
we'll send that down to the compute
nodes in every compute node will then
execute that C++ code and sign up the

192
00:24:22.000 --> 00:24:32.000
results so just to kind of go through
these how this kind of works a little
bit deeper going to start talking at the

193
00:24:32.000 --> 00:24:42.000
bottom there at this slide which is a
stream the stream is basically a set of
of these C++ programs that i talked

194
00:24:42.000 --> 00:24:52.000
about these c++ binaries so there could
be multiples of them in a single stream
each of these streams is basically built

195
00:24:52.000 --> 00:25:00.000
up of segments actually those are the
C++ bits of code and then obviously each
of them is built up of a bunch of steps

196
00:25:00.000 --> 00:25:08.000
so just to kind of visualize this
imagine you had some query maybe this
query has a you know a couple of sub

197
00:25:08.000 --> 00:25:15.000
queries and a join and things like that
and to fulfill this query we're going to
end up needing to generate a hand full

198
00:25:15.000 --> 00:25:22.000
of streams what's going to end up
happening is is to take a look at the
top one you have stream 0 the redshift

199
00:25:22.000 --> 00:25:29.000
would have in this case generated four
segments which are basically individual
kind of C++ programs these can run

200
00:25:29.000 --> 00:25:35.000
independently all at the same time which
is really awesome to get more parallel
ISM and then each of them is built up of

201
00:25:35.000 --> 00:25:48.000
a bunch of steps then what's going to
happen is is obviously you can't move
from one stream until the next stream

202
00:25:48.000 --> 00:25:59.000
until you know if the previous stream is
complete so kind of to illustrate all of
this imagine you come along with some

203
00:25:59.000 --> 00:26:05.000
client and you want to you know you
issue some query with your favorite
driver or whatever you're connected with

204
00:26:05.000 --> 00:26:12.000
jdbc odbc and you connect to the leader
note the leader nodes obviously going to
parse that query you know you're going

205
00:26:12.000 --> 00:26:18.000
to get some query plans you know
explain plans we're going to figure out
the best one to have you're going to hit

206
00:26:18.000 --> 00:26:24.000
the code generator the code generator is
going to basically generate some code
and send it all down to the compute

207
00:26:24.000 --> 00:26:31.000
nodes the compute nodes are then going
to execute this query and you know
they're going to go through all the

208
00:26:31.000 --> 00:26:36.000
segments and all the steps in the
segment then they're going to send out
the results you know once this stream is

209
00:26:36.000 --> 00:26:43.000
complete then what's going to end up
happening is in this example we're going
to end up generating a second stream

210
00:26:43.000 --> 00:26:52.000
using those results once that happens
we're going to get the results sent back
down and we're going to this is the

211
00:26:52.000 --> 00:26:58.000
second stream executing here and then
we're going to send that back up again
and there's no more streams at this

212
00:26:58.000 --> 00:27:04.000
point so then we're just going to send
back the actual final results now
there's an important thing here then it

213
00:27:04.000 --> 00:27:11.000
kind of happens so imagine you were
doing maybe some summation of data you
were you know you were doing some sort

214
00:27:11.000 --> 00:27:18.000
of some some order buys and there's
maybe a limit at the end of this and all
that kind of stuff the leader node

215
00:27:18.000 --> 00:27:24.000
actually does the last bits of this
right because both of the compute nodes
they might have summed up all their data

216
00:27:24.000 --> 00:27:31.000
and then you send it to the leader node
and then it kind of has to do that kind
of final summation and then maybe you

217
00:27:31.000 --> 00:27:36.000
have an order by in there and then the
leader node ends up doing the order by
and so you know applying the limit and

218
00:27:36.000 --> 00:27:46.000
that sort of stuff once the leader node
has done these things it then sends the
final results back up to the client

219
00:27:46.000 --> 00:27:53.000
there's also one extra bit that I didn't
really bring up earlier in this and
that's that we if any of you have used

220
00:27:53.000 --> 00:27:58.000
red shift before you kind of come along
and you'll type out some queer maybe say
select count from you know some table

221
00:27:58.000 --> 00:28:06.000
and you notice that the first time you
run it it it takes like an extra couple
of seconds what that is is that's the

222
00:28:06.000 --> 00:28:16.000
compilation step that you're waiting for
and that's what we end up doing is we
cash those compiled plans so that's why

223
00:28:16.000 --> 00:28:22.000
the second time you run it it's really
fast even if you say for example you
came along he wrote a query and you had

224
00:28:22.000 --> 00:28:27.000
a filter of some sort of predicate I'm
you were like where something equals
something and the first time it

225
00:28:27.000 --> 00:28:35.000
is that a compilation delay after that
what's going to end up happening is even
if you change the parameters in the

226
00:28:35.000 --> 00:28:44.000
query in your in your where clause yeah
it'll still use the same compiled query
so that's you know one thing to be aware

227
00:28:44.000 --> 00:28:53.000
of is that we do cash the compiled
binaries and they'll even persist
through cluster reboots and everything

228
00:28:53.000 --> 00:29:00.000
like that the only time we really wipe
them out is in your in the maintenance
cycle so they do get wiped out you know

229
00:29:00.000 --> 00:29:08.000
when we come along every couple weeks
and go through maintenance these
connects couple slides they're basically

230
00:29:08.000 --> 00:29:16.000
what I just talked about this slide and
the next one so if you want to you know
when this gets put up on to you know if

231
00:29:16.000 --> 00:29:21.000
you just want to read through the slides
at some point maybe you're up on the
slide show you can kind of read through

232
00:29:21.000 --> 00:29:33.000
so I've kind of just put up all the
notes to explain what's happening and
this is just kind of a repeat of what

233
00:29:33.000 --> 00:29:38.000
the you know the segments and steps and
sort of but sort of thing is and the
reason why is to kind of show you you

234
00:29:38.000 --> 00:29:47.000
know obviously this is happening across
all of the slices across all the nodes
in the cluster at the same time so

235
00:29:47.000 --> 00:29:54.000
continuing the theme of parallel ism if
you know you guys are looking at maybe
loading data into red shift or trying it

236
00:29:54.000 --> 00:29:59.000
obviously you know you guys are
interested in because you're here if
you're looking a load data into redshift

237
00:29:59.000 --> 00:30:07.000
one of the things with that kind of you
know you have a good foundation of
slices is the loading data what we're

238
00:30:07.000 --> 00:30:16.000
going to having here is an example with
a fictitious costs are just a single one
node cluster which is a d/s to 8xl this

239
00:30:16.000 --> 00:30:24.000
particular cluster has 16 slices in it
because that's what that node type has
and what we've done here is is imagine

240
00:30:24.000 --> 00:30:32.000
we have a CSV file that we're going to
load into the cluster just a single one
and so we go hey redshift you know

241
00:30:32.000 --> 00:30:38.000
here's some CSV file sitting on s3 go
load this and so what will happen is
it's one of the slices in the cluster

242
00:30:38.000 --> 00:30:44.000
will go and download the file
and it'll you know pull it up and parse
it and distribute it and get it written

243
00:30:44.000 --> 00:30:52.000
across the whole cluster and that will
work just fine the problem is is that
you're only using one sixteenth of the

244
00:30:52.000 --> 00:31:00.000
cluster you know that one slice is
essentially doing pretty much all the
work so if we took that same file maybe

245
00:31:00.000 --> 00:31:08.000
it just pretend it's like a one gig file
or something and we split it up into 16
files you know maybe whatever your ETL

246
00:31:08.000 --> 00:31:16.000
process is to get the data on s3 before
you drop the thousand s3 you've split it
up into 16 files then in this case if

247
00:31:16.000 --> 00:31:23.000
you go hey redshift copy these 16 files
in what redshift will do is it'll
basically get every slice in the cluster

248
00:31:23.000 --> 00:31:34.000
to load a file so the guidance around
here kind of is that you'll want to have
one file per slice or some multiple

249
00:31:34.000 --> 00:31:41.000
number of files that's you know
divisible by the number of slices of you
have in the cluster so in this case you

250
00:31:41.000 --> 00:31:49.000
have 32 files that would be totally fine
that just means each slice gets two
files to work on or you know keep going

251
00:31:49.000 --> 00:31:56.000
up you know you have 60 for files or
whatever happens to be and that will
completely work well the files ideally

252
00:31:56.000 --> 00:32:06.000
should be at least one Megan size and
less than one gig so between one meg and
one gig after gzip compression you

253
00:32:06.000 --> 00:32:14.000
definitely get a big benefit from g
zipping the files it reduces the amount
of network io that we need to do and so

254
00:32:14.000 --> 00:32:25.000
yeah huge benefit to do that so kind of
I think this is a bit of the fun part up
here we're going to kind of go through

255
00:32:25.000 --> 00:32:35.000
just some of the new stuff that we've
released the first one is that we this
is last October time frame we release

256
00:32:35.000 --> 00:32:41.000
what we call node failure tolerance so
this is like my node fails and will hold
all the connections and everything now

257
00:32:41.000 --> 00:32:48.000
so that you know well the wall with the
clusters being fixed up you know it
takes on the order of minutes to you

258
00:32:48.000 --> 00:32:53.000
know replace a down note you know the
connections will all be held and
everything like

259
00:32:53.000 --> 00:33:02.000
that the next is is that we then
released a new data type time stamp with
time zone so that was kind of a big ask

260
00:33:02.000 --> 00:33:10.000
a lot of people want you know it was
something i was in Postgres that we
didn't have the next feature that we

261
00:33:10.000 --> 00:33:17.000
also released recently was automatic
compression with create table as syntax
so if you've ever used you know the

262
00:33:17.000 --> 00:33:23.000
create table as and some select
statement what would happen in redshift
previously was is that you didn't get

263
00:33:23.000 --> 00:33:32.000
any compression settings applied now we
figure that out and look like impression
for you another one that was kind of a

264
00:33:32.000 --> 00:33:38.000
big ask was limiting users connections
you kind of got like you know some some
users some analysts or someone that

265
00:33:38.000 --> 00:33:45.000
would connect to redshift with 50
connections or 100 connections and kind
of you know sort saturating up the we

266
00:33:45.000 --> 00:33:51.000
have 500 connection limited redshift so
they'd start taking up you know a ton of
the connections so you can apply now

267
00:33:51.000 --> 00:34:02.000
connection limits per user the next is a
kind of a special feature in a way it's
it's actually a feature where if you

268
00:34:02.000 --> 00:34:10.000
have a single sort key and you're adding
data in sorted order a redshift will
complete we'll just figure out that the

269
00:34:10.000 --> 00:34:19.000
data sorted and you won't need to vacuum
your tables in any way the next is a
security feature called enhanced epc

270
00:34:19.000 --> 00:34:28.000
routing what this does is it makes it
forces redshift to route data through
your VPC when connecting to s3 and this

271
00:34:28.000 --> 00:34:37.000
allows you to place VPC endpoints
policies on to the connection so you can
restrict which buckets and stuff that

272
00:34:37.000 --> 00:34:48.000
redshift can access 2016 was a huge year
for performance for rent shift we the
team focus just massively on you know

273
00:34:48.000 --> 00:34:59.000
performance vacuums ha and 10x
performance increase in some cases it
was more than 100 x in certain scenarios

274
00:34:59.000 --> 00:35:08.000
snapshot restore is twice as fast as it
used to be and you know queries we
saw several times you know performance

275
00:35:08.000 --> 00:35:16.000
improvement in 34 x performance
improvement pretty much across the board
the next this is a brand new feature I

276
00:35:16.000 --> 00:35:22.000
was actually putting you know going
through these slides and plant them
together I just wanted to you know grab

277
00:35:22.000 --> 00:35:27.000
the latest features for you guys and
this one was I didn't even know that
this one had come out until I was you

278
00:35:27.000 --> 00:35:33.000
know going through these slides and this
was that the Z standard which is a new
compression type that we've added to red

279
00:35:33.000 --> 00:35:41.000
shift and what that is is that it's a
new compression type it was I just built
by Facebook and it gets quite a bit

280
00:35:41.000 --> 00:35:50.000
better compression ratio than lzo so
yeah new compression type has been added
the next is is to make migrations easier

281
00:35:50.000 --> 00:36:00.000
SCT stands for schema conversion tool
what we've done is is oracle and
teradata it can do a one-time push or

282
00:36:00.000 --> 00:36:10.000
pull of data out of those two platforms
and into red shift so schema conversion
tool it's a kind of a standalone product

283
00:36:10.000 --> 00:36:19.000
it's free and downloaded from the AWS
site and you can connect it to you know
a handful of different databases and

284
00:36:19.000 --> 00:36:27.000
data warehouses and what it'll do is
it'll go through the your schema and it
will convert it as much close as it can

285
00:36:27.000 --> 00:36:36.000
to a redshift schema so you know totally
a useful a really useful application to
make migrations a lot easier you know

286
00:36:36.000 --> 00:36:44.000
it'll get you probably eighty percent of
the way there and then you can kind of
tweak the last bits yourself the next is

287
00:36:44.000 --> 00:36:52.000
upcoming features so these are a handful
of features are kind of exciting the
first one I know it's the last slide I

288
00:36:52.000 --> 00:36:58.000
kind of talking about node failure
tolerance I was like okay connections
are parked so what we're going to end up

289
00:36:58.000 --> 00:37:05.000
doing going forward is is we're going to
end up resubmitting queries and such for
you so this will kind of make node

290
00:37:05.000 --> 00:37:12.000
failure tolerance kind of really fully
featured in that you know you an
in-flight query well a node went down

291
00:37:12.000 --> 00:37:18.000
that you know just happened to be times
that way we'll figure that out and
resubmit it so you don't even notice

292
00:37:18.000 --> 00:37:26.000
that is a node went down the next is is
we're going to add what are called
pre-modern roles these are rules that

293
00:37:26.000 --> 00:37:34.000
can be set up to basically if they have
an analyst or someone that's maybe
writing bad queries that come along and

294
00:37:34.000 --> 00:37:42.000
I do some Cartesian product or something
crazy you know some of those going to
return billions of rows this allows you

295
00:37:42.000 --> 00:37:51.000
to set up rules that detect those bad
queries and kill the query and basically
to save the cluster resources for other

296
00:37:51.000 --> 00:38:01.000
you know more meaningful queries the
next is is a feature that we're calling
Crete short query bias and what that

297
00:38:01.000 --> 00:38:08.000
does is that imagine you have like a
whole bunch of queries that are heating
the system obviously queries q op at a

298
00:38:08.000 --> 00:38:15.000
certain point you know they wait for the
queries before them to finish because it
makes more sense to do that what will do

299
00:38:15.000 --> 00:38:21.000
is it will detect that a query while
it's you know coming in we can you know
kind of roughly tell how many rows and

300
00:38:21.000 --> 00:38:26.000
how much data it's going to scan we're
going to analyze that and figure it out
and we're going to push it to the front

301
00:38:26.000 --> 00:38:35.000
of the queue if it's a short query the
next is is power start what that is is
that kind of ties in with short period

302
00:38:35.000 --> 00:38:43.000
bias and power start basically is is a
query when it begins executing will
basically given a whole bunch of

303
00:38:43.000 --> 00:38:53.000
resources in the system to get done and
out of the way now the whole idea around
these two features is basically to let

304
00:38:53.000 --> 00:39:02.000
you you know you want to have the short
interactive queries finish quickly and
so you know query that takes you know 23

305
00:39:02.000 --> 00:39:11.000
seconds you want it to kind of be done
in two or three seconds a query that
takes you 15 minutes if we kind of slow

306
00:39:11.000 --> 00:39:17.000
them up to let them make sure that those
short interactive queries get done and
you know instead of a long-running query

307
00:39:17.000 --> 00:39:25.000
taking 15 minutes it takes 18 minutes
that's probably fine and most people
aren't going to complain about that so

308
00:39:25.000 --> 00:39:30.000
that's kind of the with the kind where
we're going with that try and figure
that out for you

309
00:39:30.000 --> 00:39:39.000
the next is is vacuum so we're going to
be coming out with automatic vacuum hi
that's one of the next upcoming features

310
00:39:39.000 --> 00:39:48.000
that we're also working on for now we
have a really good utility that we've
released onto github from the redshift

311
00:39:48.000 --> 00:39:54.000
utils and that's you know it's on our
we've released a whole bunch of
utilities and useful queries and all

312
00:39:54.000 --> 00:40:02.000
that kind of stuff on to github and
there's one on there for vacuum and
analyze you can schedule that utility

313
00:40:02.000 --> 00:40:10.000
with cron with whatever makes sense you
know for your you know depending on your
query powder and such and yeah so you

314
00:40:10.000 --> 00:40:15.000
can use that for now but in the future
you know we want to take care of that
for you as well because we want to you

315
00:40:15.000 --> 00:40:24.000
know make redshift is easy to use as
possible the next is is I am
authentication right now you know all of

316
00:40:24.000 --> 00:40:29.000
the authentication stuff in red shift is
covered you know it's all managed in red
shift all the usernames all the

317
00:40:29.000 --> 00:40:36.000
passwords all that kind of stuff is in
there we want to we're going to link
that in with I am so you can attend a

318
00:40:36.000 --> 00:40:44.000
que using I am as well the next is I'm
going to show just mention the last one
at the bottom there now jump up so

319
00:40:44.000 --> 00:40:50.000
approximate percentile functions we're
going to be releasing some new
approximate functions in the near future

320
00:40:50.000 --> 00:40:57.000
and then the next is is the back to the
schema conversion tool we're going to be
adding support for both vertica and

321
00:40:57.000 --> 00:41:08.000
sequel server so you can convert schemas
from both of those systems those github
utilities that I mentioned these are

322
00:41:08.000 --> 00:41:17.000
some of the links that kind of get you
there and the vacuum one actually isn't
called out on this slide here but it is

323
00:41:17.000 --> 00:41:26.000
up on the site there the couple others
that are worth calling out the
administra pozar is there a handful of

324
00:41:26.000 --> 00:41:32.000
scripts that our database engineering
team the solution architects like myself
I've kind of found useful and we've

325
00:41:32.000 --> 00:41:39.000
basically added those up there there's
they're actually scripts that we run on
a very regular basis when we're working

326
00:41:39.000 --> 00:41:46.000
with redshift and you know so that's why
we've added them up there
same thing with the admin a couple in

327
00:41:46.000 --> 00:41:51.000
there that are worth calling out is the
ones that generates schema for ddl
definitely you know if you're wanting to

328
00:41:51.000 --> 00:41:58.000
you know you're a DBA type person you'll
probably want to end up going through
the Indian administrating a handful of

329
00:41:58.000 --> 00:42:09.000
those as well the last one there is is a
column encoding utility which can be
used just to encode data and such so

330
00:42:09.000 --> 00:42:18.000
that concludes this portion of the
webinar we're going to jump into the QA
at this point here thank you and thank

331
00:42:18.000 --> 00:42:24.000
you Tony thanks for the great
presentation I'd also like to thank hoo
Hong and Peter Dalton career engaging

332
00:42:24.000 --> 00:42:29.000
the audience throughout the presentation
with our Q&A answers on your screen
you'll see three poles at the bottom of

333
00:42:29.000 --> 00:42:36.000
the screen and one at the top we do
value your feedback so please take time
to complete that we also have your Q&A

334
00:42:36.000 --> 00:42:41.000
pod on the upper left where you can
continue putting your questions in for
our session we're going to start our Q&A

335
00:42:41.000 --> 00:42:48.000
session the first question here that's
kind of Alden is when will you allow
copying data from a table in one

336
00:42:48.000 --> 00:42:57.000
database to a table into another
database without having to use unloading
copy that is actually been a commonly

337
00:42:57.000 --> 00:43:05.000
asked question like Liam don't have any
kind of feedback on that or when we're
going to be adding support for that or

338
00:43:05.000 --> 00:43:13.000
anything like that but definitely pass
it along to our product management team
thank you very much for that the next

339
00:43:13.000 --> 00:43:20.000
question is does redshift have any tools
processes which look at each query be
and execute and suggest better

340
00:43:20.000 --> 00:43:30.000
compression or move data between nodes
for better execution time the first bit
is is we do have a tool for better

341
00:43:30.000 --> 00:43:39.000
compression there's one built in the
redshift called analyzed compression but
really all that one does is it just

342
00:43:39.000 --> 00:43:47.000
figures it basically just gives us
suggestions if you want to actually have
it apply the compression and recopy the

343
00:43:47.000 --> 00:43:54.000
data and move it all that there is a
utility on github for that that is very
commonly used and it uses a combination

344
00:43:54.000 --> 00:44:11.000
of analyzed compression
and you know fully copies the tables and
such for you the next question is around

345
00:44:11.000 --> 00:44:19.000
partitioning why doesn't redshift have a
concept of partitioning right now it is
true that we don't have partitioning and

346
00:44:19.000 --> 00:44:27.000
redshift most customers what they'll do
if they there's a to kind of reasons for
it the first is is in many if not most

347
00:44:27.000 --> 00:44:35.000
cases most customers don't need it
that's not just say that there aren't
cases where it is needed so when

348
00:44:35.000 --> 00:44:43.000
customers do need it the workaround
really is to kind of create multiple
tables and layer over top of a union all

349
00:44:43.000 --> 00:44:51.000
of you we've been improving the union
all views significantly over the last
year to you know so the Creator is

350
00:44:51.000 --> 00:44:58.000
performant hopefully as you know running
it against a single table there is
obviously a little bit of maintenance in

351
00:44:58.000 --> 00:45:04.000
that you have to recreate the view when
you add tables and essentially which are
you're using as partitions under the

352
00:45:04.000 --> 00:45:24.000
hood
the next question is backups how are
they done and is there a cost yeah

353
00:45:24.000 --> 00:45:35.000
redshift backs up basically continuously
asynchronously in the background what
what it does is is every five gigs of

354
00:45:35.000 --> 00:45:43.000
change data in the redshift cluster the
backup process will kick off and it will
synchronize the data if five gigs of

355
00:45:43.000 --> 00:45:50.000
data haven't changed for a period of
time eight hours so the most you'll
usually be lagging behind is eight hours

356
00:45:50.000 --> 00:45:58.000
in your automated backups and they are
free it's fully included in the cost of
redshift so you read shifts all

357
00:45:58.000 --> 00:46:16.000
inclusive pricing so backups are totally
free or well included I should say
I think that's about it for questions

358
00:46:16.000 --> 00:46:33.000
I'll wait another 15 seconds if there's
any other questions but I think that's
about it thank you guys I like to spend

359
00:46:33.000 --> 00:46:38.000
a special thank you here to our
presenter today for tell you for the
great time the presentation that you are

360
00:46:38.000 --> 00:46:44.000
presenting with us today and of course
our moderators today well hung and Peter
dolphin for answering the audience's

361
00:46:44.000 --> 00:46:50.000
questions today you have any comments on
the previous or upcoming webinars or
suggestions on the topics you wish AWS

362
00:46:50.000 --> 00:46:59.000
to cover in future webinars please feel
free to email us at AWS webcast at
amazon com your feedback will help us
improve our webinar programming thanks
everybody for joining today at this time
you could disconnect